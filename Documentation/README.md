# Customer Churn Prediction - Complete ML Pipeline

A comprehensive machine learning project to predict customer churn using multiple algorithms and advanced analysis techniques.

## ğŸ¯ Project Overview

This project implements a complete machine learning pipeline for predicting customer churn, focusing on:

- **Business Value**: Identify at-risk customers before they leave to reduce acquisition costs
- **Technical Excellence**: Comprehensive ML pipeline with proper evaluation and comparison
- **Scalability**: Modular design for easy extension and maintenance

## ğŸš€ Key Features

### ğŸ¤– Machine Learning Models
- **Logistic Regression**: Fast baseline model
- **Random Forest**: Robust ensemble method
- **Gradient Boosting**: Advanced boosting algorithm
- **XGBoost**: State-of-the-art gradient boosting

### ğŸ”§ Advanced Preprocessing
- Automated feature type detection
- Missing value imputation (multiple strategies)
- Outlier detection and treatment
- Feature scaling and normalization
- Class imbalance handling (SMOTE, undersampling)

### ğŸ“Š Comprehensive Evaluation
- Multiple metrics (Accuracy, Precision, Recall, F1, ROC-AUC)
- Cross-validation with stratified sampling
- Learning curves and validation curves
- Feature importance analysis
- Model comparison and ranking

### ğŸ“ˆ Rich Visualizations
- Interactive plots with Plotly
- ROC and Precision-Recall curves
- Confusion matrices
- Feature importance plots
- Distribution analysis

### âš™ï¸ Hyperparameter Optimization
- Grid Search
- Random Search
- Optuna (Bayesian optimization)
- Automated parameter tuning

## ğŸ“ Project Structure

```
AI-Project/
â”œâ”€â”€ ğŸ“– README.md                       # This file
â”œâ”€â”€ ğŸ“¦ requirements.txt                # Dependencies
â”œâ”€â”€ âš™ï¸ setup.py                       # Package configuration
â”œâ”€â”€ ğŸš€ main.py                        # Main pipeline script
â”œâ”€â”€ âš¡ quick_start.py                  # Quick demo script
â”‚
â”œâ”€â”€ data/                              # Data storage
â”‚   â”œâ”€â”€ raw/                          # Original datasets
â”‚   â”œâ”€â”€ processed/                    # Cleaned data
â”‚   â””â”€â”€ README.md                     # Data documentation
â”‚
â”œâ”€â”€ src/                              # Core source code
â”‚   â”œâ”€â”€ config.py                     # Configuration settings
â”‚   â”œâ”€â”€ data_loader.py               # Data loading utilities
â”‚   â”œâ”€â”€ exploratory_analysis.py      # EDA functions
â”‚   â”œâ”€â”€ feature_engineering.py       # Feature processing
â”‚   â”œâ”€â”€ model_trainer.py            # Model training
â”‚   â”œâ”€â”€ evaluator.py               # Model evaluation
â”‚   â””â”€â”€ visualizer.py              # Visualization tools
â”‚
â”œâ”€â”€ utils/                           # Utility functions
â”‚   â”œâ”€â”€ helpers.py                  # General utilities
â”‚   â”œâ”€â”€ preprocessing.py           # Advanced preprocessing
â”‚   â””â”€â”€ plotting.py               # Plotting utilities
â”‚
â”œâ”€â”€ notebooks/                      # Jupyter notebooks
â”‚   â”œâ”€â”€ 01_exploratory_analysis.ipynb
â”‚   â”œâ”€â”€ 02_feature_engineering.ipynb
â”‚   â”œâ”€â”€ 03_class_imbalance_analysis.ipynb
â”‚   â”œâ”€â”€ 04_model_training.ipynb
â”‚   â””â”€â”€ 05_model_evaluation.ipynb
â”‚
â”œâ”€â”€ models/                         # Saved models
â”‚   â”œâ”€â”€ decision_tree/
â”‚   â”œâ”€â”€ random_forest/
â”‚   â””â”€â”€ README.md
â”‚
â””â”€â”€ results/                       # Output results
    â”œâ”€â”€ figures/                   # Generated plots
    â”œâ”€â”€ reports/                   # Analysis reports
    â””â”€â”€ metrics/                   # Performance metrics
```

## ğŸ› ï¸ Installation & Setup

### 1. Clone the Repository
```bash
git clone <repository-url>
cd AI-Project
```

### 2. Create Virtual Environment (Recommended)
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### 3. Install Dependencies
```bash
pip install -r requirements.txt
```

### 4. Verify Installation
```bash
python quick_start.py
```

## ğŸš€ Quick Start

### Option 1: Quick Demo (5 minutes)
```bash
python quick_start.py
```
This runs a complete example with synthetic data.

### Option 2: Full Pipeline
```bash
python main.py --data-path your_data.csv
```

### Option 3: Custom Configuration
```bash
python main.py \
    --data-path data/raw/customer_data.csv \
    --optimization optuna \
    --models random_forest xgboost \
    --log-level INFO
```

## ğŸ“Š Usage Examples

### Basic Model Training
```python
from src.model_trainer import ModelTrainer

# Initialize trainer
trainer = ModelTrainer()

# Train all models
results = trainer.train_all_models(X_train, y_train)

# Save models
trainer.save_models()
```

### Model Evaluation
```python
from src.evaluator import ModelEvaluator

# Initialize evaluator
evaluator = ModelEvaluator()

# Evaluate models
results = evaluator.evaluate_multiple_models(
    trainer.trained_models, X_test, y_test
)

# Generate report
report = evaluator.generate_evaluation_report()
print(report)
```

### Feature Engineering
```python
from src.feature_engineering import FeatureEngineer

# Initialize feature engineer
engineer = FeatureEngineer()

# Process features
X_processed = engineer.fit_transform(X_raw)
```

## ğŸ“ˆ Expected Results

The pipeline typically achieves:
- **Accuracy**: 85-92%
- **ROC-AUC**: 0.88-0.95
- **F1-Score**: 0.82-0.90

Performance varies based on:
- Data quality and size
- Feature engineering
- Model selection
- Hyperparameter tuning

## ğŸ”¬ Advanced Features

### Hyperparameter Optimization
```bash
# Grid search
python main.py --optimization grid_search

# Random search (recommended)
python main.py --optimization random_search

# Optuna (Bayesian optimization)
python main.py --optimization optuna
```

### Custom Model Configuration
Edit `src/config.py` to:
- Add new models
- Modify hyperparameter ranges
- Change evaluation metrics
- Adjust preprocessing options

### Interactive Analysis
Use Jupyter notebooks for detailed analysis:
```bash
jupyter lab notebooks/
```

## ğŸ“‹ Command Line Options

```bash
python main.py [OPTIONS]

Options:
  --data-path PATH           Path to dataset file
  --optimization METHOD      Optimization method [grid_search|random_search|optuna|default]
  --models MODEL [MODEL...]  Specific models to train
  --log-level LEVEL         Logging level [DEBUG|INFO|WARNING|ERROR]
  --skip-eda                Skip exploratory data analysis
  --help                    Show help message
```

## ğŸ¯ Task Breakdown (9 Core Tasks)

âœ… **Task 1**: Import Libraries - Complete environment setup
âœ… **Task 2**: Exploratory Data Analysis - Comprehensive data exploration
âœ… **Task 3**: Encoding Categorical Variables - Advanced feature engineering
âœ… **Task 4**: Class Imbalance Analysis - Multiple balancing techniques
âœ… **Task 5**: Train/Validation Split - Stratified data splitting
âœ… **Task 6-7**: Decision Tree Implementation - With hyperparameter tuning
âœ… **Task 8**: Random Forest Implementation - Ensemble learning
âœ… **Task 9**: Model Evaluation - Comprehensive performance assessment

## ğŸ“Š Output Files

After running the pipeline, you'll find:

- **Models**: `models/` - Trained model files (.pkl)
- **Figures**: `results/figures/` - All visualizations (.png)
- **Reports**: `results/reports/` - Evaluation reports (.txt)
- **Metrics**: `results/metrics/` - Performance metrics (.json)

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## ğŸ“ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ†˜ Troubleshooting

### Common Issues

1. **Import Errors**: Ensure all dependencies are installed
   ```bash
   pip install -r requirements.txt
   ```

2. **Data Loading Issues**: Check data file path and format
   ```python
   # Verify data format
   import pandas as pd
   df = pd.read_csv('your_data.csv')
   print(df.info())
   ```

3. **Memory Issues**: For large datasets, use data sampling
   ```bash
   python main.py --data-path large_dataset.csv --sample-size 10000
   ```

### Getting Help

- Check the documentation in `docs/`
- Review example notebooks
- Run `python quick_start.py` for basic example
- Open an issue on GitHub

## ğŸ™ Acknowledgments

- Scikit-learn for machine learning algorithms
- Plotly for interactive visualizations
- Optuna for hyperparameter optimization
- The open-source community for inspiration

---

**Ready to predict customer churn? Start with `python quick_start.py`! ğŸš€**