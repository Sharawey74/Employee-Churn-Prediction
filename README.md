# AI Project - Employee Turnover Prediction

## ğŸ¯ Project Overview

This project is a comprehensive machine learning pipeline for predicting employee turnover using **Random Forest and XGBoost** algorithms. The project has been optimized to focus exclusively on these two high-performing tree-based models, providing automated model comparison, hyperparameter optimization, and comprehensive evaluation.

### Key Features
- **Focused Algorithm Set**: Random Forest and XGBoost only for optimal performance
- **Automated Model Selection**: Best model identification based on cross-validation scores
- **Comprehensive Evaluation**: Multiple metrics with detailed performance analysis
- **Organized Structure**: Clean project organization with dedicated directories
- **JSON Outputs**: Structured results in JSON format for easy integration
- **Validation Tools**: Comprehensive validation and debugging utilities

## ğŸ“ Directory Structure

```
AI-Project/
â”œâ”€â”€ ğŸ“Š data/                          # Dataset storage
â”‚   â”œâ”€â”€ raw/                          # Original datasets
â”‚   â”‚   â”œâ”€â”€ employee_data.csv         # Primary training data (11,413 rows, 28 features)
â”‚   â”‚   â””â”€â”€ sample_customer_churn.csv # Alternative dataset
â”‚   â””â”€â”€ processed/                    # Cleaned and engineered datasets
â”‚       â”œâ”€â”€ balanced_data.csv         # Class-balanced dataset
â”‚       â”œâ”€â”€ feature_engineered_data.csv # Final training dataset
â”‚       â”œâ”€â”€ feature_info.json         # Feature engineering metadata
â”‚       â””â”€â”€ processed_data.csv        # Intermediate processed data
â”œâ”€â”€ ğŸ¤– models/                        # Model artifacts (RF & XGBoost only)
â”‚   â”œâ”€â”€ random_forest_model.pkl      # Trained Random Forest
â”‚   â”œâ”€â”€ xgboost_model.pkl           # Trained XGBoost
â”‚   â”œâ”€â”€ best_parameters.pkl          # Optimal hyperparameters
â”‚   â”œâ”€â”€ evaluation_data.pkl          # Train/test splits
â”‚   â”œâ”€â”€ model_registry_rf_xgb.json   # Model metadata
â”‚   â”œâ”€â”€ random_forest/              # RF-specific files
â”‚   â”‚   â”œâ”€â”€ feature_importance.json
â”‚   â”‚   â”œâ”€â”€ hyperparameters.json
â”‚   â”‚   â”œâ”€â”€ metrics.json
â”‚   â”‚   â””â”€â”€ model_info.md
â”‚   â””â”€â”€ xgboost/                   # XGBoost-specific files
â”‚       â”œâ”€â”€ feature_importance.json
â”‚       â”œâ”€â”€ hyperparameters.json
â”‚       â”œâ”€â”€ metrics.json
â”‚       â””â”€â”€ model_info.md
â”œâ”€â”€ ğŸ“„ json/                        # All JSON outputs
â”‚   â”œâ”€â”€ model_comparison.json       # RF vs XGBoost comparison
â”‚   â”œâ”€â”€ best_parameters.json       # Hyperparameters in JSON format
â”‚   â”œâ”€â”€ cv_results.json            # Cross-validation scores
â”‚   â”œâ”€â”€ model_summary.json         # Training summary
â”‚   â”œâ”€â”€ test_results.json          # Test evaluation metrics
â”‚   â”œâ”€â”€ random_forest_feature_importance.json
â”‚   â””â”€â”€ xgboost_feature_importance.json
â”œâ”€â”€ ğŸ“ˆ results/                     # Training results and visualizations
â”‚   â”œâ”€â”€ figures/                   # Model performance plots
â”‚   â”‚   â”œâ”€â”€ model_comparison.png   # Performance comparison chart
â”‚   â”‚   â”œâ”€â”€ roc_curves.png         # ROC curves visualization
â”‚   â”‚   â””â”€â”€ target_distribution.png # Target variable distribution
â”‚   â”œâ”€â”€ reports/                   # Generated reports
â”‚   â””â”€â”€ metrics/                   # Performance metrics
â”œâ”€â”€ ğŸ”§ src/                        # Core modules (updated for RF & XGBoost)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py                 # Configuration (RF & XGBoost only)
â”‚   â”œâ”€â”€ model_trainer.py          # Training pipeline (restricted)
â”‚   â”œâ”€â”€ data_loader.py            # Data loading utilities
â”‚   â”œâ”€â”€ feature_engineering.py    # Feature engineering
â”‚   â”œâ”€â”€ evaluator.py              # Model evaluation
â”‚   â””â”€â”€ visualizer.py             # Visualization tools
â”œâ”€â”€ ğŸš€ main/                       # Execution scripts
â”‚   â”œâ”€â”€ main.py                   # Full pipeline (RF & XGBoost)
â”‚   â”œâ”€â”€ rf_xgb_trainer.py         # Quick trainer script
â”‚   â””â”€â”€ quick_start.py            # Simplified execution
â”œâ”€â”€ ğŸ” validations/                # Validation and debugging scripts
â”‚   â”œâ”€â”€ __init__.py               # Package initialization
â”‚   â”œâ”€â”€ analyze_cv.py             # Cross-validation analysis
â”‚   â”œâ”€â”€ analyze_pkl_detailed.py   # Detailed PKL file analysis
â”‚   â”œâ”€â”€ analyze_pkl_files.py      # Basic PKL file analysis
â”‚   â”œâ”€â”€ check_regularization.py   # Regularization parameter analysis
â”‚   â”œâ”€â”€ debug_paths.py            # Path debugging utilities
â”‚   â”œâ”€â”€ debug_trainer.py          # ModelTrainer debugging
â”‚   â”œâ”€â”€ path_demo.py              # Path navigation demonstration
â”‚   â”œâ”€â”€ setup.py                  # Project setup script
â”‚   â”œâ”€â”€ simple_validation.py      # Simple validation tests
â”‚   â”œâ”€â”€ validate_restructure.py   # Complete project validation
â”‚   â””â”€â”€ validation_output.txt     # Validation results log
â”œâ”€â”€ ğŸ§ª Testing/                    # Unit tests and integration tests
â”‚   â”œâ”€â”€ test_paths.py             # Path testing utilities
â”‚   â”œâ”€â”€ test_01_string_formatting.py
â”‚   â”œâ”€â”€ test_02_model_trainer_return.py
â”‚   â”œâ”€â”€ test_03_fixed_wrapper.py
â”‚   â”œâ”€â”€ test_04_complete_validation.py
â”‚   â””â”€â”€ ...
â”œâ”€â”€ ğŸ““ notebooks/                  # Jupyter notebooks
â”‚   â”œâ”€â”€ 01_exploratory_analysis.ipynb
â”‚   â”œâ”€â”€ 02_feature_engineering.ipynb
â”‚   â”œâ”€â”€ 03_class_imbalance_analysis.ipynb
â”‚   â”œâ”€â”€ 04_model_training.ipynb      # RF & XGBoost focus
â”‚   â”œâ”€â”€ 05_model_evaluation.ipynb    # Comparison analysis
â”‚   â””â”€â”€ 06_ModelTrainer_Fix_and_Integration.ipynb
â”œâ”€â”€ ğŸ› ï¸ utils/                      # Utility functions
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ helpers.py
â”‚   â”œâ”€â”€ plotting.py
â”‚   â””â”€â”€ preprocessing.py
â”œâ”€â”€ âœ… validation/                  # Legacy validation scripts
â””â”€â”€ ğŸ“‹ Documentation/               # Project documentation
    â”œâ”€â”€ README.md
    â””â”€â”€ requirements.txt
```

## ğŸš€ Usage Instructions

### Quick Start (Recommended)

#### Method 1: Quick Trainer Script
```bash
# Quick training with default settings
python main/rf_xgb_trainer.py

# With custom data and optimization
python main/rf_xgb_trainer.py --data-path data/raw/employee_data.csv --optimization random_search
```

#### Method 2: Full Pipeline
```bash
# Full pipeline with all steps
python main/main.py --optimization random_search

# Train specific models only
python main/main.py --models random_forest xgboost
```

#### Method 3: Python Import
```python
from main.rf_xgb_trainer import quick_train_rf_xgb

# Train and compare models
results = quick_train_rf_xgb(
    data_path='data/raw/employee_data.csv',
    optimization='random_search'
)

# Access best model
best_model = results['best_model']
best_name = results['best_model_name']
```

### Validation and Debugging

#### Run All Validations
```bash
# Complete project validation
python -m validations.validate_restructure

# Simple validation tests
python -m validations.simple_validation
```

#### Analyze Project Components
```bash
# Cross-validation analysis
python -m validations.analyze_cv

# PKL file analysis (detailed)
python -m validations.analyze_pkl_detailed

# Regularization analysis
python -m validations.check_regularization

# Debug path issues
python -m validations.debug_paths

# Test ModelTrainer functionality
python -m validations.debug_trainer
```

#### Path Navigation Demo
```bash
# Understand path navigation concepts
python -m validations.path_demo
```

### Check Evaluation Results

<<<<<<< HEAD
#### JSON Results
- **Model Comparison**: `json/model_comparison.json`
- **Cross-Validation**: `json/cv_results.json`
- **Test Results**: `json/test_results.json`
- **Feature Importance**: `json/{model}_feature_importance.json`
=======
âœ… **Task 1**: Import Libraries - Complete environment setup

âœ… **Task 2**: Exploratory Data Analysis - Comprehensive data exploration

âœ… **Task 3**: Encoding Categorical Variables - Advanced feature engineering

âœ… **Task 4**: Class Imbalance Analysis - Multiple balancing techniques

âœ… **Task 5**: Train/Validation Split - Stratified data splitting

âœ… **Task 6-7**: Decision Tree Implementation - With hyperparameter tuning

âœ… **Task 8**: Random Forest Implementation - Ensemble learning

âœ… **Task 9**: Model Evaluation - Comprehensive performance assessment
>>>>>>> 6900f7a7d71009d5023c0406c5ccb7ca341b4305

#### Visualizations
- **Performance Charts**: `results/figures/model_comparison.png`
- **ROC Curves**: `results/figures/roc_curves.png`
- **Target Distribution**: `results/figures/target_distribution.png`

#### Model Artifacts
- **Best Model**: Automatically identified and saved
- **Hyperparameters**: `models/best_parameters.pkl` and `json/best_parameters.json`
- **Model Registry**: `models/model_registry_rf_xgb.json`

## ğŸ“¦ Dependencies

### Core Requirements
```txt
pandas>=1.5.0
numpy>=1.23.0
scipy>=1.9.0
scikit-learn>=1.1.0
imbalanced-learn>=0.9.0
xgboost>=1.6.0
matplotlib>=3.5.0
seaborn>=0.11.0
plotly>=5.10.0
jupyterlab>=3.4.0
ipywidgets>=8.0.0
tqdm>=4.64.0
joblib>=1.1.0
optuna>=3.0.0
shap>=0.41.0
```

### Installation
```bash
# Install from requirements file
pip install -r requirements.txt

# Or install from setup script
python validations/setup.py install

# Or install in development mode
pip install -e .
```

## ğŸ¯ Model Configuration

### Random Forest Hyperparameters
- **n_estimators**: [100, 200, 300, 400, 500]
- **max_depth**: [10, 20, 30, 40, None]
- **min_samples_split**: [2, 5, 10, 15]
- **min_samples_leaf**: [1, 2, 4, 8]
- **max_features**: ['sqrt', 'log2', None]
- **bootstrap**: [True, False]

### XGBoost Hyperparameters
- **n_estimators**: [100, 200, 300, 400, 500]
- **learning_rate**: [0.01, 0.05, 0.1, 0.15, 0.2]
- **max_depth**: [3, 4, 5, 6, 7, 8]
- **subsample**: [0.8, 0.85, 0.9, 0.95, 1.0]
- **colsample_bytree**: [0.8, 0.85, 0.9, 0.95, 1.0]
- **reg_alpha**: [0, 0.01, 0.1, 0.5, 1.0] (L1 regularization)
- **reg_lambda**: [0, 0.01, 0.1, 0.5, 1.0] (L2 regularization)

## ğŸ”„ Cross-Validation & Regularization

### Cross-Validation Setup
- **Method**: 5-fold StratifiedKFold
- **Scoring**: ROC-AUC (primary), F1-score (secondary)
- **Iterations**: 100 (RandomizedSearchCV)
- **Parallel Processing**: Full CPU utilization
- **Random State**: Fixed for reproducibility

### Regularization Techniques
- **Random Forest**: Tree depth limits, sample splits, bootstrap sampling
- **XGBoost**: L1/L2 regularization, subsampling, column sampling
- **Early Stopping**: Prevented through proper validation

## ğŸ“Š Performance Metrics

Both models are evaluated using:
- **Accuracy**: Overall prediction correctness
- **Precision**: True positive rate (weighted average)
- **Recall**: Sensitivity (weighted average)
- **F1-Score**: Harmonic mean of precision/recall
- **ROC-AUC**: Area under ROC curve
- **Cross-Validation Score**: 5-fold CV performance

## ğŸ† Model Selection Criteria

1. **Primary**: Cross-validation ROC-AUC score
2. **Secondary**: Test set F1-score
3. **Tertiary**: Test set accuracy
4. **Considerations**: Training time, interpretability, feature importance

## ğŸ“‹ Output Storage

### Models Directory
- **Trained Models**: `{model_name}_model.pkl`
- **Hyperparameters**: `best_parameters.pkl`
- **Evaluation Data**: `evaluation_data.pkl`
- **Model-Specific**: `{model_name}/` subdirectories

### JSON Directory
- **All Results**: Structured JSON format
- **Feature Importance**: Per-model feature rankings
- **Comparison Data**: Model performance comparisons
- **Metadata**: Training configuration and timestamps

### Results Directory
- **Visualizations**: `figures/` subdirectory
- **Reports**: Generated analysis reports
- **Metrics**: Detailed performance metrics

## ğŸ”§ Notes

### Validation Scripts
- All validation and debug scripts are now organized under `validations/`
- Use Python module syntax: `python -m validations.script_name`
- Scripts are self-contained with proper path handling
- Comprehensive validation available via `validate_restructure.py`

### Path Handling
- All scripts use relative imports and proper path resolution
- Project root is automatically detected using `Path(__file__).parent.parent`
- No hardcoded paths - fully portable across environments
- Import issues resolved through proper `sys.path` management

### Project Structure
- **Organized**: Clear separation of concerns across directories
- **Scalable**: Easy to add new models or validation scripts
- **Maintainable**: Consistent structure and naming conventions
- **Documented**: Comprehensive documentation and examples

## ğŸ†˜ Troubleshooting

### Common Issues
1. **Import Errors**: Ensure you're running scripts from project root
2. **Path Issues**: Use `python -m validations.debug_paths` to check paths
3. **Model Issues**: Use `python -m validations.debug_trainer` to test models
4. **Validation Errors**: Run `python -m validations.validate_restructure` for full check

### Getting Help
- Check validation outputs in `validations/validation_output.txt`
- Run individual validation scripts for specific issues
- Review JSON outputs for detailed results
- Check model artifacts in `models/` directory

---

**Last Updated**: Project restructured with dedicated `validations/` directory and comprehensive documentation.
