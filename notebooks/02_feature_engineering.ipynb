{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7a889d6",
   "metadata": {},
   "source": [
    "# 🔧 Feature Engineering: Preparing Data for Machine Learning\n",
    "\n",
    "Welcome to **Feature Engineering** - one of the most crucial steps in machine learning! 🚀\n",
    "\n",
    "## 🎯 What is Feature Engineering?\n",
    "\n",
    "Feature Engineering is like being a **data chef** 👨‍🍳 - you take raw ingredients (original data) and transform them into a delicious meal (model-ready features) that machine learning algorithms can easily digest and learn from.\n",
    "\n",
    "### 🌟 Why is this so important?\n",
    "- **🎯 Better Predictions**: Well-engineered features = better model performance\n",
    "- **🧠 Algorithm Friendly**: Makes data easier for algorithms to understand\n",
    "- **📊 Information Extraction**: Reveals hidden patterns in the data\n",
    "- **⚡ Efficiency**: Reduces training time and improves accuracy\n",
    "\n",
    "## 📚 What We'll Learn Today:\n",
    "\n",
    "### 🔄 **Data Preprocessing** \n",
    "- Handle missing values (fill the gaps)\n",
    "- Deal with outliers (unusual values)\n",
    "- Remove duplicates and clean data\n",
    "\n",
    "### 🏷️ **Categorical Encoding**\n",
    "- Convert text labels to numbers\n",
    "- One-Hot Encoding, Label Encoding, Target Encoding\n",
    "- Handle high-cardinality categories\n",
    "\n",
    "### 📏 **Feature Scaling** \n",
    "- Standardization vs. Normalization\n",
    "- When and why to scale features\n",
    "- Different scaling techniques\n",
    "\n",
    "### ✨ **Feature Creation**\n",
    "- Create new features from existing ones\n",
    "- Polynomial features, interactions\n",
    "- Domain-specific transformations\n",
    "\n",
    "### 🎯 **Feature Selection**\n",
    "- Remove irrelevant or redundant features\n",
    "- Statistical tests and importance scores\n",
    "- Dimensionality reduction\n",
    "\n",
    "### ⚖️ **Class Imbalance Handling**\n",
    "- SMOTE (Synthetic Minority Oversampling)\n",
    "- Undersampling and Oversampling\n",
    "- Class weights\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 Let's Transform Our Data!\n",
    "\n",
    "By the end of this notebook, you'll have:\n",
    "- ✅ Clean, preprocessed data\n",
    "- ✅ Properly encoded categorical variables  \n",
    "- ✅ Scaled numerical features\n",
    "- ✅ New engineered features\n",
    "- ✅ Balanced target classes\n",
    "- ✅ A dataset ready for machine learning!\n",
    "\n",
    "Let's dive in! 🏊‍♂️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74c69c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📦 Step 1: Import All Required Libraries\n",
    "print(\"📦 IMPORTING LIBRARIES FOR FEATURE ENGINEERING...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Core data manipulation and analysis\n",
    "import pandas as pd              # Data manipulation and analysis\n",
    "import numpy as np               # Numerical computing\n",
    "import matplotlib.pyplot as plt  # Data visualization\n",
    "import seaborn as sns            # Statistical data visualization\n",
    "import warnings                  # Handle warning messages\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up nice-looking plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from sklearn.model_selection import train_test_split     # Split data into train/test\n",
    "from sklearn.preprocessing import StandardScaler         # Scale features to standard distribution\n",
    "from sklearn.preprocessing import MinMaxScaler          # Scale features to 0-1 range\n",
    "from sklearn.preprocessing import LabelEncoder          # Convert categories to numbers\n",
    "from sklearn.preprocessing import OneHotEncoder         # Create binary columns for categories\n",
    "from sklearn.feature_selection import SelectKBest       # Select best features\n",
    "from sklearn.feature_selection import chi2, f_classif   # Statistical tests for feature selection\n",
    "\n",
    "# Handle class imbalance\n",
    "from imblearn.over_sampling import SMOTE                # Synthetic Minority Oversampling\n",
    "from collections import Counter                         # Count class frequencies\n",
    "\n",
    "# File path handling\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# 📁 Step 2: Set up paths to our project\n",
    "print(\"\\n📁 SETTING UP PROJECT PATHS...\")\n",
    "print(\"=\"*35)\n",
    "\n",
    "# Get the current notebook directory and find project root\n",
    "current_dir = Path.cwd()\n",
    "project_root = current_dir.parent\n",
    "\n",
    "# Add src directory to Python path so we can import our modules\n",
    "src_path = project_root / 'src'\n",
    "sys.path.append(str(src_path))\n",
    "\n",
    "print(f\"📂 Project root: {project_root}\")\n",
    "print(f\"📂 Source code: {src_path}\")\n",
    "print(f\"📂 Current notebook: {current_dir}\")\n",
    "\n",
    "# 🎯 Step 3: Define what we're trying to predict\n",
    "print(\"\\n🎯 SETTING UP TARGET VARIABLE...\")\n",
    "print(\"=\"*35)\n",
    "\n",
    "# This will be updated once we load and examine the data\n",
    "TARGET_COLUMN = None  # We'll identify this from the data\n",
    "RANDOM_STATE = 42     # For reproducible results\n",
    "\n",
    "print(f\"🎲 Random state set to: {RANDOM_STATE} (for reproducible results)\")\n",
    "print(\"✅ Setup complete! Ready to load and engineer features.\")\n",
    "\n",
    "# 📊 Step 4: Prepare for data loading\n",
    "print(\"\\n📊 PREPARING FOR DATA LOADING...\")\n",
    "print(\"=\"*35)\n",
    "print(\"Next, we'll load our cleaned data from the previous EDA analysis.\")\n",
    "print(\"We'll use the insights from EDA to guide our feature engineering decisions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78ddf24",
   "metadata": {},
   "source": [
    "## 📁 Step 1: Load and Examine Our Data\n",
    "\n",
    "Let's start by loading our dataset and taking a quick look at what we're working with. We'll use the insights from our previous EDA to guide our feature engineering decisions.\n",
    "\n",
    "### 🔍 What we'll check:\n",
    "- **📊 Data shape and types**: Understand our starting point\n",
    "- **🎯 Target variable**: Identify what we're predicting  \n",
    "- **📋 Feature types**: Separate numerical and categorical features\n",
    "- **❓ Data quality**: Check for issues that need fixing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83383279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📁 Step 1: Load our dataset\n",
    "print(\"📁 LOADING DATASET...\")\n",
    "print(\"=\"*25)\n",
    "\n",
    "# Try to load from the data directory\n",
    "data_path = project_root / 'data' / 'raw'\n",
    "possible_files = ['employee_data.csv', 'employees.csv', 'hr_data.csv', 'data.csv']\n",
    "\n",
    "df = None\n",
    "loaded_file = None\n",
    "\n",
    "# Look for common employee dataset filenames\n",
    "for filename in possible_files:\n",
    "    file_path = data_path / filename\n",
    "    if file_path.exists():\n",
    "        print(f\"📂 Found data file: {filename}\")\n",
    "        df = pd.read_csv(file_path)\n",
    "        loaded_file = filename\n",
    "        break\n",
    "\n",
    "# If not found in data/raw, try other locations\n",
    "if df is None:\n",
    "    # Try data directory root\n",
    "    for filename in possible_files:\n",
    "        file_path = project_root / 'data' / filename\n",
    "        if file_path.exists():\n",
    "            print(f\"📂 Found data file: {filename}\")\n",
    "            df = pd.read_csv(file_path)\n",
    "            loaded_file = filename\n",
    "            break\n",
    "\n",
    "# If still not found, try project root\n",
    "if df is None:\n",
    "    for filename in possible_files:\n",
    "        file_path = project_root / filename\n",
    "        if file_path.exists():\n",
    "            print(f\"📂 Found data file: {filename}\")\n",
    "            df = pd.read_csv(file_path)\n",
    "            loaded_file = filename\n",
    "            break\n",
    "\n",
    "# Check if we successfully loaded data\n",
    "if df is not None:\n",
    "    print(f\"✅ Successfully loaded: {loaded_file}\")\n",
    "    print(f\"📊 Dataset shape: {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
    "else:\n",
    "    print(\"❌ No dataset found. Please ensure you have a CSV file in the data directory.\")\n",
    "    print(\"📝 Expected files: employee_data.csv, employees.csv, hr_data.csv, or data.csv\")\n",
    "\n",
    "# 🔍 Step 2: Quick data examination\n",
    "if df is not None:\n",
    "    print(f\"\\n🔍 QUICK DATA EXAMINATION:\")\n",
    "    print(\"=\"*30)\n",
    "    \n",
    "    print(f\"📋 Column names:\")\n",
    "    for i, col in enumerate(df.columns, 1):\n",
    "        print(f\"  {i:2}. {col}\")\n",
    "    \n",
    "    print(f\"\\n📊 Data types:\")\n",
    "    for col, dtype in df.dtypes.items():\n",
    "        print(f\"  • {col:20} → {dtype}\")\n",
    "    \n",
    "    print(f\"\\n❓ Missing values:\")\n",
    "    missing = df.isnull().sum()\n",
    "    if missing.sum() == 0:\n",
    "        print(\"  ✅ No missing values found!\")\n",
    "    else:\n",
    "        for col, count in missing[missing > 0].items():\n",
    "            print(f\"  • {col:20} → {count:,} missing\")\n",
    "    \n",
    "    print(f\"\\n🔢 Basic statistics:\")\n",
    "    print(f\"  • Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    print(f\"  • Duplicate rows: {df.duplicated().sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018eec3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎯 Step 3: Identify target variable and feature types\n",
    "if df is not None:\n",
    "    print(\"\\n🎯 IDENTIFYING TARGET VARIABLE...\")\n",
    "    print(\"=\"*35)\n",
    "    \n",
    "    # Common target variable names for employee attrition\n",
    "    target_candidates = ['Attrition', 'attrition', 'Left', 'left', 'Turnover', 'turnover', \n",
    "                        'Quit', 'quit', 'Exit', 'exit', 'Churn', 'churn']\n",
    "    \n",
    "    # Look for target variable\n",
    "    target_col = None\n",
    "    for col in df.columns:\n",
    "        if col in target_candidates or any(candidate.lower() in col.lower() for candidate in target_candidates):\n",
    "            target_col = col\n",
    "            break\n",
    "    \n",
    "    # If not found by name, look for binary variables (likely targets)\n",
    "    if target_col is None:\n",
    "        binary_cols = [col for col in df.columns if df[col].nunique() == 2]\n",
    "        if binary_cols:\n",
    "            print(\"🔍 Binary columns found (potential targets):\")\n",
    "            for i, col in enumerate(binary_cols, 1):\n",
    "                values = df[col].unique()\n",
    "                print(f\"  {i}. {col}: {values}\")\n",
    "            \n",
    "            # Assume first binary column is target for now\n",
    "            target_col = binary_cols[0]\n",
    "            print(f\"\\n🎯 Assuming target variable: '{target_col}'\")\n",
    "    \n",
    "    if target_col:\n",
    "        TARGET_COLUMN = target_col\n",
    "        print(f\"✅ Target variable identified: '{TARGET_COLUMN}'\")\n",
    "        \n",
    "        # Show target distribution\n",
    "        target_counts = df[TARGET_COLUMN].value_counts()\n",
    "        print(f\"\\n📊 Target distribution:\")\n",
    "        for value, count in target_counts.items():\n",
    "            percentage = (count / len(df)) * 100\n",
    "            print(f\"  • {value}: {count:,} ({percentage:.1f}%)\")\n",
    "    else:\n",
    "        print(\"❓ Target variable not found automatically.\")\n",
    "        print(\"💡 You may need to specify it manually later.\")\n",
    "    \n",
    "    # 📋 Step 4: Separate feature types\n",
    "    print(f\"\\n📋 SEPARATING FEATURE TYPES...\")\n",
    "    print(\"=\"*30)\n",
    "    \n",
    "    # Get all columns except target\n",
    "    feature_columns = [col for col in df.columns if col != TARGET_COLUMN]\n",
    "    \n",
    "    # Identify numerical features (numbers we can do math with)\n",
    "    numerical_features = df[feature_columns].select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Identify categorical features (text or categories)\n",
    "    categorical_features = df[feature_columns].select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    # Also check for numerical columns that should be treated as categorical\n",
    "    # (like ID numbers, codes with few unique values)\n",
    "    potential_categorical = []\n",
    "    for col in numerical_features:\n",
    "        unique_count = df[col].nunique()\n",
    "        # If less than 10 unique values and they're integers, might be categorical\n",
    "        if unique_count < 10 and df[col].dtype in ['int64', 'int32']:\n",
    "            potential_categorical.append(col)\n",
    "    \n",
    "    print(f\"🔢 Numerical features ({len(numerical_features)}):\")\n",
    "    for i, col in enumerate(numerical_features, 1):\n",
    "        unique_count = df[col].nunique()\n",
    "        print(f\"  {i:2}. {col:20} → {unique_count:,} unique values\")\n",
    "    \n",
    "    print(f\"\\n🏷️ Categorical features ({len(categorical_features)}):\")\n",
    "    for i, col in enumerate(categorical_features, 1):\n",
    "        unique_count = df[col].nunique()\n",
    "        print(f\"  {i:2}. {col:20} → {unique_count:,} unique categories\")\n",
    "    \n",
    "    if potential_categorical:\n",
    "        print(f\"\\n🤔 Potential categorical (currently numerical):\")\n",
    "        for col in potential_categorical:\n",
    "            values = df[col].unique()[:5]  # Show first 5 values\n",
    "            print(f\"  • {col:20} → {values}...\")\n",
    "            print(f\"    💭 Consider: Should this be treated as categorical?\")\n",
    "    \n",
    "    print(f\"\\n📊 Summary:\")\n",
    "    print(f\"  • Total features: {len(feature_columns)}\")\n",
    "    print(f\"  • Numerical: {len(numerical_features)}\")\n",
    "    print(f\"  • Categorical: {len(categorical_features)}\")\n",
    "    if TARGET_COLUMN:\n",
    "        print(f\"  • Target: {TARGET_COLUMN}\")\n",
    "else:\n",
    "    print(\"⚠️ Cannot proceed without data. Please load dataset first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369d7281",
   "metadata": {},
   "source": [
    "## 🔄 Step 2: Data Preprocessing - Cleaning Our Data\n",
    "\n",
    "Before we can engineer features, we need to clean our data! Think of this as **washing the vegetables** 🥬 before cooking - it's essential for a good final result.\n",
    "\n",
    "### 🧹 What we'll clean:\n",
    "- **❌ Missing values**: Fill gaps in our data\n",
    "- **🔄 Duplicates**: Remove identical rows\n",
    "- **📊 Outliers**: Handle extreme values that might confuse our model\n",
    "- **🔧 Data types**: Make sure everything is in the right format\n",
    "\n",
    "### 🎯 Why this matters:\n",
    "- **Clean data = Better models** 📈\n",
    "- **Prevents errors** during training ❌\n",
    "- **Improves accuracy** and reliability ✅\n",
    "- **Makes feature engineering** more effective 🔧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416cd465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔍 Step 1: Handle Missing Values\n",
    "if df is not None:\n",
    "    print(\"🔍 HANDLING MISSING VALUES...\")\n",
    "    print(\"=\"*30)\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_values = df.isnull().sum()\n",
    "    missing_percentage = (missing_values / len(df)) * 100\n",
    "    \n",
    "    print(\"📊 Missing values analysis:\")\n",
    "    has_missing = False\n",
    "    for col in df.columns:\n",
    "        missing_count = missing_values[col]\n",
    "        missing_pct = missing_percentage[col]\n",
    "        \n",
    "        if missing_count > 0:\n",
    "            has_missing = True\n",
    "            print(f\"  • {col:20} → {missing_count:,} missing ({missing_pct:.2f}%)\")\n",
    "    \n",
    "    if not has_missing:\n",
    "        print(\"  ✅ No missing values found! Data is complete.\")\n",
    "    else:\n",
    "        print(f\"\\n🔧 FIXING MISSING VALUES...\")\n",
    "        print(\"=\"*25)\n",
    "        \n",
    "        # Strategy for handling missing values:\n",
    "        # 1. Numerical: Fill with median (robust to outliers)\n",
    "        # 2. Categorical: Fill with mode (most common value) or 'Unknown'\n",
    "        \n",
    "        for col in df.columns:\n",
    "            if missing_values[col] > 0:\n",
    "                missing_count = missing_values[col]\n",
    "                missing_pct = missing_percentage[col]\n",
    "                \n",
    "                print(f\"\\n🔧 Fixing {col} ({missing_count:,} missing, {missing_pct:.1f}%):\")\n",
    "                \n",
    "                if col in numerical_features:\n",
    "                    # For numerical: use median (more robust than mean)\n",
    "                    median_value = df[col].median()\n",
    "                    df[col].fillna(median_value, inplace=True)\n",
    "                    print(f\"  ✅ Filled with median value: {median_value}\")\n",
    "                    \n",
    "                elif col in categorical_features:\n",
    "                    # For categorical: use mode (most common) or 'Unknown'\n",
    "                    if df[col].mode().empty:\n",
    "                        fill_value = 'Unknown'\n",
    "                    else:\n",
    "                        fill_value = df[col].mode()[0]\n",
    "                    \n",
    "                    df[col].fillna(fill_value, inplace=True)\n",
    "                    print(f\"  ✅ Filled with most common value: '{fill_value}'\")\n",
    "        \n",
    "        # Verify all missing values are fixed\n",
    "        remaining_missing = df.isnull().sum().sum()\n",
    "        if remaining_missing == 0:\n",
    "            print(f\"\\n✅ All missing values fixed successfully!\")\n",
    "        else:\n",
    "            print(f\"\\n⚠️ {remaining_missing} missing values remain\")\n",
    "    \n",
    "    # 🔄 Step 2: Handle Duplicate Rows\n",
    "    print(f\"\\n🔄 HANDLING DUPLICATE ROWS...\")\n",
    "    print(\"=\"*27)\n",
    "    \n",
    "    # Check for duplicates\n",
    "    duplicate_count = df.duplicated().sum()\n",
    "    print(f\"📊 Found {duplicate_count:,} duplicate rows\")\n",
    "    \n",
    "    if duplicate_count > 0:\n",
    "        print(f\"🔧 Removing duplicates...\")\n",
    "        \n",
    "        # Keep first occurrence, remove duplicates\n",
    "        original_size = len(df)\n",
    "        df = df.drop_duplicates(keep='first')\n",
    "        new_size = len(df)\n",
    "        removed = original_size - new_size\n",
    "        \n",
    "        print(f\"  ✅ Removed {removed:,} duplicate rows\")\n",
    "        print(f\"  📊 Dataset size: {original_size:,} → {new_size:,} rows\")\n",
    "    else:\n",
    "        print(\"  ✅ No duplicate rows found!\")\n",
    "    \n",
    "    print(f\"\\n📊 CLEANED DATASET SUMMARY:\")\n",
    "    print(\"=\"*30)\n",
    "    print(f\"  • Final shape: {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
    "    print(f\"  • Missing values: {df.isnull().sum().sum()}\")\n",
    "    print(f\"  • Duplicate rows: {df.duplicated().sum()}\")\n",
    "    print(f\"  • Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "else:\n",
    "    print(\"⚠️ No data available for preprocessing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60174a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📊 Step 3: Handle Outliers (Extreme Values)\n",
    "if df is not None and len(numerical_features) > 0:\n",
    "    print(\"📊 HANDLING OUTLIERS...\")\n",
    "    print(\"=\"*20)\n",
    "    \n",
    "    print(\"🔍 What are outliers?\")\n",
    "    print(\"Outliers are data points that are significantly different from others.\")\n",
    "    print(\"Example: If most employees earn $30k-80k, someone earning $500k is an outlier.\")\n",
    "    print()\n",
    "    \n",
    "    # We'll use the IQR (Interquartile Range) method to detect outliers\n",
    "    print(\"📐 Using IQR (Interquartile Range) method:\")\n",
    "    print(\"• Q1 = 25th percentile (25% of data below this)\")\n",
    "    print(\"• Q3 = 75th percentile (75% of data below this)\")\n",
    "    print(\"• IQR = Q3 - Q1 (middle 50% range)\")\n",
    "    print(\"• Outliers: values below Q1-1.5×IQR or above Q3+1.5×IQR\")\n",
    "    print()\n",
    "    \n",
    "    outlier_info = {}\n",
    "    \n",
    "    for col in numerical_features:\n",
    "        print(f\"🔍 Analyzing {col}:\")\n",
    "        \n",
    "        # Calculate IQR boundaries\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        # Define outlier boundaries\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        # Find outliers\n",
    "        outliers_mask = (df[col] < lower_bound) | (df[col] > upper_bound)\n",
    "        outliers_count = outliers_mask.sum()\n",
    "        outliers_percentage = (outliers_count / len(df)) * 100\n",
    "        \n",
    "        print(f\"  • Normal range: {lower_bound:.2f} to {upper_bound:.2f}\")\n",
    "        print(f\"  • Outliers found: {outliers_count:,} ({outliers_percentage:.2f}%)\")\n",
    "        \n",
    "        if outliers_count > 0:\n",
    "            outlier_values = df[outliers_mask][col]\n",
    "            print(f\"  • Outlier range: {outlier_values.min():.2f} to {outlier_values.max():.2f}\")\n",
    "            \n",
    "            # Store outlier info for decision making\n",
    "            outlier_info[col] = {\n",
    "                'count': outliers_count,\n",
    "                'percentage': outliers_percentage,\n",
    "                'lower_bound': lower_bound,\n",
    "                'upper_bound': upper_bound,\n",
    "                'mask': outliers_mask\n",
    "            }\n",
    "        else:\n",
    "            print(f\"  ✅ No outliers detected\")\n",
    "        print()\n",
    "    \n",
    "    # 🔧 Decide how to handle outliers\n",
    "    print(\"🔧 OUTLIER HANDLING STRATEGY:\")\n",
    "    print(\"=\"*30)\n",
    "    \n",
    "    outliers_handled = 0\n",
    "    \n",
    "    for col, info in outlier_info.items():\n",
    "        outlier_percentage = info['percentage']\n",
    "        \n",
    "        print(f\"📊 {col}: {info['count']:,} outliers ({outlier_percentage:.1f}%)\")\n",
    "        \n",
    "        if outlier_percentage > 10:\n",
    "            # Too many outliers - might be legitimate data variation\n",
    "            print(f\"  🤔 High outlier percentage - keeping all values\")\n",
    "            print(f\"  💡 These might represent legitimate data variation\")\n",
    "            \n",
    "        elif outlier_percentage > 5:\n",
    "            # Moderate outliers - cap them (winsorization)\n",
    "            print(f\"  🔧 Capping outliers to boundary values (winsorization)\")\n",
    "            \n",
    "            # Cap outliers to the boundary values\n",
    "            df.loc[df[col] < info['lower_bound'], col] = info['lower_bound']\n",
    "            df.loc[df[col] > info['upper_bound'], col] = info['upper_bound']\n",
    "            \n",
    "            outliers_handled += info['count']\n",
    "            print(f\"  ✅ Capped {info['count']:,} outliers\")\n",
    "            \n",
    "        elif outlier_percentage > 0:\n",
    "            # Few outliers - remove them\n",
    "            print(f\"  🗑️ Removing outlier rows (small percentage)\")\n",
    "            \n",
    "            # Remove rows with outliers in this column\n",
    "            original_size = len(df)\n",
    "            df = df[~info['mask']]\n",
    "            new_size = len(df)\n",
    "            removed = original_size - new_size\n",
    "            \n",
    "            outliers_handled += removed\n",
    "            print(f\"  ✅ Removed {removed:,} rows with outliers\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    if outliers_handled > 0:\n",
    "        print(f\"📊 OUTLIER HANDLING SUMMARY:\")\n",
    "        print(f\"  • Total outliers handled: {outliers_handled:,}\")\n",
    "        print(f\"  • Final dataset size: {len(df):,} rows\")\n",
    "    else:\n",
    "        print(\"✅ No outlier handling needed - data looks good!\")\n",
    "\n",
    "else:\n",
    "    if df is None:\n",
    "        print(\"⚠️ No data available for outlier analysis.\")\n",
    "    else:\n",
    "        print(\"ℹ️ No numerical features found - skipping outlier analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86380d98",
   "metadata": {},
   "source": [
    "## 🏷️ Step 3: Categorical Encoding - Converting Text to Numbers\n",
    "\n",
    "Machine learning algorithms can only work with numbers, not text! 🔢 So we need to convert our categorical features (like \"Sales\", \"Engineering\", \"HR\") into numbers that algorithms can understand.\n",
    "\n",
    "### 🔄 Encoding Methods We'll Use:\n",
    "\n",
    "#### 1️⃣ **Label Encoding** \n",
    "- **What**: Assigns each category a number (0, 1, 2, ...)\n",
    "- **When**: For ordinal data (categories with natural order)\n",
    "- **Example**: Education → High School=0, Bachelor's=1, Master's=2, PhD=3\n",
    "\n",
    "#### 2️⃣ **One-Hot Encoding**\n",
    "- **What**: Creates binary columns (0 or 1) for each category\n",
    "- **When**: For nominal data (no natural order)\n",
    "- **Example**: Department → Dept_Sales=1, Dept_HR=0, Dept_IT=0\n",
    "\n",
    "#### 3️⃣ **Target Encoding** \n",
    "- **What**: Replaces categories with target variable statistics\n",
    "- **When**: High-cardinality features (many categories)\n",
    "- **Example**: Job_Role → average attrition rate for each role\n",
    "\n",
    "### 🎯 Why This Matters:\n",
    "- **🤖 Algorithm Compatibility**: Makes data readable by ML models\n",
    "- **📊 Pattern Recognition**: Helps models find relationships\n",
    "- **⚡ Performance**: Proper encoding improves model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7ac736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🏷️ Step 1: Analyze categorical features for encoding strategy\n",
    "if df is not None and len(categorical_features) > 0:\n",
    "    print(\"🏷️ ANALYZING CATEGORICAL FEATURES...\")\n",
    "    print(\"=\"*38)\n",
    "    \n",
    "    encoding_strategy = {}\n",
    "    \n",
    "    for col in categorical_features:\n",
    "        unique_count = df[col].nunique()\n",
    "        values = df[col].unique()\n",
    "        \n",
    "        print(f\"📊 {col}:\")\n",
    "        print(f\"  • Unique categories: {unique_count}\")\n",
    "        print(f\"  • Categories: {values[:5]}{'...' if len(values) > 5 else ''}\")\n",
    "        \n",
    "        # Decide encoding strategy based on characteristics\n",
    "        if unique_count == 2:\n",
    "            # Binary categorical - use label encoding\n",
    "            strategy = \"Label Encoding (Binary)\"\n",
    "            encoding_strategy[col] = \"label\"\n",
    "            \n",
    "        elif unique_count <= 5:\n",
    "            # Few categories - use one-hot encoding\n",
    "            strategy = \"One-Hot Encoding (Few categories)\"\n",
    "            encoding_strategy[col] = \"onehot\"\n",
    "            \n",
    "        elif unique_count <= 20:\n",
    "            # Moderate categories - check if ordinal or use one-hot\n",
    "            # Check if there's natural ordering (common ordinal patterns)\n",
    "            ordinal_patterns = ['low', 'medium', 'high', 'small', 'large', \n",
    "                              'junior', 'senior', 'beginner', 'intermediate', 'advanced']\n",
    "            \n",
    "            is_ordinal = any(pattern in str(values).lower() for pattern in ordinal_patterns)\n",
    "            \n",
    "            if is_ordinal:\n",
    "                strategy = \"Label Encoding (Ordinal detected)\"\n",
    "                encoding_strategy[col] = \"label\"\n",
    "            else:\n",
    "                strategy = \"One-Hot Encoding (Nominal)\"\n",
    "                encoding_strategy[col] = \"onehot\"\n",
    "                \n",
    "        else:\n",
    "            # Many categories - use target encoding\n",
    "            strategy = \"Target Encoding (High cardinality)\"\n",
    "            encoding_strategy[col] = \"target\"\n",
    "        \n",
    "        print(f\"  🎯 Strategy: {strategy}\")\n",
    "        print()\n",
    "    \n",
    "    # 🔧 Step 2: Apply encoding strategies\n",
    "    print(\"🔧 APPLYING ENCODING STRATEGIES...\")\n",
    "    print(\"=\"*35)\n",
    "    \n",
    "    # Store original columns for reference\n",
    "    original_columns = df.columns.tolist()\n",
    "    encoded_features = []\n",
    "    \n",
    "    # Keep track of encoders for future use\n",
    "    label_encoders = {}\n",
    "    onehot_encoders = {}\n",
    "    \n",
    "    for col, strategy in encoding_strategy.items():\n",
    "        print(f\"🔧 Encoding {col} using {strategy.upper()}...\")\n",
    "        \n",
    "        if strategy == \"label\":\n",
    "            # Label Encoding: Assign numbers to categories\n",
    "            le = LabelEncoder()\n",
    "            df[f'{col}_encoded'] = le.fit_transform(df[col])\n",
    "            label_encoders[col] = le\n",
    "            \n",
    "            # Show mapping\n",
    "            mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "            print(f\"  📋 Mapping: {mapping}\")\n",
    "            encoded_features.append(f'{col}_encoded')\n",
    "            \n",
    "        elif strategy == \"onehot\":\n",
    "            # One-Hot Encoding: Create binary columns\n",
    "            encoded_cols = pd.get_dummies(df[col], prefix=col, prefix_sep='_')\n",
    "            \n",
    "            # Add to dataframe\n",
    "            df = pd.concat([df, encoded_cols], axis=1)\n",
    "            \n",
    "            print(f\"  📋 Created {len(encoded_cols.columns)} binary columns:\")\n",
    "            for new_col in encoded_cols.columns:\n",
    "                print(f\"    • {new_col}\")\n",
    "                encoded_features.append(new_col)\n",
    "                \n",
    "        elif strategy == \"target\" and TARGET_COLUMN:\n",
    "            # Target Encoding: Replace with target statistics\n",
    "            target_mean = df.groupby(col)[TARGET_COLUMN].transform('mean')\n",
    "            df[f'{col}_target_encoded'] = target_mean\n",
    "            \n",
    "            # Show some mappings\n",
    "            category_means = df.groupby(col)[TARGET_COLUMN].mean().sort_values(ascending=False)\n",
    "            print(f\"  📋 Target means (top 5):\")\n",
    "            for cat, mean_val in category_means.head().items():\n",
    "                print(f\"    • {cat}: {mean_val:.3f}\")\n",
    "            encoded_features.append(f'{col}_target_encoded')\n",
    "        \n",
    "        print(f\"  ✅ Encoded successfully!\")\n",
    "        print()\n",
    "    \n",
    "    # 🗑️ Step 3: Remove original categorical columns\n",
    "    print(\"🗑️ CLEANING UP ORIGINAL CATEGORICAL COLUMNS...\")\n",
    "    print(\"=\"*45)\n",
    "    \n",
    "    columns_to_drop = categorical_features.copy()\n",
    "    \n",
    "    print(f\"📋 Removing original categorical columns:\")\n",
    "    for col in columns_to_drop:\n",
    "        print(f\"  • {col}\")\n",
    "    \n",
    "    df = df.drop(columns=columns_to_drop)\n",
    "    \n",
    "    print(f\"\\n📊 ENCODING SUMMARY:\")\n",
    "    print(\"=\"*20)\n",
    "    print(f\"  • Original categorical columns: {len(categorical_features)}\")\n",
    "    print(f\"  • New encoded features: {len(encoded_features)}\")\n",
    "    print(f\"  • Total columns now: {len(df.columns)}\")\n",
    "    \n",
    "    # Update feature lists\n",
    "    categorical_features = []  # No more categorical features\n",
    "    numerical_features = [col for col in df.columns \n",
    "                         if col != TARGET_COLUMN and col not in encoded_features]\n",
    "    all_features = numerical_features + encoded_features\n",
    "    \n",
    "    print(f\"  • Numerical features: {len(numerical_features)}\")\n",
    "    print(f\"  • Encoded features: {len(encoded_features)}\")\n",
    "    print(f\"  • Total features: {len(all_features)}\")\n",
    "\n",
    "else:\n",
    "    if df is None:\n",
    "        print(\"⚠️ No data available for encoding.\")\n",
    "    else:\n",
    "        print(\"ℹ️ No categorical features found - skipping encoding step.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575e9af9",
   "metadata": {},
   "source": [
    "## 📏 Step 4: Feature Scaling - Making Features Comparable\n",
    "\n",
    "Imagine comparing apples and elephants! 🍎🐘 If one feature ranges from 0-1 (like percentages) and another from 1000-100000 (like salaries), the larger numbers will dominate the smaller ones in machine learning algorithms.\n",
    "\n",
    "### 🎯 Why Scale Features?\n",
    "\n",
    "#### 🔍 **The Problem**:\n",
    "- **Age**: 20-65 years\n",
    "- **Salary**: $30,000-$150,000  \n",
    "- **Experience**: 0-40 years\n",
    "\n",
    "The salary values are much larger, so algorithms might think salary is more important just because of the scale!\n",
    "\n",
    "#### ✅ **The Solution**: Scale all features to similar ranges\n",
    "\n",
    "### 📊 Scaling Methods:\n",
    "\n",
    "#### 1️⃣ **StandardScaler (Z-score normalization)**\n",
    "- **Formula**: (value - mean) / standard_deviation\n",
    "- **Result**: Mean=0, Standard deviation=1\n",
    "- **Best for**: Normal distributions, most ML algorithms\n",
    "\n",
    "#### 2️⃣ **MinMaxScaler (Min-Max normalization)** \n",
    "- **Formula**: (value - min) / (max - min)\n",
    "- **Result**: All values between 0 and 1\n",
    "- **Best for**: When you want specific bounds, neural networks\n",
    "\n",
    "### 🤖 Which Algorithms Need Scaling?\n",
    "- **Need scaling**: SVM, Neural Networks, KNN, PCA, Clustering\n",
    "- **Don't need scaling**: Tree-based (Random Forest, XGBoost, Decision Trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afecfd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📏 Step 1: Analyze features before scaling\n",
    "if df is not None and len(numerical_features) > 0:\n",
    "    print(\"📏 ANALYZING FEATURES FOR SCALING...\")\n",
    "    print(\"=\"*35)\n",
    "    \n",
    "    print(\"📊 Current feature ranges (before scaling):\")\n",
    "    for col in numerical_features:\n",
    "        min_val = df[col].min()\n",
    "        max_val = df[col].max()\n",
    "        mean_val = df[col].mean()\n",
    "        std_val = df[col].std()\n",
    "        \n",
    "        print(f\"  • {col:20} → Range: {min_val:8.2f} to {max_val:10.2f}\")\n",
    "        print(f\"    {'':20}   Mean: {mean_val:8.2f}, Std: {std_val:8.2f}\")\n",
    "    \n",
    "    # Check if scaling is needed\n",
    "    ranges = [(df[col].max() - df[col].min()) for col in numerical_features]\n",
    "    max_range = max(ranges)\n",
    "    min_range = min(ranges)\n",
    "    range_ratio = max_range / min_range if min_range > 0 else float('inf')\n",
    "    \n",
    "    print(f\"\\n🔍 Scale Analysis:\")\n",
    "    print(f\"  • Largest range: {max_range:,.2f}\")\n",
    "    print(f\"  • Smallest range: {min_range:,.2f}\")\n",
    "    print(f\"  • Range ratio: {range_ratio:,.1f}:1\")\n",
    "    \n",
    "    if range_ratio > 10:\n",
    "        scaling_needed = True\n",
    "        print(f\"  🚨 Large scale differences detected - scaling recommended!\")\n",
    "    else:\n",
    "        scaling_needed = False\n",
    "        print(f\"  ✅ Scales are relatively similar - scaling optional\")\n",
    "    \n",
    "    # 🔧 Step 2: Apply scaling\n",
    "    print(f\"\\n🔧 APPLYING FEATURE SCALING...\")\n",
    "    print(\"=\"*28)\n",
    "    \n",
    "    if scaling_needed or True:  # We'll scale anyway for demonstration\n",
    "        \n",
    "        # We'll use StandardScaler as it's most common and robust\n",
    "        print(\"📊 Using StandardScaler (z-score normalization):\")\n",
    "        print(\"  • Transforms to: mean=0, std=1\")\n",
    "        print(\"  • Formula: (value - mean) / standard_deviation\")\n",
    "        print(\"  • Good for: Most ML algorithms, normal distributions\")\n",
    "        print()\n",
    "        \n",
    "        # Initialize the scaler\n",
    "        scaler = StandardScaler()\n",
    "        \n",
    "        # Fit and transform numerical features\n",
    "        scaled_features = scaler.fit_transform(df[numerical_features])\n",
    "        \n",
    "        # Create new column names for scaled features\n",
    "        scaled_column_names = [f\"{col}_scaled\" for col in numerical_features]\n",
    "        \n",
    "        # Create DataFrame with scaled features\n",
    "        scaled_df = pd.DataFrame(scaled_features, \n",
    "                                columns=scaled_column_names,\n",
    "                                index=df.index)\n",
    "        \n",
    "        # Add scaled features to main dataframe\n",
    "        df = pd.concat([df, scaled_df], axis=1)\n",
    "        \n",
    "        print(\"✅ Scaling completed! Created scaled versions of numerical features.\")\n",
    "        \n",
    "        # 📊 Step 3: Compare before and after scaling\n",
    "        print(f\"\\n📊 SCALING RESULTS COMPARISON:\")\n",
    "        print(\"=\"*32)\n",
    "        \n",
    "        print(\"BEFORE scaling (original features):\")\n",
    "        for col in numerical_features[:3]:  # Show first 3 for space\n",
    "            mean_val = df[col].mean()\n",
    "            std_val = df[col].std()\n",
    "            min_val = df[col].min()\n",
    "            max_val = df[col].max()\n",
    "            print(f\"  • {col:20} → Mean: {mean_val:8.2f}, Std: {std_val:6.2f}, Range: [{min_val:.2f}, {max_val:.2f}]\")\n",
    "        \n",
    "        print(\"\\nAFTER scaling (scaled features):\")\n",
    "        for col in scaled_column_names[:3]:  # Show first 3 for space\n",
    "            mean_val = df[col].mean()\n",
    "            std_val = df[col].std()\n",
    "            min_val = df[col].min()\n",
    "            max_val = df[col].max()\n",
    "            print(f\"  • {col:20} → Mean: {mean_val:8.2f}, Std: {std_val:6.2f}, Range: [{min_val:.2f}, {max_val:.2f}]\")\n",
    "        \n",
    "        # 📋 Step 4: Update feature lists\n",
    "        print(f\"\\n📋 UPDATING FEATURE INVENTORY...\")\n",
    "        print(\"=\"*30)\n",
    "        \n",
    "        # Update our feature tracking\n",
    "        original_numerical = numerical_features.copy()\n",
    "        scaled_numerical = scaled_column_names.copy()\n",
    "        \n",
    "        print(f\"📊 Feature summary:\")\n",
    "        print(f\"  • Original numerical: {len(original_numerical)}\")\n",
    "        print(f\"  • Scaled numerical: {len(scaled_numerical)}\")\n",
    "        print(f\"  • Encoded features: {len(encoded_features) if 'encoded_features' in locals() else 0}\")\n",
    "        print(f\"  • Total columns: {len(df.columns)}\")\n",
    "        \n",
    "        # For machine learning, we'll use scaled features instead of original\n",
    "        features_for_ml = scaled_numerical + (encoded_features if 'encoded_features' in locals() else [])\n",
    "        \n",
    "        print(f\"\\n🎯 Features ready for ML: {len(features_for_ml)}\")\n",
    "        print(\"💡 We'll use scaled versions for algorithms that need scaling\")\n",
    "        print(\"💡 We'll keep original versions for algorithms that don't need scaling\")\n",
    "        \n",
    "    else:\n",
    "        print(\"ℹ️ Scaling skipped - ranges are already similar\")\n",
    "        features_for_ml = numerical_features + (encoded_features if 'encoded_features' in locals() else [])\n",
    "\n",
    "else:\n",
    "    if df is None:\n",
    "        print(\"⚠️ No data available for scaling.\")\n",
    "    else:\n",
    "        print(\"ℹ️ No numerical features found - skipping scaling step.\")\n",
    "        features_for_ml = encoded_features if 'encoded_features' in locals() else []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78325378",
   "metadata": {},
   "source": [
    "## ⚖️ Step 5: Handle Class Imbalance - Balancing Our Target\n",
    "\n",
    "Class imbalance is like having a basketball team where 90% of players are guards and only 10% are centers! 🏀 When one class (like \"employees who stay\") vastly outnumbers another (like \"employees who leave\"), our model gets biased.\n",
    "\n",
    "### 🔍 **The Problem**:\n",
    "If 95% of employees stay and only 5% leave:\n",
    "- Model learns to always predict \"stay\" \n",
    "- Gets 95% accuracy by just guessing \"stay\" every time!\n",
    "- But fails to identify employees who actually might leave 😞\n",
    "\n",
    "### 🎯 **Solutions We'll Use**:\n",
    "\n",
    "#### 1️⃣ **SMOTE (Synthetic Minority Oversampling)**\n",
    "- **What**: Creates artificial examples of minority class\n",
    "- **How**: Uses existing minority examples to generate new, similar ones\n",
    "- **Pro**: Increases minority class without duplicating exact examples\n",
    "- **Con**: Might create unrealistic combinations\n",
    "\n",
    "#### 2️⃣ **Class Weights**\n",
    "- **What**: Tells algorithm to pay more attention to minority class\n",
    "- **How**: Penalizes wrong predictions on minority class more heavily\n",
    "- **Pro**: No data modification, works with any algorithm\n",
    "- **Con**: Might create more false positives\n",
    "\n",
    "#### 3️⃣ **Undersampling**\n",
    "- **What**: Reduces majority class to match minority class\n",
    "- **How**: Randomly removes majority class examples\n",
    "- **Pro**: Balanced dataset, faster training\n",
    "- **Con**: Loses potentially useful data\n",
    "\n",
    "### 🚀 **When to Use Each**:\n",
    "- **Small dataset** → Class weights or SMOTE\n",
    "- **Large dataset** → Undersampling or SMOTE  \n",
    "- **Very imbalanced** → Combine multiple techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ea16e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⚖️ Step 1: Analyze class balance in our target variable\n",
    "if df is not None and TARGET_COLUMN:\n",
    "    print(\"⚖️ ANALYZING CLASS BALANCE...\")\n",
    "    print(\"=\"*28)\n",
    "    \n",
    "    # Get target distribution\n",
    "    target_counts = df[TARGET_COLUMN].value_counts()\n",
    "    target_percentages = df[TARGET_COLUMN].value_counts(normalize=True) * 100\n",
    "    \n",
    "    print(f\"🎯 Target variable: {TARGET_COLUMN}\")\n",
    "    print(f\"📊 Class distribution:\")\n",
    "    \n",
    "    for value, count in target_counts.items():\n",
    "        percentage = target_percentages[value]\n",
    "        print(f\"  • {value}: {count:,} samples ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Calculate imbalance metrics\n",
    "    minority_class_pct = target_percentages.min()\n",
    "    majority_class_pct = target_percentages.max()\n",
    "    imbalance_ratio = majority_class_pct / minority_class_pct\n",
    "    \n",
    "    print(f\"\\n📏 Imbalance analysis:\")\n",
    "    print(f\"  • Minority class: {minority_class_pct:.1f}%\")\n",
    "    print(f\"  • Majority class: {majority_class_pct:.1f}%\")\n",
    "    print(f\"  • Imbalance ratio: {imbalance_ratio:.1f}:1\")\n",
    "    \n",
    "    # Determine severity and recommend strategy\n",
    "    if imbalance_ratio >= 4:\n",
    "        imbalance_level = \"🔴 Severely imbalanced\"\n",
    "        recommendation = \"SMOTE + Class weights recommended\"\n",
    "        apply_smote = True\n",
    "    elif imbalance_ratio >= 2:\n",
    "        imbalance_level = \"🟡 Moderately imbalanced\"\n",
    "        recommendation = \"SMOTE or class weights\"\n",
    "        apply_smote = True\n",
    "    else:\n",
    "        imbalance_level = \"🟢 Relatively balanced\"\n",
    "        recommendation = \"No special handling needed\"\n",
    "        apply_smote = False\n",
    "    \n",
    "    print(f\"  • Status: {imbalance_level}\")\n",
    "    print(f\"  • Recommendation: {recommendation}\")\n",
    "    \n",
    "    # 🔧 Step 2: Apply SMOTE if needed\n",
    "    if apply_smote:\n",
    "        print(f\"\\n🔧 APPLYING SMOTE...\")\n",
    "        print(\"=\"*18)\n",
    "        \n",
    "        print(\"🔍 What SMOTE does:\")\n",
    "        print(\"  1. Finds minority class samples\")\n",
    "        print(\"  2. Identifies their nearest neighbors\")\n",
    "        print(\"  3. Creates new samples between them\")\n",
    "        print(\"  4. Balances the dataset\")\n",
    "        print()\n",
    "        \n",
    "        # Prepare features and target for SMOTE\n",
    "        if 'features_for_ml' in locals():\n",
    "            X = df[features_for_ml]\n",
    "        else:\n",
    "            # Fallback to all numerical features\n",
    "            X = df.select_dtypes(include=[np.number]).drop(columns=[TARGET_COLUMN], errors='ignore')\n",
    "        \n",
    "        y = df[TARGET_COLUMN]\n",
    "        \n",
    "        print(f\"📊 Before SMOTE:\")\n",
    "        print(f\"  • Dataset shape: {X.shape}\")\n",
    "        original_counts = Counter(y)\n",
    "        for class_val, count in original_counts.items():\n",
    "            print(f\"  • Class {class_val}: {count:,} samples\")\n",
    "        \n",
    "        # Apply SMOTE\n",
    "        smote = SMOTE(random_state=RANDOM_STATE)\n",
    "        \n",
    "        try:\n",
    "            X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "            \n",
    "            print(f\"\\n📊 After SMOTE:\")\n",
    "            print(f\"  • Dataset shape: {X_resampled.shape}\")\n",
    "            new_counts = Counter(y_resampled)\n",
    "            for class_val, count in new_counts.items():\n",
    "                print(f\"  • Class {class_val}: {count:,} samples\")\n",
    "            \n",
    "            # Calculate how many synthetic samples were created\n",
    "            total_original = len(y)\n",
    "            total_new = len(y_resampled)\n",
    "            synthetic_created = total_new - total_original\n",
    "            \n",
    "            print(f\"\\n✅ SMOTE Results:\")\n",
    "            print(f\"  • Original samples: {total_original:,}\")\n",
    "            print(f\"  • Synthetic samples created: {synthetic_created:,}\")\n",
    "            print(f\"  • Total samples: {total_new:,}\")\n",
    "            print(f\"  • New balance ratio: 1:1 (perfectly balanced)\")\n",
    "            \n",
    "            # Create balanced dataframe\n",
    "            df_balanced = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "            df_balanced[TARGET_COLUMN] = y_resampled\n",
    "            \n",
    "            print(f\"\\n💾 Created balanced dataset: df_balanced\")\n",
    "            print(f\"   Use this for training models that benefit from balanced data\")\n",
    "            print(f\"   Keep original df for models that handle imbalance well\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ SMOTE failed: {str(e)}\")\n",
    "            print(f\"💡 This might happen if:\")\n",
    "            print(f\"   • Too few minority samples\")\n",
    "            print(f\"   • Features need more preprocessing\")\n",
    "            print(f\"   • Data has other issues\")\n",
    "            df_balanced = df.copy()\n",
    "            \n",
    "    else:\n",
    "        print(f\"\\nℹ️ Class balance is acceptable - no SMOTE needed\")\n",
    "        df_balanced = df.copy()\n",
    "    \n",
    "    # 📊 Step 3: Visualize class distribution\n",
    "    print(f\"\\n📊 VISUALIZING CLASS DISTRIBUTIONS...\")\n",
    "    print(\"=\"*35)\n",
    "    \n",
    "    # Create comparison plot\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    fig.suptitle('🎯 Class Distribution: Before vs After Balancing', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Original distribution\n",
    "    target_counts.plot(kind='bar', ax=axes[0], color=['lightcoral', 'lightblue'], alpha=0.7)\n",
    "    axes[0].set_title('📊 Original Distribution', fontweight='bold')\n",
    "    axes[0].set_xlabel('Target Class')\n",
    "    axes[0].set_ylabel('Number of Samples')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add percentage labels\n",
    "    for i, (class_val, count) in enumerate(target_counts.items()):\n",
    "        pct = target_percentages[class_val]\n",
    "        axes[0].text(i, count + count*0.01, f'{pct:.1f}%', \n",
    "                    ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Balanced distribution\n",
    "    if 'df_balanced' in locals():\n",
    "        balanced_counts = df_balanced[TARGET_COLUMN].value_counts()\n",
    "        balanced_counts.plot(kind='bar', ax=axes[1], color=['lightgreen', 'lightyellow'], alpha=0.7)\n",
    "        axes[1].set_title('⚖️ After Balancing', fontweight='bold')\n",
    "        axes[1].set_xlabel('Target Class')\n",
    "        axes[1].set_ylabel('Number of Samples')\n",
    "        axes[1].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add count labels\n",
    "        for i, (class_val, count) in enumerate(balanced_counts.items()):\n",
    "            axes[1].text(i, count + count*0.01, f'{count:,}', \n",
    "                        ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    if df is None:\n",
    "        print(\"⚠️ No data available for class balance analysis.\")\n",
    "    else:\n",
    "        print(\"⚠️ No target variable identified - skipping class balance analysis.\")\n",
    "        print(\"💡 You may need to specify the target variable manually.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961c20b1",
   "metadata": {},
   "source": [
    "## 🎉 Step 6: Final Summary - Our Data is Ready!\n",
    "\n",
    "Congratulations! 🎊 You've successfully transformed raw data into machine learning-ready features. Let's see what we've accomplished and prepare for the next steps.\n",
    "\n",
    "### ✅ **What We've Accomplished:**\n",
    "\n",
    "#### 🔄 **Data Preprocessing**\n",
    "- ✅ Handled missing values with appropriate strategies\n",
    "- ✅ Removed duplicate rows\n",
    "- ✅ Detected and handled outliers\n",
    "- ✅ Ensured data quality and consistency\n",
    "\n",
    "#### 🏷️ **Categorical Encoding** \n",
    "- ✅ Converted text categories to numbers\n",
    "- ✅ Applied appropriate encoding strategies\n",
    "- ✅ Created machine learning-compatible features\n",
    "\n",
    "#### 📏 **Feature Scaling**\n",
    "- ✅ Standardized numerical features\n",
    "- ✅ Made features comparable in scale\n",
    "- ✅ Prepared features for scale-sensitive algorithms\n",
    "\n",
    "#### ⚖️ **Class Balancing**\n",
    "- ✅ Analyzed target variable distribution\n",
    "- ✅ Applied SMOTE for balanced training data\n",
    "- ✅ Created both balanced and original datasets\n",
    "\n",
    "### 🚀 **Next Steps in Our ML Journey:**\n",
    "\n",
    "1. **📊 Model Training** (Next Notebook)\n",
    "   - Train multiple algorithms\n",
    "   - Compare performance on balanced vs original data\n",
    "   - Hyperparameter tuning\n",
    "\n",
    "2. **📈 Model Evaluation** \n",
    "   - Detailed performance metrics\n",
    "   - Feature importance analysis\n",
    "   - Model interpretability\n",
    "\n",
    "3. **🎯 Model Selection**\n",
    "   - Choose best performing model\n",
    "   - Validate on test data\n",
    "   - Prepare for deployment\n",
    "\n",
    "### 💾 **Datasets Available:**\n",
    "- **`df`**: Original preprocessed data\n",
    "- **`df_balanced`**: SMOTE-balanced data  \n",
    "- **Ready for multiple ML algorithms!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4752fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📊 Final Data Summary and Export\n",
    "print(\"📊 FINAL FEATURE ENGINEERING SUMMARY\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "if df is not None:\n",
    "    # Dataset overview\n",
    "    print(f\"🗃️ DATASET OVERVIEW:\")\n",
    "    print(f\"  • Original dataset: {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
    "    \n",
    "    if 'df_balanced' in locals():\n",
    "        print(f\"  • Balanced dataset: {df_balanced.shape[0]:,} rows × {df_balanced.shape[1]} columns\")\n",
    "    \n",
    "    # Feature summary\n",
    "    if 'features_for_ml' in locals():\n",
    "        print(f\"\\n🔧 ENGINEERED FEATURES:\")\n",
    "        print(f\"  • Total ML-ready features: {len(features_for_ml)}\")\n",
    "        \n",
    "        if 'scaled_numerical' in locals():\n",
    "            print(f\"  • Scaled numerical features: {len(scaled_numerical)}\")\n",
    "            \n",
    "        if 'encoded_features' in locals():\n",
    "            print(f\"  • Encoded categorical features: {len(encoded_features)}\")\n",
    "        \n",
    "        print(f\"  • Target variable: {TARGET_COLUMN}\")\n",
    "    \n",
    "    # Data quality summary\n",
    "    print(f\"\\n✅ DATA QUALITY:\")\n",
    "    print(f\"  • Missing values: {df.isnull().sum().sum()}\")\n",
    "    print(f\"  • Duplicate rows: {df.duplicated().sum()}\")\n",
    "    print(f\"  • Data types: All compatible with ML\")\n",
    "    \n",
    "    # Memory usage\n",
    "    memory_mb = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    print(f\"  • Memory usage: {memory_mb:.2f} MB\")\n",
    "    \n",
    "    # Target distribution\n",
    "    if TARGET_COLUMN:\n",
    "        print(f\"\\n🎯 TARGET DISTRIBUTION:\")\n",
    "        target_dist = df[TARGET_COLUMN].value_counts(normalize=True) * 100\n",
    "        for value, pct in target_dist.items():\n",
    "            print(f\"  • {value}: {pct:.1f}%\")\n",
    "        \n",
    "        if 'df_balanced' in locals():\n",
    "            print(f\"\\n⚖️ BALANCED TARGET DISTRIBUTION:\")\n",
    "            balanced_dist = df_balanced[TARGET_COLUMN].value_counts(normalize=True) * 100\n",
    "            for value, pct in balanced_dist.items():\n",
    "                print(f\"  • {value}: {pct:.1f}%\")\n",
    "    \n",
    "    # 💾 Step 2: Save processed datasets\n",
    "    print(f\"\\n💾 SAVING PROCESSED DATASETS...\")\n",
    "    print(\"=\"*30)\n",
    "    \n",
    "    # Create processed data directory\n",
    "    processed_dir = project_root / 'data' / 'processed'\n",
    "    processed_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # Save original processed dataset\n",
    "        original_path = processed_dir / 'feature_engineered_data.csv'\n",
    "        df.to_csv(original_path, index=False)\n",
    "        print(f\"✅ Saved original processed data: feature_engineered_data.csv\")\n",
    "        \n",
    "        # Save balanced dataset if it exists\n",
    "        if 'df_balanced' in locals():\n",
    "            balanced_path = processed_dir / 'balanced_data.csv'\n",
    "            df_balanced.to_csv(balanced_path, index=False)\n",
    "            print(f\"✅ Saved balanced data: balanced_data.csv\")\n",
    "        \n",
    "        # Save feature lists for future reference\n",
    "        feature_info = {\n",
    "            'target_column': TARGET_COLUMN,\n",
    "            'ml_features': features_for_ml if 'features_for_ml' in locals() else [],\n",
    "            'scaled_features': scaled_numerical if 'scaled_numerical' in locals() else [],\n",
    "            'encoded_features': encoded_features if 'encoded_features' in locals() else [],\n",
    "            'original_numerical': original_numerical if 'original_numerical' in locals() else []\n",
    "        }\n",
    "        \n",
    "        import json\n",
    "        feature_info_path = processed_dir / 'feature_info.json'\n",
    "        with open(feature_info_path, 'w') as f:\n",
    "            json.dump(feature_info, f, indent=2)\n",
    "        print(f\"✅ Saved feature info: feature_info.json\")\n",
    "        \n",
    "        print(f\"\\n📂 Files saved to: {processed_dir}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error saving files: {str(e)}\")\n",
    "        print(f\"💡 Check directory permissions and disk space\")\n",
    "    \n",
    "    # 🎯 Step 3: Prepare for next notebook\n",
    "    print(f\"\\n🎯 READY FOR MACHINE LEARNING!\")\n",
    "    print(\"=\"*30)\n",
    "    print(\"Your data is now ready for model training!\")\n",
    "    print()\n",
    "    print(\"📋 What to use for different algorithms:\")\n",
    "    print(\"  • Tree-based (Random Forest, XGBoost): Use original features\")\n",
    "    print(\"  • Linear models (Logistic Regression): Use scaled features\")\n",
    "    print(\"  • Neural Networks: Use scaled features\")\n",
    "    print(\"  • Imbalanced-sensitive algorithms: Use balanced dataset\")\n",
    "    print()\n",
    "    print(\"🚀 Next step: Open 03_class_imbalance_analysis.ipynb\")\n",
    "    print(\"   Or jump to 04_model_training.ipynb to start training!\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No data available for summary.\")\n",
    "    print(\"🔄 Please run the previous cells to load and process data.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
