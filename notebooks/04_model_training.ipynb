{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38d2cfb1",
   "metadata": {},
   "source": [
    "# ü§ñ Model Training: Building Our Machine Learning Models\n",
    "\n",
    "Welcome to the **Model Training** phase! üöÄ This is where we transform our prepared data into intelligent predictive models. Think of this as training a team of specialists, each with different strengths and approaches.\n",
    "\n",
    "## üéØ What is Model Training?\n",
    "\n",
    "Model training is like teaching different students the same subject using their preferred learning styles:\n",
    "- **üìä Logistic Regression**: The mathematician who finds linear relationships\n",
    "- **üå≥ Random Forest**: The committee that votes on decisions  \n",
    "- **‚ö° XGBoost**: The iterative learner who learns from mistakes\n",
    "- **üß† Neural Network**: The pattern recognizer with multiple layers\n",
    "\n",
    "### üèÜ **Our Model Training Strategy:**\n",
    "\n",
    "#### 1Ô∏è‚É£ **Multiple Algorithm Testing**\n",
    "- Train 4-6 different algorithms\n",
    "- Compare their strengths and weaknesses\n",
    "- Find the best performer for our specific data\n",
    "\n",
    "#### 2Ô∏è‚É£ **Proper Validation**\n",
    "- Use cross-validation for robust evaluation\n",
    "- Separate train/validation/test sets\n",
    "- Ensure models generalize well\n",
    "\n",
    "#### 3Ô∏è‚É£ **Hyperparameter Tuning**\n",
    "- Optimize model settings for best performance\n",
    "- Use systematic search methods\n",
    "- Balance performance vs. overfitting\n",
    "\n",
    "#### 4Ô∏è‚É£ **Imbalanced Data Handling**\n",
    "- Test with balanced and original datasets\n",
    "- Use appropriate evaluation metrics\n",
    "- Apply class weights where beneficial\n",
    "\n",
    "### üìö **What We'll Accomplish:**\n",
    "\n",
    "#### üîß **Model Implementation**\n",
    "- Logistic Regression (baseline linear model)\n",
    "- Random Forest (ensemble tree method)\n",
    "- Gradient Boosting (XGBoost)\n",
    "- Support Vector Machine (SVM)\n",
    "\n",
    "#### üìä **Performance Analysis**\n",
    "- Accuracy, Precision, Recall, F1-Score\n",
    "- ROC-AUC and Precision-Recall AUC\n",
    "- Confusion matrices and classification reports\n",
    "- Feature importance analysis\n",
    "\n",
    "#### üéõÔ∏è **Optimization**\n",
    "- Grid search for hyperparameters\n",
    "- Cross-validation for robust evaluation\n",
    "- Model comparison and selection\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Ready to Train?\n",
    "\n",
    "By the end of this notebook, you'll have:\n",
    "- ‚úÖ **Trained multiple ML models** with proper validation\n",
    "- ‚úÖ **Optimized hyperparameters** for best performance  \n",
    "- ‚úÖ **Compared model performance** comprehensively\n",
    "- ‚úÖ **Selected the best model** for your use case\n",
    "- ‚úÖ **Understood feature importance** and model interpretability\n",
    "\n",
    "Let's build some intelligent models! üèóÔ∏èü§ñ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b49465b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ IMPORTING MACHINE LEARNING LIBRARIES...\n",
      "==========================================\n",
      "‚úÖ XGBoost available\n",
      "\n",
      "üìÅ SETTING UP PROJECT STRUCTURE...\n",
      "===================================\n",
      "üìÇ Project root: c:\\Users\\DELL\\Desktop\\AI-Project\\AI-Project\n",
      "üìÇ Models directory: c:\\Users\\DELL\\Desktop\\AI-Project\\AI-Project\\models\n",
      "üìÇ Results directory: c:\\Users\\DELL\\Desktop\\AI-Project\\AI-Project\\results\n",
      "\n",
      "üéØ SETTING TRAINING PARAMETERS...\n",
      "==============================\n",
      "üé≤ Random state: 42\n",
      "üìä Test set size: 20%\n",
      "üìä Validation size: 20%\n",
      "üîÑ Cross-validation folds: 5\n",
      "\n",
      "‚úÖ Setup complete! Ready to train models.\n"
     ]
    }
   ],
   "source": [
    "# üì¶ Step 1: Import All Machine Learning Libraries\n",
    "print(\"üì¶ IMPORTING MACHINE LEARNING LIBRARIES...\")\n",
    "print(\"=\"*42)\n",
    "\n",
    "# Core data science libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up beautiful visualizations\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Machine Learning Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# XGBoost - Advanced gradient boosting\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    from xgboost import XGBClassifier\n",
    "    print(\"‚úÖ XGBoost available\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è XGBoost not available - will skip XGBoost models\")\n",
    "    xgb = None\n",
    "\n",
    "# Model Selection and Validation\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    cross_val_score,\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    StratifiedKFold\n",
    ")\n",
    "\n",
    "# Evaluation Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_curve,\n",
    "    precision_recall_curve,\n",
    "    auc\n",
    ")\n",
    "\n",
    "# Data Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Utilities\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# üìÅ Step 2: Set up project structure\n",
    "print(\"\\nüìÅ SETTING UP PROJECT STRUCTURE...\")\n",
    "print(\"=\"*35)\n",
    "\n",
    "current_dir = Path.cwd()\n",
    "project_root = current_dir.parent\n",
    "src_path = project_root / 'src'\n",
    "sys.path.append(str(src_path))\n",
    "\n",
    "# Create directories for saving models\n",
    "models_dir = project_root / 'models'\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "results_dir = project_root / 'results'\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"üìÇ Project root: {project_root}\")\n",
    "print(f\"üìÇ Models directory: {models_dir}\")\n",
    "print(f\"üìÇ Results directory: {results_dir}\")\n",
    "\n",
    "# üéØ Step 3: Set training parameters\n",
    "print(f\"\\nüéØ SETTING TRAINING PARAMETERS...\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# Global parameters\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "VALIDATION_SIZE = 0.2\n",
    "CV_FOLDS = 5\n",
    "\n",
    "# Performance tracking\n",
    "model_performance = {}\n",
    "training_times = {}\n",
    "\n",
    "print(f\"üé≤ Random state: {RANDOM_STATE}\")\n",
    "print(f\"üìä Test set size: {TEST_SIZE*100:.0f}%\")\n",
    "print(f\"üìä Validation size: {VALIDATION_SIZE*100:.0f}%\")\n",
    "print(f\"üîÑ Cross-validation folds: {CV_FOLDS}\")\n",
    "\n",
    "print(\"\\n‚úÖ Setup complete! Ready to train models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d126b0",
   "metadata": {},
   "source": [
    "## üìä Step 1: Data Preparation for Training\n",
    "\n",
    "Before training our models, we need to properly prepare our data. This includes loading our processed features, splitting data, and ensuring everything is ready for machine learning algorithms.\n",
    "\n",
    "### üîß **Data Preparation Steps:**\n",
    "- **üìÅ Load processed data** from feature engineering\n",
    "- **üéØ Identify features and target** variables\n",
    "- **‚úÇÔ∏è Split data** into train/validation/test sets\n",
    "- **üìè Scale features** where necessary\n",
    "- **‚öñÔ∏è Handle class imbalance** appropriately\n",
    "\n",
    "### üìã **Data Splitting Strategy:**\n",
    "- **60% Training**: To teach the models\n",
    "- **20% Validation**: To tune hyperparameters  \n",
    "- **20% Testing**: To evaluate final performance\n",
    "\n",
    "### üé≤ **Why Proper Splitting Matters:**\n",
    "- **Prevents data leakage** between sets\n",
    "- **Ensures fair evaluation** of model performance\n",
    "- **Enables reliable hyperparameter tuning**\n",
    "- **Provides unbiased final assessment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7cce4d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä LOADING AND PREPARING DATA...\n",
      "================================\n",
      "‚úÖ Loaded feature engineered data: (11413, 28)\n",
      "‚úÖ Loaded feature information\n",
      "\n",
      "üéØ IDENTIFYING FEATURES AND TARGET...\n",
      "===================================\n",
      "üéØ Target variable: quit\n",
      "üìä Features for ML: 20\n",
      "‚úÖ Data prepared: X(11413, 20), y(11413,)\n",
      "\n",
      "üìä Class distribution:\n",
      "  ‚Ä¢ 0: 9,430 (82.6%)\n",
      "  ‚Ä¢ 1: 1,983 (17.4%)\n",
      "\n",
      "‚úÇÔ∏è SPLITTING DATA...\n",
      "==================\n",
      "üìä Data splits:\n",
      "  ‚Ä¢ Training: 6,847 samples (60.0%)\n",
      "  ‚Ä¢ Validation: 2,283 samples (20.0%)\n",
      "  ‚Ä¢ Test: 2,283 samples (20.0%)\n",
      "\n",
      "‚úÖ Class distribution maintained:\n",
      "  ‚Ä¢ Train: 82.6% / 17.4%\n",
      "  ‚Ä¢ Val: 82.6% / 17.4%\n",
      "  ‚Ä¢ Test: 82.6% / 17.4%\n",
      "\n",
      "üìè FEATURE SCALING...\n",
      "==================\n",
      "üìä Feature scale analysis:\n",
      "  ‚Ä¢ Max range: 4.38\n",
      "  ‚Ä¢ Min range: 0.00\n",
      "  ‚Ä¢ Range ratio: inf:1\n",
      "üîß Applying StandardScaler...\n",
      "‚úÖ Features scaled successfully\n",
      "\n",
      "üéØ READY FOR MODEL TRAINING!\n",
      "==============================\n",
      "‚úÖ Features prepared: 20 features\n",
      "‚úÖ Data split: Train/Val/Test ready\n",
      "‚úÖ Scaling: Applied\n",
      "‚úÖ Target: quit identified\n"
     ]
    }
   ],
   "source": [
    "# üìä Step 1: Load and prepare data for training\n",
    "print(\"üìä LOADING AND PREPARING DATA...\")\n",
    "print(\"=\"*32)\n",
    "\n",
    "# Load processed data\n",
    "processed_dir = project_root / 'data' / 'processed'\n",
    "data_loaded = False\n",
    "\n",
    "# Try to load feature engineered data\n",
    "try:\n",
    "    df_path = processed_dir / 'feature_engineered_data.csv'\n",
    "    if df_path.exists():\n",
    "        df = pd.read_csv(df_path)\n",
    "        print(f\"‚úÖ Loaded feature engineered data: {df.shape}\")\n",
    "        data_loaded = True\n",
    "        \n",
    "        # Try to load feature info\n",
    "        feature_info_path = processed_dir / 'feature_info.json'\n",
    "        if feature_info_path.exists():\n",
    "            with open(feature_info_path, 'r') as f:\n",
    "                feature_info = json.load(f)\n",
    "            print(f\"‚úÖ Loaded feature information\")\n",
    "        else:\n",
    "            feature_info = None\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading processed data: {e}\")\n",
    "\n",
    "# Fallback to raw data if needed\n",
    "if not data_loaded:\n",
    "    print(\"üîÑ Falling back to raw data loading...\")\n",
    "    # Add raw data loading logic here if needed\n",
    "\n",
    "if data_loaded:\n",
    "    # üéØ Step 2: Identify features and target\n",
    "    print(f\"\\nüéØ IDENTIFYING FEATURES AND TARGET...\")\n",
    "    print(\"=\"*35)\n",
    "    \n",
    "    # Get target column\n",
    "    if feature_info and 'target_column' in feature_info:\n",
    "        target_col = feature_info['target_column']\n",
    "    else:\n",
    "        # Auto-detect target\n",
    "        target_candidates = ['Attrition', 'attrition', 'Left', 'left']\n",
    "        target_col = None\n",
    "        for col in df.columns:\n",
    "            if col in target_candidates or any(candidate.lower() in col.lower() for candidate in target_candidates):\n",
    "                target_col = col\n",
    "                break\n",
    "        \n",
    "        if target_col is None:\n",
    "            binary_cols = [col for col in df.columns if df[col].nunique() == 2]\n",
    "            if binary_cols:\n",
    "                target_col = binary_cols[0]\n",
    "    \n",
    "    if target_col:\n",
    "        print(f\"üéØ Target variable: {target_col}\")\n",
    "        \n",
    "        # Get features for ML\n",
    "        if feature_info and 'ml_features' in feature_info:\n",
    "            feature_columns = feature_info['ml_features']\n",
    "        else:\n",
    "            # Use all numerical columns except target\n",
    "            feature_columns = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "            if target_col in feature_columns:\n",
    "                feature_columns.remove(target_col)\n",
    "        \n",
    "        print(f\"üìä Features for ML: {len(feature_columns)}\")\n",
    "        \n",
    "        # Prepare X and y\n",
    "        X = df[feature_columns]\n",
    "        y = df[target_col]\n",
    "        \n",
    "        print(f\"‚úÖ Data prepared: X{X.shape}, y{y.shape}\")\n",
    "        \n",
    "        # Check class distribution\n",
    "        class_distribution = y.value_counts()\n",
    "        print(f\"\\nüìä Class distribution:\")\n",
    "        for class_val, count in class_distribution.items():\n",
    "            percentage = (count / len(y)) * 100\n",
    "            print(f\"  ‚Ä¢ {class_val}: {count:,} ({percentage:.1f}%)\")\n",
    "        \n",
    "        # ‚úÇÔ∏è Step 3: Split data into train/validation/test\n",
    "        print(f\"\\n‚úÇÔ∏è SPLITTING DATA...\")\n",
    "        print(\"=\"*18)\n",
    "        \n",
    "        # First split: separate test set\n",
    "        X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "            X, y, test_size=TEST_SIZE, \n",
    "            random_state=RANDOM_STATE, \n",
    "            stratify=y\n",
    "        )\n",
    "        \n",
    "        # Second split: separate train and validation\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_temp, y_temp, \n",
    "            test_size=VALIDATION_SIZE/(1-TEST_SIZE),  # Adjust for remaining data\n",
    "            random_state=RANDOM_STATE, \n",
    "            stratify=y_temp\n",
    "        )\n",
    "        \n",
    "        print(f\"üìä Data splits:\")\n",
    "        print(f\"  ‚Ä¢ Training: {X_train.shape[0]:,} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "        print(f\"  ‚Ä¢ Validation: {X_val.shape[0]:,} samples ({X_val.shape[0]/len(X)*100:.1f}%)\")\n",
    "        print(f\"  ‚Ä¢ Test: {X_test.shape[0]:,} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "        \n",
    "        # Verify stratification worked\n",
    "        print(f\"\\n‚úÖ Class distribution maintained:\")\n",
    "        for dataset_name, y_split in [(\"Train\", y_train), (\"Val\", y_val), (\"Test\", y_test)]:\n",
    "            dist = y_split.value_counts(normalize=True) * 100\n",
    "            print(f\"  ‚Ä¢ {dataset_name}: {dist.iloc[0]:.1f}% / {dist.iloc[1]:.1f}%\")\n",
    "        \n",
    "        # üìè Step 4: Scale features if needed\n",
    "        print(f\"\\nüìè FEATURE SCALING...\")\n",
    "        print(\"=\"*18)\n",
    "        \n",
    "        # Check if we need scaling (for algorithms that require it)\n",
    "        # Only calculate ranges for numerical columns\n",
    "        numerical_cols = X_train.select_dtypes(include=[np.number]).columns\n",
    "        if len(numerical_cols) > 0:\n",
    "            feature_ranges = X_train[numerical_cols].max() - X_train[numerical_cols].min()\n",
    "            max_range = feature_ranges.max()\n",
    "            min_range = feature_ranges.min()\n",
    "            range_ratio = max_range / min_range if min_range > 0 else float('inf')\n",
    "        else:\n",
    "            range_ratio = 1.0\n",
    "            max_range = 0.0\n",
    "            min_range = 0.0\n",
    "        \n",
    "        print(f\"üìä Feature scale analysis:\")\n",
    "        print(f\"  ‚Ä¢ Max range: {max_range:.2f}\")\n",
    "        print(f\"  ‚Ä¢ Min range: {min_range:.2f}\")\n",
    "        print(f\"  ‚Ä¢ Range ratio: {range_ratio:.1f}:1\")\n",
    "        \n",
    "        if range_ratio > 10:\n",
    "            print(f\"üîß Applying StandardScaler...\")\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_val_scaled = scaler.transform(X_val)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            \n",
    "            # Convert back to DataFrames\n",
    "            X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "            X_val_scaled = pd.DataFrame(X_val_scaled, columns=X_val.columns, index=X_val.index)\n",
    "            X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "            \n",
    "            print(f\"‚úÖ Features scaled successfully\")\n",
    "            scaling_applied = True\n",
    "        else:\n",
    "            print(f\"‚ÑπÔ∏è Scaling not necessary - features are similarly scaled\")\n",
    "            X_train_scaled = X_train.copy()\n",
    "            X_val_scaled = X_val.copy()\n",
    "            X_test_scaled = X_test.copy()\n",
    "            scaler = None\n",
    "            scaling_applied = False\n",
    "        \n",
    "        print(f\"\\nüéØ READY FOR MODEL TRAINING!\")\n",
    "        print(\"=\"*30)\n",
    "        print(f\"‚úÖ Features prepared: {X_train.shape[1]} features\")\n",
    "        print(f\"‚úÖ Data split: Train/Val/Test ready\")\n",
    "        print(f\"‚úÖ Scaling: {'Applied' if scaling_applied else 'Not needed'}\")\n",
    "        print(f\"‚úÖ Target: {target_col} identified\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå Could not identify target variable\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå No data available for training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7e6373",
   "metadata": {},
   "source": [
    "## ü§ñ Step 2: Model Definition and Training\n",
    "\n",
    "Now let's define our ensemble of machine learning models! We'll train multiple algorithms to find the best performer for our specific dataset and problem.\n",
    "\n",
    "### üéØ **Our Model Arsenal:**\n",
    "\n",
    "#### üìä **Logistic Regression**\n",
    "- **Type**: Linear classifier\n",
    "- **Strengths**: Fast, interpretable, probabilistic outputs\n",
    "- **Best for**: Linear relationships, baseline model\n",
    "- **Scaling needed**: Yes\n",
    "\n",
    "#### üå≥ **Random Forest**\n",
    "- **Type**: Ensemble of decision trees\n",
    "- **Strengths**: Handles non-linear patterns, feature importance\n",
    "- **Best for**: Robust performance, mixed data types\n",
    "- **Scaling needed**: No\n",
    "\n",
    "#### ‚ö° **XGBoost**\n",
    "- **Type**: Gradient boosting ensemble\n",
    "- **Strengths**: High performance, handles missing values\n",
    "- **Best for**: Competitions, complex patterns\n",
    "- **Scaling needed**: No\n",
    "\n",
    "#### üß† **Support Vector Machine**\n",
    "- **Type**: Margin-based classifier\n",
    "- **Strengths**: Good with high dimensions, kernel tricks\n",
    "- **Best for**: Text data, complex boundaries\n",
    "- **Scaling needed**: Yes\n",
    "\n",
    "### üîß **Training Strategy:**\n",
    "- **Cross-validation** for robust performance estimation\n",
    "- **Hyperparameter tuning** for optimal settings\n",
    "- **Class weight balancing** for imbalanced data\n",
    "- **Performance tracking** for comprehensive comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa9c4b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ DEFINING MODEL ENSEMBLE...\n",
      "===========================\n",
      "‚öñÔ∏è Class weights calculated: {0: 0.605072463768116, 1: 2.8793103448275863}\n",
      "üìä Models defined: 5 algorithms\n",
      "  ‚Ä¢ Logistic Regression: Linear classifier with balanced class weights (Requires scaling)\n",
      "  ‚Ä¢ Random Forest: Ensemble of 100 decision trees (No scaling needed)\n",
      "  ‚Ä¢ Gradient Boosting: Sequential boosting algorithm (No scaling needed)\n",
      "  ‚Ä¢ Support Vector Machine: Margin-based classifier with RBF kernel (Requires scaling)\n",
      "  ‚Ä¢ XGBoost: Advanced gradient boosting (No scaling needed)\n",
      "\n",
      "üöÄ TRAINING ALL MODELS...\n",
      "========================\n",
      "\n",
      "üîß Training Logistic Regression...\n",
      "  üìè Using scaled features\n",
      "  ‚úÖ Success! Training time: 0.01s\n",
      "     Validation F1: 0.6043, ROC-AUC: 0.8508\n",
      "\n",
      "üîß Training Random Forest...\n",
      "  üìä Using original features\n",
      "  ‚úÖ Success! Training time: 0.24s\n",
      "     Validation F1: 0.9407, ROC-AUC: 0.9834\n",
      "\n",
      "üîß Training Gradient Boosting...\n",
      "  üìä Using original features\n",
      "  ‚úÖ Success! Training time: 0.63s\n",
      "     Validation F1: 0.9354, ROC-AUC: 0.9865\n",
      "\n",
      "üîß Training Support Vector Machine...\n",
      "  üìè Using scaled features\n",
      "  ‚úÖ Success! Training time: 2.84s\n",
      "     Validation F1: 0.8387, ROC-AUC: 0.9640\n",
      "\n",
      "üîß Training XGBoost...\n",
      "  üìä Using original features\n",
      "  ‚úÖ Success! Training time: 0.11s\n",
      "     Validation F1: 0.9425, ROC-AUC: 0.9872\n",
      "\n",
      "üìä CROSS-VALIDATION EVALUATION...\n",
      "================================\n",
      "\n",
      "üîÑ CV for Logistic Regression...\n",
      "  ‚Ä¢ accuracy: 0.8005 (¬±0.0100)\n",
      "  ‚Ä¢ precision: 0.4617 (¬±0.0136)\n",
      "  ‚Ä¢ recall: 0.8865 (¬±0.0165)\n",
      "  ‚Ä¢ f1: 0.6070 (¬±0.0118)\n",
      "  ‚Ä¢ roc_auc: 0.8597 (¬±0.0091)\n",
      "\n",
      "üîÑ CV for Random Forest...\n",
      "  ‚Ä¢ accuracy: 0.9816 (¬±0.0029)\n",
      "  ‚Ä¢ precision: 0.9863 (¬±0.0080)\n",
      "  ‚Ä¢ recall: 0.9067 (¬±0.0146)\n",
      "  ‚Ä¢ f1: 0.9448 (¬±0.0088)\n",
      "  ‚Ä¢ roc_auc: 0.9778 (¬±0.0086)\n",
      "\n",
      "üîÑ CV for Gradient Boosting...\n",
      "  ‚Ä¢ accuracy: 0.9804 (¬±0.0026)\n",
      "  ‚Ä¢ precision: 0.9632 (¬±0.0078)\n",
      "  ‚Ä¢ recall: 0.9226 (¬±0.0141)\n",
      "  ‚Ä¢ f1: 0.9424 (¬±0.0079)\n",
      "  ‚Ä¢ roc_auc: 0.9852 (¬±0.0041)\n",
      "\n",
      "üîÑ CV for Support Vector Machine...\n",
      "  ‚Ä¢ accuracy: 0.9334 (¬±0.0017)\n",
      "  ‚Ä¢ precision: 0.7569 (¬±0.0062)\n",
      "  ‚Ä¢ recall: 0.9084 (¬±0.0166)\n",
      "  ‚Ä¢ f1: 0.8256 (¬±0.0053)\n",
      "  ‚Ä¢ roc_auc: 0.9638 (¬±0.0040)\n",
      "\n",
      "üîÑ CV for XGBoost...\n",
      "  ‚Ä¢ accuracy: 0.9794 (¬±0.0032)\n",
      "  ‚Ä¢ precision: 0.9605 (¬±0.0096)\n",
      "  ‚Ä¢ recall: 0.9193 (¬±0.0148)\n",
      "  ‚Ä¢ f1: 0.9394 (¬±0.0097)\n",
      "  ‚Ä¢ roc_auc: 0.9803 (¬±0.0042)\n",
      "\n",
      "üìã TRAINING SUMMARY:\n",
      "====================\n",
      "‚úÖ Models successfully trained: 5\n",
      "‚ùå Models failed: 0\n",
      "üèÜ Best model (by validation F1): XGBoost (F1: 0.9425)\n",
      "\n",
      "üéØ Ready for detailed evaluation and model selection!\n",
      "\n",
      "üíæ SAVING MODELS AND DATA FOR EVALUATION...\n",
      "========================================\n",
      "‚úÖ Saved Logistic Regression to c:\\Users\\DELL\\Desktop\\AI-Project\\AI-Project\\models\\logistic_regression_model.pkl\n",
      "‚úÖ Saved Random Forest to c:\\Users\\DELL\\Desktop\\AI-Project\\AI-Project\\models\\random_forest_model.pkl\n",
      "‚úÖ Saved Gradient Boosting to c:\\Users\\DELL\\Desktop\\AI-Project\\AI-Project\\models\\gradient_boosting_model.pkl\n",
      "‚úÖ Saved Support Vector Machine to c:\\Users\\DELL\\Desktop\\AI-Project\\AI-Project\\models\\support_vector_machine_model.pkl\n",
      "‚úÖ Saved XGBoost to c:\\Users\\DELL\\Desktop\\AI-Project\\AI-Project\\models\\xgboost_model.pkl\n",
      "‚úÖ Saved evaluation data to c:\\Users\\DELL\\Desktop\\AI-Project\\AI-Project\\models\\evaluation_data.pkl\n",
      "\n",
      "üéâ All models and data saved! Ready for evaluation.\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries for model saving\n",
    "import joblib\n",
    "\n",
    "# ü§ñ Step 1: Define our model ensemble\n",
    "if 'X_train' in locals() and X_train is not None:\n",
    "    print(\"ü§ñ DEFINING MODEL ENSEMBLE...\")\n",
    "    print(\"=\"*27)\n",
    "    \n",
    "    # Calculate class weights for imbalanced data\n",
    "    classes = np.unique(y_train)\n",
    "    class_weights = compute_class_weight('balanced', classes=classes, y=y_train)\n",
    "    class_weight_dict = dict(zip(classes, class_weights))\n",
    "    \n",
    "    print(f\"‚öñÔ∏è Class weights calculated: {class_weight_dict}\")\n",
    "    \n",
    "    # Define models with their configurations\n",
    "    models = {\n",
    "        'Logistic Regression': {\n",
    "            'model': LogisticRegression(\n",
    "                random_state=RANDOM_STATE,\n",
    "                class_weight='balanced',\n",
    "                max_iter=1000\n",
    "            ),\n",
    "            'needs_scaling': True,\n",
    "            'description': 'Linear classifier with balanced class weights'\n",
    "        },\n",
    "        \n",
    "        'Random Forest': {\n",
    "            'model': RandomForestClassifier(\n",
    "                n_estimators=100,\n",
    "                random_state=RANDOM_STATE,\n",
    "                class_weight='balanced',\n",
    "                n_jobs=-1\n",
    "            ),\n",
    "            'needs_scaling': False,\n",
    "            'description': 'Ensemble of 100 decision trees'\n",
    "        },\n",
    "        \n",
    "        'Gradient Boosting': {\n",
    "            'model': GradientBoostingClassifier(\n",
    "                n_estimators=100,\n",
    "                random_state=RANDOM_STATE,\n",
    "                learning_rate=0.1\n",
    "            ),\n",
    "            'needs_scaling': False,\n",
    "            'description': 'Sequential boosting algorithm'\n",
    "        },\n",
    "        \n",
    "        'Support Vector Machine': {\n",
    "            'model': SVC(\n",
    "                random_state=RANDOM_STATE,\n",
    "                class_weight='balanced',\n",
    "                probability=True  # Enable probability estimates\n",
    "            ),\n",
    "            'needs_scaling': True,\n",
    "            'description': 'Margin-based classifier with RBF kernel'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Add XGBoost if available\n",
    "    if xgb is not None:\n",
    "        models['XGBoost'] = {\n",
    "            'model': XGBClassifier(\n",
    "                n_estimators=100,\n",
    "                random_state=RANDOM_STATE,\n",
    "                eval_metric='logloss',\n",
    "                scale_pos_weight=class_weights[1]/class_weights[0]  # Handle imbalance\n",
    "            ),\n",
    "            'needs_scaling': False,\n",
    "            'description': 'Advanced gradient boosting'\n",
    "        }\n",
    "    \n",
    "    print(f\"üìä Models defined: {len(models)} algorithms\")\n",
    "    for name, config in models.items():\n",
    "        scaling = \"Requires scaling\" if config['needs_scaling'] else \"No scaling needed\"\n",
    "        print(f\"  ‚Ä¢ {name}: {config['description']} ({scaling})\")\n",
    "    \n",
    "    # üöÄ Step 2: Train all models\n",
    "    print(f\"\\nüöÄ TRAINING ALL MODELS...\")\n",
    "    print(\"=\"*24)\n",
    "    \n",
    "    trained_models = {}\n",
    "    training_results = {}\n",
    "    \n",
    "    for model_name, config in models.items():\n",
    "        print(f\"\\nüîß Training {model_name}...\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Choose appropriate dataset (scaled or original)\n",
    "            if config['needs_scaling']:\n",
    "                X_train_use = X_train_scaled\n",
    "                X_val_use = X_val_scaled\n",
    "                X_test_use = X_test_scaled\n",
    "                print(f\"  üìè Using scaled features\")\n",
    "            else:\n",
    "                X_train_use = X_train\n",
    "                X_val_use = X_val\n",
    "                X_test_use = X_test\n",
    "                print(f\"  üìä Using original features\")\n",
    "            \n",
    "            # Train the model\n",
    "            model = config['model']\n",
    "            model.fit(X_train_use, y_train)\n",
    "            \n",
    "            # Record training time\n",
    "            training_time = time.time() - start_time\n",
    "            \n",
    "            # Make predictions on validation set\n",
    "            y_val_pred = model.predict(X_val_use)\n",
    "            y_val_proba = model.predict_proba(X_val_use)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "            \n",
    "            # Calculate validation metrics\n",
    "            val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "            val_precision = precision_score(y_val, y_val_pred, average='binary')\n",
    "            val_recall = recall_score(y_val, y_val_pred, average='binary')\n",
    "            val_f1 = f1_score(y_val, y_val_pred, average='binary')\n",
    "            \n",
    "            if y_val_proba is not None:\n",
    "                val_roc_auc = roc_auc_score(y_val, y_val_proba)\n",
    "            else:\n",
    "                val_roc_auc = None\n",
    "            \n",
    "            # Store results\n",
    "            trained_models[model_name] = {\n",
    "                'model': model,\n",
    "                'needs_scaling': config['needs_scaling'],\n",
    "                'scaler': scaler if config['needs_scaling'] else None\n",
    "            }\n",
    "            \n",
    "            training_results[model_name] = {\n",
    "                'training_time': training_time,\n",
    "                'val_accuracy': val_accuracy,\n",
    "                'val_precision': val_precision,\n",
    "                'val_recall': val_recall,\n",
    "                'val_f1': val_f1,\n",
    "                'val_roc_auc': val_roc_auc,\n",
    "                'predictions': y_val_pred,\n",
    "                'probabilities': y_val_proba\n",
    "            }\n",
    "\n",
    "            print(f\"  ‚úÖ Success! Training time: {training_time:.2f}s\")\n",
    "            roc_auc_display=f\"{val_roc_auc:.4f}\" if val_roc_auc is not None else 'N/A'\n",
    "            print(f\"     Validation F1: {val_f1:.4f}, ROC-AUC: {roc_auc_display}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Failed: {str(e)}\")\n",
    "            training_results[model_name] = None\n",
    "    \n",
    "    # üìä Step 3: Cross-validation for robust evaluation\n",
    "    print(f\"\\nüìä CROSS-VALIDATION EVALUATION...\")\n",
    "    print(\"=\"*32)\n",
    "    \n",
    "    cv_results = {}\n",
    "    cv_scores = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "    \n",
    "    for model_name, model_info in trained_models.items():\n",
    "        if model_info is not None:\n",
    "            print(f\"\\nüîÑ CV for {model_name}...\")\n",
    "            \n",
    "            model = model_info['model']\n",
    "            X_use = X_train_scaled if model_info['needs_scaling'] else X_train\n",
    "            \n",
    "            cv_results[model_name] = {}\n",
    "            \n",
    "            for score in cv_scores:\n",
    "                try:\n",
    "                    scores = cross_val_score(\n",
    "                        model, X_use, y_train, \n",
    "                        cv=StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE),\n",
    "                        scoring=score,\n",
    "                        n_jobs=-1\n",
    "                    )\n",
    "                    \n",
    "                    cv_results[model_name][score] = {\n",
    "                        'mean': scores.mean(),\n",
    "                        'std': scores.std(),\n",
    "                        'scores': scores\n",
    "                    }\n",
    "                    \n",
    "                    print(f\"  ‚Ä¢ {score}: {scores.mean():.4f} (¬±{scores.std():.4f})\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"  ‚ö†Ô∏è {score}: Failed ({str(e)})\")\n",
    "                    cv_results[model_name][score] = None\n",
    "    \n",
    "    # üìã Step 4: Performance summary\n",
    "    print(f\"\\nüìã TRAINING SUMMARY:\")\n",
    "    print(\"=\"*20)\n",
    "    \n",
    "    print(f\"‚úÖ Models successfully trained: {len([r for r in training_results.values() if r is not None])}\")\n",
    "    print(f\"‚ùå Models failed: {len([r for r in training_results.values() if r is None])}\")\n",
    "    \n",
    "    # Find best performing model (by F1 score on validation)\n",
    "    best_f1 = 0\n",
    "    best_model_name = None\n",
    "    \n",
    "    for model_name, results in training_results.items():\n",
    "        if results is not None and results['val_f1'] > best_f1:\n",
    "            best_f1 = results['val_f1']\n",
    "            best_model_name = model_name\n",
    "    \n",
    "    if best_model_name:\n",
    "        print(f\"üèÜ Best model (by validation F1): {best_model_name} (F1: {best_f1:.4f})\")\n",
    "    \n",
    "    print(f\"\\nüéØ Ready for detailed evaluation and model selection!\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No training data available. Please run data preparation first.\")\n",
    "\n",
    "    # üìÅ Step 5: Save trained models and data for evaluation\n",
    "print(f\"\\nüíæ SAVING MODELS AND DATA FOR EVALUATION...\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Save trained models\n",
    "for model_name, model_info in trained_models.items():\n",
    "    if model_info is not None:\n",
    "        try:\n",
    "            # Save the model\n",
    "            model_path = models_dir / f\"{model_name.replace(' ', '_').lower()}_model.pkl\"\n",
    "            joblib.dump(model_info, model_path)\n",
    "            print(f\"‚úÖ Saved {model_name} to {model_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to save {model_name}: {e}\")\n",
    "\n",
    "# Save test data and other variables\n",
    "data_to_save = {\n",
    "    'X_test': X_test,\n",
    "    'y_test': y_test,\n",
    "    'X_train': X_train,\n",
    "    'y_train': y_train,\n",
    "    'X_test_scaled': X_test_scaled,\n",
    "    'feature_columns': feature_columns,\n",
    "    'target_col': target_col\n",
    "}\n",
    "\n",
    "try:\n",
    "    data_path = models_dir / 'evaluation_data.pkl'\n",
    "    joblib.dump(data_to_save, data_path)\n",
    "    print(f\"‚úÖ Saved evaluation data to {data_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to save evaluation data: {e}\")\n",
    "\n",
    "print(\"\\nüéâ All models and data saved! Ready for evaluation.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
