{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38d2cfb1",
   "metadata": {},
   "source": [
    "# 🤖 Model Training: Building Our Machine Learning Models\n",
    "\n",
    "Welcome to the **Model Training** phase! 🚀 This is where we transform our prepared data into intelligent predictive models. Think of this as training a team of specialists, each with different strengths and approaches.\n",
    "\n",
    "## 🎯 What is Model Training?\n",
    "\n",
    "Model training is like teaching different students the same subject using their preferred learning styles:\n",
    "- **📊 Logistic Regression**: The mathematician who finds linear relationships\n",
    "- **🌳 Random Forest**: The committee that votes on decisions  \n",
    "- **⚡ XGBoost**: The iterative learner who learns from mistakes\n",
    "- **🧠 Neural Network**: The pattern recognizer with multiple layers\n",
    "\n",
    "### 🏆 **Our Model Training Strategy:**\n",
    "\n",
    "#### 1️⃣ **Multiple Algorithm Testing**\n",
    "- Train 4-6 different algorithms\n",
    "- Compare their strengths and weaknesses\n",
    "- Find the best performer for our specific data\n",
    "\n",
    "#### 2️⃣ **Proper Validation**\n",
    "- Use cross-validation for robust evaluation\n",
    "- Separate train/validation/test sets\n",
    "- Ensure models generalize well\n",
    "\n",
    "#### 3️⃣ **Hyperparameter Tuning**\n",
    "- Optimize model settings for best performance\n",
    "- Use systematic search methods\n",
    "- Balance performance vs. overfitting\n",
    "\n",
    "#### 4️⃣ **Imbalanced Data Handling**\n",
    "- Test with balanced and original datasets\n",
    "- Use appropriate evaluation metrics\n",
    "- Apply class weights where beneficial\n",
    "\n",
    "### 📚 **What We'll Accomplish:**\n",
    "\n",
    "#### 🔧 **Model Implementation**\n",
    "- Logistic Regression (baseline linear model)\n",
    "- Random Forest (ensemble tree method)\n",
    "- Gradient Boosting (XGBoost)\n",
    "- Support Vector Machine (SVM)\n",
    "\n",
    "#### 📊 **Performance Analysis**\n",
    "- Accuracy, Precision, Recall, F1-Score\n",
    "- ROC-AUC and Precision-Recall AUC\n",
    "- Confusion matrices and classification reports\n",
    "- Feature importance analysis\n",
    "\n",
    "#### 🎛️ **Optimization**\n",
    "- Grid search for hyperparameters\n",
    "- Cross-validation for robust evaluation\n",
    "- Model comparison and selection\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 Ready to Train?\n",
    "\n",
    "By the end of this notebook, you'll have:\n",
    "- ✅ **Trained multiple ML models** with proper validation\n",
    "- ✅ **Optimized hyperparameters** for best performance  \n",
    "- ✅ **Compared model performance** comprehensively\n",
    "- ✅ **Selected the best model** for your use case\n",
    "- ✅ **Understood feature importance** and model interpretability\n",
    "\n",
    "Let's build some intelligent models! 🏗️🤖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49465b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📦 Step 1: Import All Machine Learning Libraries\n",
    "print(\"📦 IMPORTING MACHINE LEARNING LIBRARIES...\")\n",
    "print(\"=\"*42)\n",
    "\n",
    "# Core data science libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up beautiful visualizations\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Machine Learning Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# XGBoost - Advanced gradient boosting\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    from xgboost import XGBClassifier\n",
    "    print(\"✅ XGBoost available\")\n",
    "except ImportError:\n",
    "    print(\"⚠️ XGBoost not available - will skip XGBoost models\")\n",
    "    xgb = None\n",
    "\n",
    "# Model Selection and Validation\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    cross_val_score,\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    StratifiedKFold\n",
    ")\n",
    "\n",
    "# Evaluation Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_curve,\n",
    "    precision_recall_curve,\n",
    "    auc\n",
    ")\n",
    "\n",
    "# Data Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Utilities\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# 📁 Step 2: Set up project structure\n",
    "print(\"\\n📁 SETTING UP PROJECT STRUCTURE...\")\n",
    "print(\"=\"*35)\n",
    "\n",
    "current_dir = Path.cwd()\n",
    "project_root = current_dir.parent\n",
    "src_path = project_root / 'src'\n",
    "sys.path.append(str(src_path))\n",
    "\n",
    "# Create directories for saving models\n",
    "models_dir = project_root / 'models'\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "results_dir = project_root / 'results'\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"📂 Project root: {project_root}\")\n",
    "print(f\"📂 Models directory: {models_dir}\")\n",
    "print(f\"📂 Results directory: {results_dir}\")\n",
    "\n",
    "# 🎯 Step 3: Set training parameters\n",
    "print(f\"\\n🎯 SETTING TRAINING PARAMETERS...\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# Global parameters\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "VALIDATION_SIZE = 0.2\n",
    "CV_FOLDS = 5\n",
    "\n",
    "# Performance tracking\n",
    "model_performance = {}\n",
    "training_times = {}\n",
    "\n",
    "print(f\"🎲 Random state: {RANDOM_STATE}\")\n",
    "print(f\"📊 Test set size: {TEST_SIZE*100:.0f}%\")\n",
    "print(f\"📊 Validation size: {VALIDATION_SIZE*100:.0f}%\")\n",
    "print(f\"🔄 Cross-validation folds: {CV_FOLDS}\")\n",
    "\n",
    "print(\"\\n✅ Setup complete! Ready to train models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d126b0",
   "metadata": {},
   "source": [
    "## 📊 Step 1: Data Preparation for Training\n",
    "\n",
    "Before training our models, we need to properly prepare our data. This includes loading our processed features, splitting data, and ensuring everything is ready for machine learning algorithms.\n",
    "\n",
    "### 🔧 **Data Preparation Steps:**\n",
    "- **📁 Load processed data** from feature engineering\n",
    "- **🎯 Identify features and target** variables\n",
    "- **✂️ Split data** into train/validation/test sets\n",
    "- **📏 Scale features** where necessary\n",
    "- **⚖️ Handle class imbalance** appropriately\n",
    "\n",
    "### 📋 **Data Splitting Strategy:**\n",
    "- **60% Training**: To teach the models\n",
    "- **20% Validation**: To tune hyperparameters  \n",
    "- **20% Testing**: To evaluate final performance\n",
    "\n",
    "### 🎲 **Why Proper Splitting Matters:**\n",
    "- **Prevents data leakage** between sets\n",
    "- **Ensures fair evaluation** of model performance\n",
    "- **Enables reliable hyperparameter tuning**\n",
    "- **Provides unbiased final assessment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cce4d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📊 Step 1: Load and prepare data for training\n",
    "print(\"📊 LOADING AND PREPARING DATA...\")\n",
    "print(\"=\"*32)\n",
    "\n",
    "# Load processed data\n",
    "processed_dir = project_root / 'data' / 'processed'\n",
    "data_loaded = False\n",
    "\n",
    "# Try to load feature engineered data\n",
    "try:\n",
    "    df_path = processed_dir / 'feature_engineered_data.csv'\n",
    "    if df_path.exists():\n",
    "        df = pd.read_csv(df_path)\n",
    "        print(f\"✅ Loaded feature engineered data: {df.shape}\")\n",
    "        data_loaded = True\n",
    "        \n",
    "        # Try to load feature info\n",
    "        feature_info_path = processed_dir / 'feature_info.json'\n",
    "        if feature_info_path.exists():\n",
    "            with open(feature_info_path, 'r') as f:\n",
    "                feature_info = json.load(f)\n",
    "            print(f\"✅ Loaded feature information\")\n",
    "        else:\n",
    "            feature_info = None\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading processed data: {e}\")\n",
    "\n",
    "# Fallback to raw data if needed\n",
    "if not data_loaded:\n",
    "    print(\"🔄 Falling back to raw data loading...\")\n",
    "    # Add raw data loading logic here if needed\n",
    "\n",
    "if data_loaded:\n",
    "    # 🎯 Step 2: Identify features and target\n",
    "    print(f\"\\n🎯 IDENTIFYING FEATURES AND TARGET...\")\n",
    "    print(\"=\"*35)\n",
    "    \n",
    "    # Get target column\n",
    "    if feature_info and 'target_column' in feature_info:\n",
    "        target_col = feature_info['target_column']\n",
    "    else:\n",
    "        # Auto-detect target\n",
    "        target_candidates = ['Attrition', 'attrition', 'Left', 'left']\n",
    "        target_col = None\n",
    "        for col in df.columns:\n",
    "            if col in target_candidates or any(candidate.lower() in col.lower() for candidate in target_candidates):\n",
    "                target_col = col\n",
    "                break\n",
    "        \n",
    "        if target_col is None:\n",
    "            binary_cols = [col for col in df.columns if df[col].nunique() == 2]\n",
    "            if binary_cols:\n",
    "                target_col = binary_cols[0]\n",
    "    \n",
    "    if target_col:\n",
    "        print(f\"🎯 Target variable: {target_col}\")\n",
    "        \n",
    "        # Get features for ML\n",
    "        if feature_info and 'ml_features' in feature_info:\n",
    "            feature_columns = feature_info['ml_features']\n",
    "        else:\n",
    "            # Use all numerical columns except target\n",
    "            feature_columns = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "            if target_col in feature_columns:\n",
    "                feature_columns.remove(target_col)\n",
    "        \n",
    "        print(f\"📊 Features for ML: {len(feature_columns)}\")\n",
    "        \n",
    "        # Prepare X and y\n",
    "        X = df[feature_columns]\n",
    "        y = df[target_col]\n",
    "        \n",
    "        print(f\"✅ Data prepared: X{X.shape}, y{y.shape}\")\n",
    "        \n",
    "        # Check class distribution\n",
    "        class_distribution = y.value_counts()\n",
    "        print(f\"\\n📊 Class distribution:\")\n",
    "        for class_val, count in class_distribution.items():\n",
    "            percentage = (count / len(y)) * 100\n",
    "            print(f\"  • {class_val}: {count:,} ({percentage:.1f}%)\")\n",
    "        \n",
    "        # ✂️ Step 3: Split data into train/validation/test\n",
    "        print(f\"\\n✂️ SPLITTING DATA...\")\n",
    "        print(\"=\"*18)\n",
    "        \n",
    "        # First split: separate test set\n",
    "        X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "            X, y, test_size=TEST_SIZE, \n",
    "            random_state=RANDOM_STATE, \n",
    "            stratify=y\n",
    "        )\n",
    "        \n",
    "        # Second split: separate train and validation\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_temp, y_temp, \n",
    "            test_size=VALIDATION_SIZE/(1-TEST_SIZE),  # Adjust for remaining data\n",
    "            random_state=RANDOM_STATE, \n",
    "            stratify=y_temp\n",
    "        )\n",
    "        \n",
    "        print(f\"📊 Data splits:\")\n",
    "        print(f\"  • Training: {X_train.shape[0]:,} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "        print(f\"  • Validation: {X_val.shape[0]:,} samples ({X_val.shape[0]/len(X)*100:.1f}%)\")\n",
    "        print(f\"  • Test: {X_test.shape[0]:,} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "        \n",
    "        # Verify stratification worked\n",
    "        print(f\"\\n✅ Class distribution maintained:\")\n",
    "        for dataset_name, y_split in [(\"Train\", y_train), (\"Val\", y_val), (\"Test\", y_test)]:\n",
    "            dist = y_split.value_counts(normalize=True) * 100\n",
    "            print(f\"  • {dataset_name}: {dist.iloc[0]:.1f}% / {dist.iloc[1]:.1f}%\")\n",
    "        \n",
    "        # 📏 Step 4: Scale features if needed\n",
    "        print(f\"\\n📏 FEATURE SCALING...\")\n",
    "        print(\"=\"*18)\n",
    "        \n",
    "        # Check if we need scaling (for algorithms that require it)\n",
    "        feature_ranges = X_train.max() - X_train.min()\n",
    "        max_range = feature_ranges.max()\n",
    "        min_range = feature_ranges.min()\n",
    "        range_ratio = max_range / min_range if min_range > 0 else float('inf')\n",
    "        \n",
    "        print(f\"📊 Feature scale analysis:\")\n",
    "        print(f\"  • Max range: {max_range:.2f}\")\n",
    "        print(f\"  • Min range: {min_range:.2f}\")\n",
    "        print(f\"  • Range ratio: {range_ratio:.1f}:1\")\n",
    "        \n",
    "        if range_ratio > 10:\n",
    "            print(f\"🔧 Applying StandardScaler...\")\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_val_scaled = scaler.transform(X_val)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            \n",
    "            # Convert back to DataFrames\n",
    "            X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "            X_val_scaled = pd.DataFrame(X_val_scaled, columns=X_val.columns, index=X_val.index)\n",
    "            X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "            \n",
    "            print(f\"✅ Features scaled successfully\")\n",
    "            scaling_applied = True\n",
    "        else:\n",
    "            print(f\"ℹ️ Scaling not necessary - features are similarly scaled\")\n",
    "            X_train_scaled = X_train.copy()\n",
    "            X_val_scaled = X_val.copy()\n",
    "            X_test_scaled = X_test.copy()\n",
    "            scaler = None\n",
    "            scaling_applied = False\n",
    "        \n",
    "        print(f\"\\n🎯 READY FOR MODEL TRAINING!\")\n",
    "        print(\"=\"*30)\n",
    "        print(f\"✅ Features prepared: {X_train.shape[1]} features\")\n",
    "        print(f\"✅ Data split: Train/Val/Test ready\")\n",
    "        print(f\"✅ Scaling: {'Applied' if scaling_applied else 'Not needed'}\")\n",
    "        print(f\"✅ Target: {target_col} identified\")\n",
    "        \n",
    "    else:\n",
    "        print(\"❌ Could not identify target variable\")\n",
    "        \n",
    "else:\n",
    "    print(\"❌ No data available for training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7e6373",
   "metadata": {},
   "source": [
    "## 🤖 Step 2: Model Definition and Training\n",
    "\n",
    "Now let's define our ensemble of machine learning models! We'll train multiple algorithms to find the best performer for our specific dataset and problem.\n",
    "\n",
    "### 🎯 **Our Model Arsenal:**\n",
    "\n",
    "#### 📊 **Logistic Regression**\n",
    "- **Type**: Linear classifier\n",
    "- **Strengths**: Fast, interpretable, probabilistic outputs\n",
    "- **Best for**: Linear relationships, baseline model\n",
    "- **Scaling needed**: Yes\n",
    "\n",
    "#### 🌳 **Random Forest**\n",
    "- **Type**: Ensemble of decision trees\n",
    "- **Strengths**: Handles non-linear patterns, feature importance\n",
    "- **Best for**: Robust performance, mixed data types\n",
    "- **Scaling needed**: No\n",
    "\n",
    "#### ⚡ **XGBoost**\n",
    "- **Type**: Gradient boosting ensemble\n",
    "- **Strengths**: High performance, handles missing values\n",
    "- **Best for**: Competitions, complex patterns\n",
    "- **Scaling needed**: No\n",
    "\n",
    "#### 🧠 **Support Vector Machine**\n",
    "- **Type**: Margin-based classifier\n",
    "- **Strengths**: Good with high dimensions, kernel tricks\n",
    "- **Best for**: Text data, complex boundaries\n",
    "- **Scaling needed**: Yes\n",
    "\n",
    "### 🔧 **Training Strategy:**\n",
    "- **Cross-validation** for robust performance estimation\n",
    "- **Hyperparameter tuning** for optimal settings\n",
    "- **Class weight balancing** for imbalanced data\n",
    "- **Performance tracking** for comprehensive comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9c4b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🤖 Step 1: Define our model ensemble\n",
    "if 'X_train' in locals() and X_train is not None:\n",
    "    print(\"🤖 DEFINING MODEL ENSEMBLE...\")\n",
    "    print(\"=\"*27)\n",
    "    \n",
    "    # Calculate class weights for imbalanced data\n",
    "    classes = np.unique(y_train)\n",
    "    class_weights = compute_class_weight('balanced', classes=classes, y=y_train)\n",
    "    class_weight_dict = dict(zip(classes, class_weights))\n",
    "    \n",
    "    print(f\"⚖️ Class weights calculated: {class_weight_dict}\")\n",
    "    \n",
    "    # Define models with their configurations\n",
    "    models = {\n",
    "        'Logistic Regression': {\n",
    "            'model': LogisticRegression(\n",
    "                random_state=RANDOM_STATE,\n",
    "                class_weight='balanced',\n",
    "                max_iter=1000\n",
    "            ),\n",
    "            'needs_scaling': True,\n",
    "            'description': 'Linear classifier with balanced class weights'\n",
    "        },\n",
    "        \n",
    "        'Random Forest': {\n",
    "            'model': RandomForestClassifier(\n",
    "                n_estimators=100,\n",
    "                random_state=RANDOM_STATE,\n",
    "                class_weight='balanced',\n",
    "                n_jobs=-1\n",
    "            ),\n",
    "            'needs_scaling': False,\n",
    "            'description': 'Ensemble of 100 decision trees'\n",
    "        },\n",
    "        \n",
    "        'Gradient Boosting': {\n",
    "            'model': GradientBoostingClassifier(\n",
    "                n_estimators=100,\n",
    "                random_state=RANDOM_STATE,\n",
    "                learning_rate=0.1\n",
    "            ),\n",
    "            'needs_scaling': False,\n",
    "            'description': 'Sequential boosting algorithm'\n",
    "        },\n",
    "        \n",
    "        'Support Vector Machine': {\n",
    "            'model': SVC(\n",
    "                random_state=RANDOM_STATE,\n",
    "                class_weight='balanced',\n",
    "                probability=True  # Enable probability estimates\n",
    "            ),\n",
    "            'needs_scaling': True,\n",
    "            'description': 'Margin-based classifier with RBF kernel'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Add XGBoost if available\n",
    "    if xgb is not None:\n",
    "        models['XGBoost'] = {\n",
    "            'model': XGBClassifier(\n",
    "                n_estimators=100,\n",
    "                random_state=RANDOM_STATE,\n",
    "                eval_metric='logloss',\n",
    "                scale_pos_weight=class_weights[1]/class_weights[0]  # Handle imbalance\n",
    "            ),\n",
    "            'needs_scaling': False,\n",
    "            'description': 'Advanced gradient boosting'\n",
    "        }\n",
    "    \n",
    "    print(f\"📊 Models defined: {len(models)} algorithms\")\n",
    "    for name, config in models.items():\n",
    "        scaling = \"Requires scaling\" if config['needs_scaling'] else \"No scaling needed\"\n",
    "        print(f\"  • {name}: {config['description']} ({scaling})\")\n",
    "    \n",
    "    # 🚀 Step 2: Train all models\n",
    "    print(f\"\\n🚀 TRAINING ALL MODELS...\")\n",
    "    print(\"=\"*24)\n",
    "    \n",
    "    trained_models = {}\n",
    "    training_results = {}\n",
    "    \n",
    "    for model_name, config in models.items():\n",
    "        print(f\"\\n🔧 Training {model_name}...\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Choose appropriate dataset (scaled or original)\n",
    "            if config['needs_scaling']:\n",
    "                X_train_use = X_train_scaled\n",
    "                X_val_use = X_val_scaled\n",
    "                X_test_use = X_test_scaled\n",
    "                print(f\"  📏 Using scaled features\")\n",
    "            else:\n",
    "                X_train_use = X_train\n",
    "                X_val_use = X_val\n",
    "                X_test_use = X_test\n",
    "                print(f\"  📊 Using original features\")\n",
    "            \n",
    "            # Train the model\n",
    "            model = config['model']\n",
    "            model.fit(X_train_use, y_train)\n",
    "            \n",
    "            # Record training time\n",
    "            training_time = time.time() - start_time\n",
    "            \n",
    "            # Make predictions on validation set\n",
    "            y_val_pred = model.predict(X_val_use)\n",
    "            y_val_proba = model.predict_proba(X_val_use)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "            \n",
    "            # Calculate validation metrics\n",
    "            val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "            val_precision = precision_score(y_val, y_val_pred, average='binary')\n",
    "            val_recall = recall_score(y_val, y_val_pred, average='binary')\n",
    "            val_f1 = f1_score(y_val, y_val_pred, average='binary')\n",
    "            \n",
    "            if y_val_proba is not None:\n",
    "                val_roc_auc = roc_auc_score(y_val, y_val_proba)\n",
    "            else:\n",
    "                val_roc_auc = None\n",
    "            \n",
    "            # Store results\n",
    "            trained_models[model_name] = {\n",
    "                'model': model,\n",
    "                'needs_scaling': config['needs_scaling'],\n",
    "                'scaler': scaler if config['needs_scaling'] else None\n",
    "            }\n",
    "            \n",
    "            training_results[model_name] = {\n",
    "                'training_time': training_time,\n",
    "                'val_accuracy': val_accuracy,\n",
    "                'val_precision': val_precision,\n",
    "                'val_recall': val_recall,\n",
    "                'val_f1': val_f1,\n",
    "                'val_roc_auc': val_roc_auc,\n",
    "                'predictions': y_val_pred,\n",
    "                'probabilities': y_val_proba\n",
    "            }\n",
    "            \n",
    "            print(f\"  ✅ Success! Training time: {training_time:.2f}s\")\n",
    "            print(f\"     Validation F1: {val_f1:.4f}, ROC-AUC: {val_roc_auc:.4f if val_roc_auc else 'N/A'}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Failed: {str(e)}\")\n",
    "            training_results[model_name] = None\n",
    "    \n",
    "    # 📊 Step 3: Cross-validation for robust evaluation\n",
    "    print(f\"\\n📊 CROSS-VALIDATION EVALUATION...\")\n",
    "    print(\"=\"*32)\n",
    "    \n",
    "    cv_results = {}\n",
    "    cv_scores = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "    \n",
    "    for model_name, model_info in trained_models.items():\n",
    "        if model_info is not None:\n",
    "            print(f\"\\n🔄 CV for {model_name}...\")\n",
    "            \n",
    "            model = model_info['model']\n",
    "            X_use = X_train_scaled if model_info['needs_scaling'] else X_train\n",
    "            \n",
    "            cv_results[model_name] = {}\n",
    "            \n",
    "            for score in cv_scores:\n",
    "                try:\n",
    "                    scores = cross_val_score(\n",
    "                        model, X_use, y_train, \n",
    "                        cv=StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE),\n",
    "                        scoring=score,\n",
    "                        n_jobs=-1\n",
    "                    )\n",
    "                    \n",
    "                    cv_results[model_name][score] = {\n",
    "                        'mean': scores.mean(),\n",
    "                        'std': scores.std(),\n",
    "                        'scores': scores\n",
    "                    }\n",
    "                    \n",
    "                    print(f\"  • {score}: {scores.mean():.4f} (±{scores.std():.4f})\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"  ⚠️ {score}: Failed ({str(e)})\")\n",
    "                    cv_results[model_name][score] = None\n",
    "    \n",
    "    # 📋 Step 4: Performance summary\n",
    "    print(f\"\\n📋 TRAINING SUMMARY:\")\n",
    "    print(\"=\"*20)\n",
    "    \n",
    "    print(f\"✅ Models successfully trained: {len([r for r in training_results.values() if r is not None])}\")\n",
    "    print(f\"❌ Models failed: {len([r for r in training_results.values() if r is None])}\")\n",
    "    \n",
    "    # Find best performing model (by F1 score on validation)\n",
    "    best_f1 = 0\n",
    "    best_model_name = None\n",
    "    \n",
    "    for model_name, results in training_results.items():\n",
    "        if results is not None and results['val_f1'] > best_f1:\n",
    "            best_f1 = results['val_f1']\n",
    "            best_model_name = model_name\n",
    "    \n",
    "    if best_model_name:\n",
    "        print(f\"🏆 Best model (by validation F1): {best_model_name} (F1: {best_f1:.4f})\")\n",
    "    \n",
    "    print(f\"\\n🎯 Ready for detailed evaluation and model selection!\")\n",
    "\n",
    "else:\n",
    "    print(\"⚠️ No training data available. Please run data preparation first.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
