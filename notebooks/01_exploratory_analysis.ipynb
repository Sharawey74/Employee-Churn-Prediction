{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb369a58",
   "metadata": {},
   "source": [
    "# ğŸ“Š Employee Turnover Prediction - Exploratory Data Analysis\n",
    "\n",
    "Welcome to the **Employee Turnover Prediction** project! In this notebook, we'll explore our dataset to understand employee behavior patterns and identify factors that contribute to employee turnover.\n",
    "\n",
    "## ğŸ¯ **Learning Objectives**\n",
    "By the end of this notebook, you will understand:\n",
    "1. How to load and inspect employee data\n",
    "2. How to identify data quality issues (missing values, duplicates)\n",
    "3. How to analyze numerical and categorical features\n",
    "4. How to detect patterns in employee turnover\n",
    "5. How to create visualizations for data insights\n",
    "\n",
    "## ğŸ“ **What is Exploratory Data Analysis (EDA)?**\n",
    "EDA is like being a detective ğŸ•µï¸â€â™‚ï¸ - we examine the data to:\n",
    "- **Understand** what we're working with\n",
    "- **Discover** hidden patterns and relationships\n",
    "- **Identify** problems that need fixing\n",
    "- **Get insights** before building ML models\n",
    "\n",
    "Let's start our investigation! ğŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe0f8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”§ Step 1: Setup and Imports\n",
    "# This section imports all the tools we need for data analysis\n",
    "\n",
    "import pandas as pd              # For data manipulation (like Excel but more powerful)\n",
    "import numpy as np               # For numerical operations and arrays\n",
    "import matplotlib.pyplot as plt  # For creating charts and graphs\n",
    "import seaborn as sns           # For beautiful statistical visualizations\n",
    "import warnings                 # To hide unnecessary warning messages\n",
    "\n",
    "# Configure settings for better output\n",
    "plt.style.use('default')        # Set a clean chart style\n",
    "sns.set_palette(\"husl\")         # Set attractive colors for charts\n",
    "warnings.filterwarnings('ignore') # Hide warning messages for cleaner output\n",
    "\n",
    "# Configure pandas to show more data\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.width', None)        # No width limit\n",
    "pd.set_option('display.max_colwidth', 100)  # Max column width\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")\n",
    "print(\"ğŸ“Š Ready to start data exploration!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6308964d",
   "metadata": {},
   "source": [
    "## ğŸ“ Step 2: Loading the Data\n",
    "\n",
    "Now let's load our employee dataset. Think of this as opening a spreadsheet with information about employees.\n",
    "\n",
    "**What we're looking for:**\n",
    "- How many employees are in our dataset?\n",
    "- What information do we have about each employee?\n",
    "- Are there any obvious problems with the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a01c402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“‚ Load the employee dataset\n",
    "# We're loading a CSV file (Comma Separated Values) - like a simple spreadsheet\n",
    "\n",
    "# File path to our data (relative to the notebooks folder)\n",
    "data_path = '../data/raw/employee_data.csv'\n",
    "\n",
    "# Load the data using pandas\n",
    "# pandas.read_csv() reads CSV files and creates a DataFrame (like a smart table)\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Let's see what we loaded\n",
    "print(\"ğŸ‰ Data loaded successfully!\")\n",
    "print(f\"ğŸ“Š Dataset shape: {df.shape}\")  # (rows, columns)\n",
    "print(f\"ğŸ‘¥ Number of employees: {df.shape[0]:,}\")\n",
    "print(f\"ğŸ“‹ Number of features/columns: {df.shape[1]}\")\n",
    "\n",
    "# Show the first few rows to see what our data looks like\n",
    "print(\"\\nğŸ“‹ First 5 rows of our dataset:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15abb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ” Let's understand what each column means\n",
    "print(\"ğŸ“‹ COLUMN EXPLANATION:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "column_explanations = {\n",
    "    'satisfaction_level': 'ğŸ˜Š How satisfied the employee is (0-1 scale)',\n",
    "    'last_evaluation': 'ğŸ“ˆ Performance rating from last review (0-1 scale)', \n",
    "    'number_project': 'ğŸ“Š Number of projects assigned to employee',\n",
    "    'average_montly_hours': 'â° Average monthly working hours',\n",
    "    'time_spend_company': 'ğŸ“… Number of years at the company',\n",
    "    'Work_accident': 'âš ï¸ Whether employee had work accident (0=No, 1=Yes)',\n",
    "    'promotion_last_5years': 'ğŸš€ Promoted in last 5 years (0=No, 1=Yes)',\n",
    "    'department': 'ğŸ¢ Which department employee works in',\n",
    "    'salary': 'ğŸ’° Salary level (low, medium, high)',\n",
    "    'quit': 'ğŸšª Target variable - Did employee quit? (0=No, 1=Yes)'\n",
    "}\n",
    "\n",
    "for col, explanation in column_explanations.items():\n",
    "    if col in df.columns:\n",
    "        print(f\"{col:20} â†’ {explanation}\")\n",
    "\n",
    "print(f\"\\nğŸ¯ Our TARGET: 'quit' column - this is what we want to predict!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080b5172",
   "metadata": {},
   "source": [
    "## ğŸ” Step 3: Data Quality Check\n",
    "\n",
    "Before we analyze anything, we need to check if our data is \"clean\" and ready for analysis. This is like checking if all the information in our employee files is complete and correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdba240a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ” Check 1: Basic information about our dataset\n",
    "print(\"ğŸ“Š DATASET INFO:\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Shape: {df.shape} (rows, columns)\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# ğŸ” Check 2: Data types - what kind of information is in each column?\n",
    "print(\"\\nğŸ“‹ DATA TYPES:\")\n",
    "print(\"=\"*40)\n",
    "print(df.dtypes)\n",
    "\n",
    "# ğŸ” Check 3: Missing values (empty cells)\n",
    "print(\"\\nâ“ MISSING VALUES:\")\n",
    "print(\"=\"*40)\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percentage = (missing_values / len(df)) * 100\n",
    "\n",
    "for col in df.columns:\n",
    "    missing_count = missing_values[col]\n",
    "    missing_pct = missing_percentage[col]\n",
    "    if missing_count > 0:\n",
    "        print(f\"{col:20} â†’ {missing_count:,} missing ({missing_pct:.2f}%)\")\n",
    "\n",
    "if missing_values.sum() == 0:\n",
    "    print(\"âœ… No missing values found!\")\n",
    "else:\n",
    "    print(f\"âš ï¸ Total missing values: {missing_values.sum():,}\")\n",
    "\n",
    "# ğŸ” Check 4: Duplicate rows (same employee data appearing twice)\n",
    "print(\"\\nğŸ”„ DUPLICATE ROWS:\")\n",
    "print(\"=\"*40)\n",
    "duplicates = df.duplicated().sum()\n",
    "if duplicates > 0:\n",
    "    print(f\"âš ï¸ Found {duplicates:,} duplicate rows ({duplicates/len(df)*100:.2f}%)\")\n",
    "else:\n",
    "    print(\"âœ… No duplicate rows found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cca303",
   "metadata": {},
   "source": [
    "## ğŸ“Š Step 4: Analyzing Numerical Features\n",
    "\n",
    "Now let's dive deep into our numerical features! These are the numbers that can tell us a lot about our employees. Think of it like looking at test scores, ages, or heights - numbers that can be compared, averaged, and measured.\n",
    "\n",
    "### What we'll discover:\n",
    "- ğŸ“ˆ **Summary statistics**: Average, median, min, max values\n",
    "- ğŸ“ **Distribution shapes**: Are the numbers spread out evenly or bunched up?\n",
    "- ğŸ¯ **Outliers**: Any unusual or extreme values that stand out?\n",
    "- ğŸ”— **Relationships**: How do these numbers relate to each other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4a2ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”¢ Step 1: Identify numerical columns\n",
    "# These are columns with numbers that we can do math with\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(\"ğŸ“Š NUMERICAL FEATURES:\")\n",
    "print(\"=\"*40)\n",
    "for i, col in enumerate(numerical_cols, 1):\n",
    "    print(f\"{i}. {col}\")\n",
    "\n",
    "print(f\"\\nTotal numerical features: {len(numerical_cols)}\")\n",
    "\n",
    "# ğŸ“ˆ Step 2: Get summary statistics\n",
    "print(\"\\nğŸ“ˆ SUMMARY STATISTICS:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# .describe() gives us key statistics for each numerical column\n",
    "stats = df[numerical_cols].describe()\n",
    "print(stats.round(2))  # Round to 2 decimal places for easier reading\n",
    "\n",
    "# ğŸ¯ Step 3: Look for potential outliers using IQR method\n",
    "print(\"\\nğŸ¯ OUTLIER DETECTION:\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "for col in numerical_cols:\n",
    "    # IQR (Interquartile Range) method:\n",
    "    # Q1 = 25th percentile, Q3 = 75th percentile\n",
    "    # IQR = Q3 - Q1\n",
    "    # Outliers are values below Q1 - 1.5*IQR or above Q3 + 1.5*IQR\n",
    "    \n",
    "    Q1 = df[col].quantile(0.25)  # 25th percentile\n",
    "    Q3 = df[col].quantile(0.75)  # 75th percentile\n",
    "    IQR = Q3 - Q1                # Interquartile Range\n",
    "    \n",
    "    # Calculate outlier boundaries\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Find outliers\n",
    "    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)][col]\n",
    "    \n",
    "    print(f\"{col:20} â†’ {len(outliers):,} outliers ({len(outliers)/len(df)*100:.2f}%)\")\n",
    "    if len(outliers) > 0:\n",
    "        print(f\"{'':20}   Range: {outliers.min():.2f} to {outliers.max():.2f}\")\n",
    "        print(f\"{'':20}   Normal range: {lower_bound:.2f} to {upper_bound:.2f}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7098e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š Step 4: Create visualizations for numerical features\n",
    "print(\"ğŸ“Š CREATING VISUALIZATIONS FOR NUMERICAL FEATURES...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Set up the plotting area with multiple subplots\n",
    "# We'll create 2 rows: histograms on top, box plots on bottom\n",
    "fig, axes = plt.subplots(2, len(numerical_cols), figsize=(15, 10))\n",
    "fig.suptitle('ğŸ“Š Numerical Features Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# If we only have one numerical column, axes won't be 2D\n",
    "if len(numerical_cols) == 1:\n",
    "    axes = axes.reshape(2, 1)\n",
    "\n",
    "for i, col in enumerate(numerical_cols):\n",
    "    # ğŸ“ˆ Top row: Histograms (show distribution shape)\n",
    "    # A histogram shows how many employees have values in different ranges\n",
    "    axes[0, i].hist(df[col], bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[0, i].set_title(f'ğŸ“ˆ {col}\\nDistribution', fontweight='bold')\n",
    "    axes[0, i].set_xlabel('Value')\n",
    "    axes[0, i].set_ylabel('Number of Employees')\n",
    "    axes[0, i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add mean line (average value)\n",
    "    mean_val = df[col].mean()\n",
    "    axes[0, i].axvline(mean_val, color='red', linestyle='--', \n",
    "                       label=f'Mean: {mean_val:.2f}')\n",
    "    axes[0, i].legend()\n",
    "    \n",
    "    # ğŸ“¦ Bottom row: Box plots (show outliers and quartiles)\n",
    "    # A box plot shows the \"box\" containing 50% of the data and whiskers for the rest\n",
    "    box_plot = axes[1, i].boxplot(df[col], patch_artist=True)\n",
    "    box_plot['boxes'][0].set_facecolor('lightgreen')\n",
    "    axes[1, i].set_title(f'ğŸ“¦ {col}\\nOutlier Detection', fontweight='bold')\n",
    "    axes[1, i].set_ylabel('Value')\n",
    "    axes[1, i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ğŸ” Step 5: Correlation analysis (how features relate to each other)\n",
    "print(\"\\nğŸ”— CORRELATION ANALYSIS:\")\n",
    "print(\"=\"*40)\n",
    "print(\"Correlation tells us how strongly two features are related.\")\n",
    "print(\"â€¢ 1.0 = Perfect positive relationship (as one goes up, other goes up)\")\n",
    "print(\"â€¢ 0.0 = No relationship\")\n",
    "print(\"â€¢ -1.0 = Perfect negative relationship (as one goes up, other goes down)\")\n",
    "print()\n",
    "\n",
    "correlation_matrix = df[numerical_cols].corr()\n",
    "print(\"Correlation Matrix:\")\n",
    "print(correlation_matrix.round(3))\n",
    "\n",
    "# Create correlation heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, \n",
    "            annot=True,           # Show correlation values\n",
    "            cmap='coolwarm',      # Color scheme\n",
    "            center=0,             # Center colormap at 0\n",
    "            square=True,          # Make cells square-shaped\n",
    "            fmt='.3f')            # Format numbers to 3 decimal places\n",
    "plt.title('ğŸ”— Feature Correlation Heatmap', fontweight='bold', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d7f993",
   "metadata": {},
   "source": [
    "## ğŸ·ï¸ Step 5: Analyzing Categorical Features\n",
    "\n",
    "Now let's explore our categorical features! These are like labels or categories - think of them as different groups or types. For example: departments (Sales, IT, HR), education levels (Bachelor's, Master's), or job roles.\n",
    "\n",
    "### What we'll discover:\n",
    "- ğŸ“‹ **Categories**: What different groups exist in each feature?\n",
    "- ğŸ“Š **Frequency**: How many employees are in each category?\n",
    "- ğŸ¯ **Patterns**: Are some categories more common than others?\n",
    "- ğŸ” **Relationships**: How do categories relate to employee leaving?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d78acb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ·ï¸ Step 1: Identify categorical columns\n",
    "# These are columns with text or categories (not numbers we can do math with)\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(\"ğŸ·ï¸ CATEGORICAL FEATURES:\")\n",
    "print(\"=\"*40)\n",
    "for i, col in enumerate(categorical_cols, 1):\n",
    "    print(f\"{i}. {col}\")\n",
    "print(f\"\\nTotal categorical features: {len(categorical_cols)}\")\n",
    "\n",
    "# ğŸ“Š Step 2: Explore each categorical feature\n",
    "print(\"\\nğŸ“Š CATEGORICAL FEATURE ANALYSIS:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\nğŸ” {col.upper()}:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Count how many employees are in each category\n",
    "    value_counts = df[col].value_counts()\n",
    "    \n",
    "    print(f\"Number of unique categories: {df[col].nunique()}\")\n",
    "    print(f\"Most common category: '{value_counts.index[0]}' ({value_counts.iloc[0]:,} employees)\")\n",
    "    \n",
    "    # Show all categories and their counts\n",
    "    print(\"\\nCategory breakdown:\")\n",
    "    for category, count in value_counts.items():\n",
    "        percentage = (count / len(df)) * 100\n",
    "        print(f\"  â€¢ {category:20} â†’ {count:5,} employees ({percentage:5.1f}%)\")\n",
    "    \n",
    "    # Check for any missing or unusual values\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        print(f\"âš ï¸ Missing values: {df[col].isnull().sum()}\")\n",
    "    \n",
    "    print()  # Add space between features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848252c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š Step 3: Create visualizations for categorical features\n",
    "print(\"ğŸ“Š CREATING VISUALIZATIONS FOR CATEGORICAL FEATURES...\")\n",
    "print(\"=\"*55)\n",
    "\n",
    "# Calculate how many rows and columns we need for our subplots\n",
    "n_categorical = len(categorical_cols)\n",
    "n_cols = 3  # 3 charts per row\n",
    "n_rows = (n_categorical + n_cols - 1) // n_cols  # Calculate rows needed\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))\n",
    "fig.suptitle('ğŸ·ï¸ Categorical Features Distribution', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Handle case where we have only one row\n",
    "if n_rows == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "elif n_categorical == 1:\n",
    "    axes = axes.reshape(-1, 1)\n",
    "\n",
    "# Create a bar chart for each categorical feature\n",
    "for i, col in enumerate(categorical_cols):\n",
    "    row = i // n_cols\n",
    "    col_idx = i % n_cols\n",
    "    \n",
    "    # Get value counts for this feature\n",
    "    value_counts = df[col].value_counts()\n",
    "    \n",
    "    # Create bar chart\n",
    "    bars = axes[row, col_idx].bar(range(len(value_counts)), \n",
    "                                  value_counts.values, \n",
    "                                  color='lightblue', \n",
    "                                  edgecolor='navy', \n",
    "                                  alpha=0.7)\n",
    "    \n",
    "    # Customize the chart\n",
    "    axes[row, col_idx].set_title(f'ğŸ“Š {col}', fontweight='bold')\n",
    "    axes[row, col_idx].set_xlabel('Categories')\n",
    "    axes[row, col_idx].set_ylabel('Number of Employees')\n",
    "    \n",
    "    # Set category names on x-axis (rotated for readability)\n",
    "    axes[row, col_idx].set_xticks(range(len(value_counts)))\n",
    "    axes[row, col_idx].set_xticklabels(value_counts.index, rotation=45, ha='right')\n",
    "    \n",
    "    # Add value labels on top of bars\n",
    "    for bar, count in zip(bars, value_counts.values):\n",
    "        axes[row, col_idx].text(bar.get_x() + bar.get_width()/2, \n",
    "                               bar.get_height() + bar.get_height()*0.01,\n",
    "                               f'{count:,}', \n",
    "                               ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    axes[row, col_idx].grid(True, alpha=0.3)\n",
    "\n",
    "# Hide empty subplots if we have any\n",
    "for i in range(len(categorical_cols), n_rows * n_cols):\n",
    "    row = i // n_cols\n",
    "    col_idx = i % n_cols\n",
    "    axes[row, col_idx].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ğŸ¯ Step 4: Analyze category distribution patterns\n",
    "print(\"\\nğŸ¯ CATEGORY DISTRIBUTION PATTERNS:\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "for col in categorical_cols:\n",
    "    value_counts = df[col].value_counts()\n",
    "    \n",
    "    # Calculate distribution metrics\n",
    "    total_categories = len(value_counts)\n",
    "    most_common_pct = (value_counts.iloc[0] / len(df)) * 100\n",
    "    least_common_pct = (value_counts.iloc[-1] / len(df)) * 100\n",
    "    \n",
    "    print(f\"\\nğŸ“Š {col}:\")\n",
    "    print(f\"  â€¢ Total categories: {total_categories}\")\n",
    "    print(f\"  â€¢ Most common: {most_common_pct:.1f}% of employees\")\n",
    "    print(f\"  â€¢ Least common: {least_common_pct:.1f}% of employees\")\n",
    "    \n",
    "    # Check if distribution is balanced or skewed\n",
    "    if most_common_pct > 70:\n",
    "        print(f\"  âš ï¸ Highly skewed - most employees in one category\")\n",
    "    elif most_common_pct > 50:\n",
    "        print(f\"  ğŸ“Š Moderately skewed distribution\")\n",
    "    else:\n",
    "        print(f\"  âœ… Relatively balanced distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c624fae7",
   "metadata": {},
   "source": [
    "## ğŸ¯ Step 6: Target Variable Analysis\n",
    "\n",
    "Now let's focus on our target variable - the thing we're trying to predict! This is probably whether an employee will leave the company (attrition). Understanding our target is crucial because:\n",
    "\n",
    "### Why this matters:\n",
    "- ğŸ† **Goal clarity**: This is what our model will predict\n",
    "- âš–ï¸ **Balance check**: Are we predicting a rare event or common one?\n",
    "- ğŸ” **Insight discovery**: What patterns exist in who leaves vs. stays?\n",
    "- ğŸ›ï¸ **Model strategy**: Helps us choose the right approach\n",
    "\n",
    "### What we'll discover:\n",
    "- ğŸ“Š **Distribution**: How many employees leave vs. stay?\n",
    "- ğŸ”„ **Balance**: Is our dataset balanced or imbalanced?\n",
    "- ğŸ“ˆ **Relationships**: Which features influence leaving the most?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77d0f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ¯ Step 1: Identify the target variable\n",
    "# Let's look for the column that indicates whether employee left\n",
    "target_candidates = ['Attrition', 'attrition', 'Left', 'left', 'Turnover', 'turnover', \n",
    "                    'Quit', 'quit', 'Exit', 'exit', 'Churn', 'churn']\n",
    "\n",
    "target_col = None\n",
    "for col in df.columns:\n",
    "    if col in target_candidates or any(candidate.lower() in col.lower() for candidate in target_candidates):\n",
    "        target_col = col\n",
    "        break\n",
    "\n",
    "if target_col:\n",
    "    print(f\"ğŸ¯ TARGET VARIABLE FOUND: '{target_col}'\")\n",
    "else:\n",
    "    print(\"â“ Target variable not automatically detected. Let's examine all columns:\")\n",
    "    for col in df.columns:\n",
    "        unique_vals = df[col].nunique()\n",
    "        if unique_vals <= 5:  # Likely categorical with few categories\n",
    "            print(f\"  â€¢ {col}: {unique_vals} unique values â†’ {df[col].unique()}\")\n",
    "    \n",
    "    # If we can't find it automatically, let's assume it's a common name\n",
    "    # You might need to adjust this based on your actual dataset\n",
    "    possible_targets = [col for col in df.columns if df[col].nunique() == 2]\n",
    "    if possible_targets:\n",
    "        target_col = possible_targets[0]\n",
    "        print(f\"\\nğŸ¯ ASSUMING TARGET VARIABLE: '{target_col}' (binary variable)\")\n",
    "\n",
    "if target_col:\n",
    "    print(f\"\\nğŸ“Š TARGET VARIABLE ANALYSIS: {target_col}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # ğŸ“Š Step 2: Basic target distribution\n",
    "    target_counts = df[target_col].value_counts()\n",
    "    target_percentages = df[target_col].value_counts(normalize=True) * 100\n",
    "    \n",
    "    print(\"Distribution:\")\n",
    "    for value, count in target_counts.items():\n",
    "        percentage = target_percentages[value]\n",
    "        print(f\"  â€¢ {value}: {count:,} employees ({percentage:.1f}%)\")\n",
    "    \n",
    "    # âš–ï¸ Step 3: Check for class imbalance\n",
    "    print(f\"\\nâš–ï¸ CLASS BALANCE ANALYSIS:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    minority_class_pct = target_percentages.min()\n",
    "    majority_class_pct = target_percentages.max()\n",
    "    \n",
    "    print(f\"Minority class: {minority_class_pct:.1f}%\")\n",
    "    print(f\"Majority class: {majority_class_pct:.1f}%\")\n",
    "    print(f\"Imbalance ratio: {majority_class_pct/minority_class_pct:.1f}:1\")\n",
    "    \n",
    "    if minority_class_pct < 10:\n",
    "        balance_status = \"ğŸ”´ Severely imbalanced\"\n",
    "        balance_note = \"Will need special techniques (SMOTE, class weights, etc.)\"\n",
    "    elif minority_class_pct < 20:\n",
    "        balance_status = \"ğŸŸ¡ Moderately imbalanced\"\n",
    "        balance_note = \"Consider class weights or sampling techniques\"\n",
    "    elif minority_class_pct < 40:\n",
    "        balance_status = \"ğŸŸ  Slightly imbalanced\"\n",
    "        balance_note = \"Might benefit from class weights\"\n",
    "    else:\n",
    "        balance_status = \"ğŸŸ¢ Well balanced\"\n",
    "        balance_note = \"Good for standard ML algorithms\"\n",
    "    \n",
    "    print(f\"Status: {balance_status}\")\n",
    "    print(f\"Recommendation: {balance_note}\")\n",
    "else:\n",
    "    print(\"âŒ Could not identify target variable. Please specify manually.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbea3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š Step 4: Visualize target variable distribution\n",
    "if target_col:\n",
    "    print(\"ğŸ“Š CREATING TARGET VARIABLE VISUALIZATIONS...\")\n",
    "    print(\"=\"*45)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    fig.suptitle(f'ğŸ¯ Target Variable Analysis: {target_col}', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Pie chart - shows proportions clearly\n",
    "    target_counts = df[target_col].value_counts()\n",
    "    colors = ['lightcoral', 'lightblue']\n",
    "    explode = (0.05, 0)  # Slightly separate the slices\n",
    "    \n",
    "    axes[0].pie(target_counts.values, \n",
    "                labels=target_counts.index, \n",
    "                autopct='%1.1f%%',\n",
    "                colors=colors,\n",
    "                explode=explode,\n",
    "                startangle=90,\n",
    "                textprops={'fontweight': 'bold'})\n",
    "    axes[0].set_title('ğŸ¥§ Target Distribution\\n(Pie Chart)', fontweight='bold')\n",
    "    \n",
    "    # Bar chart - shows actual numbers clearly\n",
    "    bars = axes[1].bar(target_counts.index, target_counts.values, \n",
    "                       color=colors, alpha=0.7, edgecolor='navy')\n",
    "    axes[1].set_title('ğŸ“Š Target Distribution\\n(Bar Chart)', fontweight='bold')\n",
    "    axes[1].set_xlabel('Target Value')\n",
    "    axes[1].set_ylabel('Number of Employees')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, count in zip(bars, target_counts.values):\n",
    "        axes[1].text(bar.get_x() + bar.get_width()/2, \n",
    "                     bar.get_height() + bar.get_height()*0.01,\n",
    "                     f'{count:,}', \n",
    "                     ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # ğŸ” Step 5: Analyze target relationships with other features\n",
    "    print(f\"\\nğŸ” TARGET vs NUMERICAL FEATURES:\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # For numerical features, compare averages between target groups\n",
    "    if len(numerical_cols) > 0:\n",
    "        print(\"Average values by target group:\")\n",
    "        print()\n",
    "        \n",
    "        target_groups = df.groupby(target_col)[numerical_cols].mean()\n",
    "        \n",
    "        for col in numerical_cols:\n",
    "            print(f\"ğŸ“Š {col}:\")\n",
    "            for target_value in target_groups.index:\n",
    "                avg_value = target_groups.loc[target_value, col]\n",
    "                print(f\"  â€¢ {target_value}: {avg_value:.2f}\")\n",
    "            \n",
    "            # Calculate the difference between groups\n",
    "            if len(target_groups) == 2:\n",
    "                diff = abs(target_groups[col].iloc[1] - target_groups[col].iloc[0])\n",
    "                diff_pct = (diff / target_groups[col].mean()) * 100\n",
    "                print(f\"  â€¢ Difference: {diff:.2f} ({diff_pct:.1f}%)\")\n",
    "            print()\n",
    "    \n",
    "    # ğŸ“‹ Step 6: Analyze target relationships with categorical features\n",
    "    print(f\"ğŸ“‹ TARGET vs CATEGORICAL FEATURES:\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    if len(categorical_cols) > 0:\n",
    "        print(\"Target distribution within each category:\")\n",
    "        print()\n",
    "        \n",
    "        for col in categorical_cols:\n",
    "            print(f\"ğŸ·ï¸ {col}:\")\n",
    "            \n",
    "            # Create a cross-tabulation (contingency table)\n",
    "            crosstab = pd.crosstab(df[col], df[target_col], normalize='index') * 100\n",
    "            \n",
    "            for category in crosstab.index:\n",
    "                print(f\"  â€¢ {category}:\")\n",
    "                for target_value in crosstab.columns:\n",
    "                    percentage = crosstab.loc[category, target_value]\n",
    "                    print(f\"    - {target_value}: {percentage:.1f}%\")\n",
    "            print()\n",
    "            \n",
    "            # Identify categories with highest/lowest target rates\n",
    "            if len(crosstab.columns) == 2:\n",
    "                # Assuming binary target, get the rate of the positive class\n",
    "                target_rates = crosstab.iloc[:, 1]  # Second column (usually 'Yes', 'Left', etc.)\n",
    "                highest_rate_cat = target_rates.idxmax()\n",
    "                lowest_rate_cat = target_rates.idxmin()\n",
    "                \n",
    "                print(f\"  ğŸ“ˆ Highest rate: {highest_rate_cat} ({target_rates.max():.1f}%)\")\n",
    "                print(f\"  ğŸ“‰ Lowest rate: {lowest_rate_cat} ({target_rates.min():.1f}%)\")\n",
    "                print()\n",
    "\n",
    "else:\n",
    "    print(\"âš ï¸ Skipping target analysis - target variable not identified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a94a2ef",
   "metadata": {},
   "source": [
    "## ğŸ’¡ Step 7: Key Insights & Next Steps\n",
    "\n",
    "Congratulations! ğŸ‰ You've completed your first comprehensive Exploratory Data Analysis. Let's summarize what we discovered and plan our next moves.\n",
    "\n",
    "### ğŸ” What We Learned:\n",
    "\n",
    "#### ğŸ“Š **Data Quality**\n",
    "- âœ… Checked for missing values and duplicates\n",
    "- âœ… Verified data types and basic statistics\n",
    "- âœ… Identified any data quality issues\n",
    "\n",
    "#### ğŸ”¢ **Numerical Features**\n",
    "- âœ… Found patterns in continuous variables\n",
    "- âœ… Detected outliers using statistical methods\n",
    "- âœ… Understood feature distributions and relationships\n",
    "\n",
    "#### ğŸ·ï¸ **Categorical Features**  \n",
    "- âœ… Explored different categories and their frequencies\n",
    "- âœ… Identified balanced vs. imbalanced features\n",
    "- âœ… Understood the diversity in categorical data\n",
    "\n",
    "#### ğŸ¯ **Target Variable**\n",
    "- âœ… Analyzed target distribution and balance\n",
    "- âœ… Found relationships between features and target\n",
    "- âœ… Identified important patterns for prediction\n",
    "\n",
    "### ğŸš€ **Next Steps in Our ML Journey:**\n",
    "\n",
    "1. **ğŸ“Š Feature Engineering** (Next Notebook)\n",
    "   - Create new features from existing ones\n",
    "   - Handle categorical variables (encoding)\n",
    "   - Scale numerical features\n",
    "   - Feature selection\n",
    "\n",
    "2. **âš–ï¸ Class Imbalance Handling** (Notebook 3)\n",
    "   - Apply SMOTE or other sampling techniques\n",
    "   - Use class weights\n",
    "   - Evaluate different approaches\n",
    "\n",
    "3. **ğŸ¤– Model Training** (Notebook 4)\n",
    "   - Train multiple algorithms\n",
    "   - Hyperparameter tuning\n",
    "   - Cross-validation\n",
    "\n",
    "4. **ğŸ“ˆ Model Evaluation** (Notebook 5)\n",
    "   - Compare model performance\n",
    "   - Feature importance analysis\n",
    "   - Final model selection\n",
    "\n",
    "### ğŸ’­ **Remember:**\n",
    "- EDA is iterative - you can always come back and explore more\n",
    "- Every insight here helps inform our modeling decisions\n",
    "- The patterns we found will guide feature engineering\n",
    "- Understanding your data is the foundation of successful ML!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8a0516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“‹ Final Summary: Our Dataset at a Glance\n",
    "print(\"ğŸ“‹ FINAL DATASET SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"ğŸ—ƒï¸ Dataset Shape: {df.shape[0]:,} rows Ã— {df.shape[1]} columns\")\n",
    "print(f\"ğŸ’¾ Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Feature Types:\")\n",
    "print(f\"  â€¢ Numerical features: {len(numerical_cols)}\")\n",
    "print(f\"  â€¢ Categorical features: {len(categorical_cols)}\")\n",
    "\n",
    "if target_col:\n",
    "    print(f\"\\nğŸ¯ Target Variable: {target_col}\")\n",
    "    target_dist = df[target_col].value_counts(normalize=True) * 100\n",
    "    for value, pct in target_dist.items():\n",
    "        print(f\"  â€¢ {value}: {pct:.1f}%\")\n",
    "\n",
    "print(f\"\\nâœ… Data Quality:\")\n",
    "print(f\"  â€¢ Missing values: {df.isnull().sum().sum()}\")\n",
    "print(f\"  â€¢ Duplicate rows: {df.duplicated().sum()}\")\n",
    "\n",
    "# Quick feature importance hint\n",
    "if target_col and len(numerical_cols) > 0:\n",
    "    print(f\"\\nğŸ” Quick Insights:\")\n",
    "    correlations = df[numerical_cols + [target_col]].corr()[target_col].abs().sort_values(ascending=False)[1:]\n",
    "    if len(correlations) > 0:\n",
    "        top_corr_feature = correlations.index[0]\n",
    "        top_corr_value = correlations.iloc[0]\n",
    "        print(f\"  â€¢ Strongest numerical correlation with target: {top_corr_feature} ({top_corr_value:.3f})\")\n",
    "\n",
    "print(f\"\\nğŸš€ Ready for Feature Engineering!\")\n",
    "print(\"=\"*50)\n",
    "print(\"You now have a solid understanding of your data.\")\n",
    "print(\"Time to move to the next notebook: 02_feature_engineering.ipynb\")\n",
    "print(\"ğŸ’¡ Pro tip: Keep this analysis handy as a reference!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
