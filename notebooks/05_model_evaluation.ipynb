{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0ba3e59",
   "metadata": {},
   "source": [
    "# 📈 Model Evaluation: Measuring and Comparing Performance\n",
    "\n",
    "Welcome to **Model Evaluation** - the final and crucial step in our machine learning journey! 🎯 This is where we become data scientists and thoroughly assess our models to choose the best one for production.\n",
    "\n",
    "## 🔍 What is Model Evaluation?\n",
    "\n",
    "Model evaluation is like conducting a **comprehensive job interview** for our ML models. We test them from every angle to see which one truly understands our data and can make reliable predictions in the real world.\n",
    "\n",
    "### 🎭 **The Evaluation Challenge:**\n",
    "- **📊 Accuracy isn't everything**: A model with 95% accuracy might still be terrible\n",
    "- **⚖️ Imbalanced data requires special metrics**: Standard metrics can be misleading\n",
    "- **🎯 Business context matters**: Different errors have different costs\n",
    "- **🔮 Real-world performance**: Lab performance vs. production performance\n",
    "\n",
    "## 📚 **Comprehensive Evaluation Framework:**\n",
    "\n",
    "### 📊 **Classification Metrics Deep Dive**\n",
    "- **Accuracy**: Overall correctness (but can be misleading)\n",
    "- **Precision**: Of predicted positives, how many were correct?\n",
    "- **Recall**: Of actual positives, how many did we catch?\n",
    "- **F1-Score**: Harmonic mean of precision and recall\n",
    "- **ROC-AUC**: Ability to distinguish between classes\n",
    "- **PR-AUC**: Performance on imbalanced data\n",
    "\n",
    "### 🎯 **Advanced Evaluation Techniques**\n",
    "- **Confusion Matrix Analysis**: Detailed error breakdown\n",
    "- **ROC and PR Curves**: Visual performance assessment\n",
    "- **Feature Importance**: What drives predictions?\n",
    "- **Cross-Validation**: Robust performance estimation\n",
    "- **Error Analysis**: Understanding model failures\n",
    "\n",
    "### 🏆 **Model Selection Criteria**\n",
    "- **Primary Metric**: Best performance on key business metric\n",
    "- **Stability**: Consistent performance across different data splits\n",
    "- **Interpretability**: Can we understand and trust the model?\n",
    "- **Efficiency**: Training and prediction speed\n",
    "- **Robustness**: Performance on edge cases\n",
    "\n",
    "### 📈 **Business Impact Assessment**\n",
    "- **Cost-Benefit Analysis**: Value of correct vs. cost of errors\n",
    "- **Production Readiness**: Real-world deployment considerations\n",
    "- **Monitoring Strategy**: How to track performance over time\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 **What We'll Accomplish:**\n",
    "\n",
    "By the end of this notebook, you'll:\n",
    "- ✅ **Master evaluation metrics** for imbalanced classification\n",
    "- ✅ **Create comprehensive performance reports** with visualizations\n",
    "- ✅ **Understand model strengths and weaknesses** deeply\n",
    "- ✅ **Select the optimal model** for your specific use case\n",
    "- ✅ **Prepare for production deployment** with confidence\n",
    "- ✅ **Set up monitoring and maintenance** strategies\n",
    "\n",
    "Let's evaluate our models like true data scientists! 🔬📊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c02073ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 IMPORTING EVALUATION LIBRARIES...\n",
      "===================================\n",
      "✅ SHAP available for model interpretation\n",
      "\n",
      "📁 SETTING UP EVALUATION ENVIRONMENT...\n",
      "=====================================\n",
      "📂 Models: c:\\Users\\DELL\\Desktop\\AI-Project\\AI-Project\\models\n",
      "📂 Results: c:\\Users\\DELL\\Desktop\\AI-Project\\AI-Project\\results\n",
      "📂 Reports: c:\\Users\\DELL\\Desktop\\AI-Project\\AI-Project\\results\\reports\n",
      "📂 Figures: c:\\Users\\DELL\\Desktop\\AI-Project\\AI-Project\\results\\figures\n",
      "\n",
      "🎯 EVALUATION PARAMETERS...\n",
      "=========================\n",
      "🎲 Random state: 42\n",
      "📊 Primary metrics: ['f1_score', 'precision', 'recall', 'roc_auc', 'average_precision']\n",
      "🎯 Business metric: f1_score\n",
      "📈 Confidence level: 0.95\n",
      "\n",
      "✅ Evaluation environment ready!\n"
     ]
    }
   ],
   "source": [
    "# 📦 Step 1: Import Comprehensive Evaluation Libraries\n",
    "print(\"📦 IMPORTING EVALUATION LIBRARIES...\")\n",
    "print(\"=\"*35)\n",
    "\n",
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Enhanced plotting setup for evaluation\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"Set2\")\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Comprehensive evaluation metrics\n",
    "from sklearn.metrics import (\n",
    "    # Basic metrics\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    # Advanced metrics\n",
    "    roc_auc_score, average_precision_score, log_loss,\n",
    "    # Detailed analysis\n",
    "    classification_report, confusion_matrix,\n",
    "    # Curves and plots\n",
    "    roc_curve, precision_recall_curve, det_curve,\n",
    "    # Multi-class extensions\n",
    "    roc_auc_score, precision_recall_fscore_support\n",
    ")\n",
    "\n",
    "# Model interpretation and feature importance\n",
    "try:\n",
    "    import shap\n",
    "    print(\"✅ SHAP available for model interpretation\")\n",
    "except ImportError:\n",
    "    print(\"⚠️ SHAP not available - will use basic feature importance\")\n",
    "    shap = None\n",
    "\n",
    "# Advanced plotting\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, RocCurveDisplay, PrecisionRecallDisplay\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Model loading utilities\n",
    "import pickle\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "import json\n",
    "import sys\n",
    "\n",
    "# 📁 Step 2: Set up evaluation environment\n",
    "print(\"\\n📁 SETTING UP EVALUATION ENVIRONMENT...\")\n",
    "print(\"=\"*37)\n",
    "\n",
    "current_dir = Path.cwd()\n",
    "project_root = current_dir.parent\n",
    "src_path = project_root / 'src'\n",
    "sys.path.append(str(src_path))\n",
    "\n",
    "# Directories\n",
    "models_dir = project_root / 'models'\n",
    "results_dir = project_root / 'results'\n",
    "reports_dir = results_dir / 'reports'\n",
    "figures_dir = results_dir / 'figures'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for directory in [results_dir, reports_dir, figures_dir]:\n",
    "    directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"📂 Models: {models_dir}\")\n",
    "print(f\"📂 Results: {results_dir}\")\n",
    "print(f\"📂 Reports: {reports_dir}\")\n",
    "print(f\"📂 Figures: {figures_dir}\")\n",
    "\n",
    "# 🎯 Step 3: Define evaluation parameters\n",
    "print(f\"\\n🎯 EVALUATION PARAMETERS...\")\n",
    "print(\"=\"*25)\n",
    "\n",
    "# Evaluation configuration\n",
    "RANDOM_STATE = 42\n",
    "CONFIDENCE_LEVEL = 0.95\n",
    "DECIMAL_PLACES = 4\n",
    "\n",
    "# Metrics to focus on (for imbalanced classification)\n",
    "PRIMARY_METRICS = ['f1_score', 'precision', 'recall', 'roc_auc', 'average_precision']\n",
    "BUSINESS_METRIC = 'f1_score'  # Can be changed based on business needs\n",
    "\n",
    "print(f\"🎲 Random state: {RANDOM_STATE}\")\n",
    "print(f\"📊 Primary metrics: {PRIMARY_METRICS}\")\n",
    "print(f\"🎯 Business metric: {BUSINESS_METRIC}\")\n",
    "print(f\"📈 Confidence level: {CONFIDENCE_LEVEL}\")\n",
    "\n",
    "print(\"\\n✅ Evaluation environment ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0532a03",
   "metadata": {},
   "source": [
    "## 📊 Step 1: Load Trained Models and Test Data\n",
    "\n",
    "Let's start by loading our trained models and preparing for comprehensive evaluation. We'll analyze performance from multiple angles to make an informed decision.\n",
    "\n",
    "### 🔍 **Evaluation Philosophy:**\n",
    "- **📊 Multiple metrics**: No single metric tells the whole story\n",
    "- **📈 Visual analysis**: Charts reveal patterns that numbers hide\n",
    "- **🎯 Business context**: Consider real-world impact of predictions\n",
    "- **⚖️ Fairness assessment**: Ensure models work for all groups\n",
    "- **🔮 Generalization**: Test on truly unseen data\n",
    "\n",
    "### 📋 **Evaluation Checklist:**\n",
    "- ✅ Load trained models and test data\n",
    "- ✅ Calculate comprehensive metrics\n",
    "- ✅ Create visualization dashboards\n",
    "- ✅ Analyze feature importance\n",
    "- ✅ Assess model reliability\n",
    "- ✅ Make final model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18249a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 LOADING MODELS AND TEST DATA...\n",
      "=================================\n",
      "✅ Loaded Gradient Boosting\n",
      "✅ Loaded Logistic Regression\n",
      "✅ Loaded Random Forest\n",
      "✅ Loaded Support Vector Machine\n",
      "✅ Loaded Xgboost\n",
      "✅ Loaded evaluation data\n",
      "❌ No trained models available for evaluation\n",
      "💡 Please run the model training notebook first\n"
     ]
    }
   ],
   "source": [
    "# 📊 Step 1: Load trained models and test data\n",
    "print(\"📊 LOADING MODELS AND TEST DATA...\")\n",
    "print(\"=\"*33)\n",
    "\n",
    "# Try to load from saved files\n",
    "try:\n",
    "    trained_models = {}\n",
    "    models_loaded = False\n",
    "    \n",
    "    # Load all model files\n",
    "    model_files = list(models_dir.glob('*_model.pkl'))\n",
    "    for model_file in model_files:\n",
    "        try:\n",
    "            model_info = joblib.load(model_file)\n",
    "            model_name = model_file.stem.replace('_model', '').replace('_', ' ').title()\n",
    "            trained_models[model_name] = model_info\n",
    "            models_loaded = True\n",
    "            print(f\"✅ Loaded {model_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to load {model_file}: {e}\")\n",
    "    \n",
    "    # Load evaluation data\n",
    "    data_path = models_dir / 'evaluation_data.pkl'\n",
    "    if data_path.exists():\n",
    "        evaluation_data = joblib.load(data_path)\n",
    "        X_test = evaluation_data['X_test']\n",
    "        y_test = evaluation_data['y_test']\n",
    "        X_train = evaluation_data['X_train']\n",
    "        X_test_scaled = evaluation_data['X_test_scaled']\n",
    "        feature_columns = evaluation_data['feature_columns']\n",
    "        target_col = evaluation_data['target_col']\n",
    "        print(\"✅ Loaded evaluation data\")\n",
    "    \n",
    "    models_available = models_loaded and 'X_test' in locals()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading from files: {e}\")\n",
    "    models_available = False\n",
    "\n",
    "if not models_available:\n",
    "    print(\"🔄 Trying to use in-memory models...\")\n",
    "    # Check if models are available from training notebook\n",
    "    if 'trained_models' in globals() and 'X_test' in globals():\n",
    "        print(\"✅ Using models from training notebook session\")\n",
    "        models_available = True\n",
    "    else:\n",
    "        print(\"❌ No models available. Please run training first.\")\n",
    "        models_available = False\n",
    "    \n",
    "    # 📈 Step 2: Generate predictions on test set\n",
    "    print(f\"\\n📈 GENERATING TEST SET PREDICTIONS...\")\n",
    "    print(\"=\"*36)\n",
    "    \n",
    "    test_results = {}\n",
    "    \n",
    "    for model_name, model_info in trained_models.items():\n",
    "        print(f\"\\n🔮 Evaluating {model_name}...\")\n",
    "        \n",
    "        try:\n",
    "            model = model_info['model']\n",
    "            needs_scaling = model_info['needs_scaling']\n",
    "            \n",
    "            # Use appropriate test data (scaled or original)\n",
    "            X_test_use = X_test_scaled if needs_scaling else X_test\n",
    "            \n",
    "            # Generate predictions\n",
    "            y_test_pred = model.predict(X_test_use)\n",
    "            \n",
    "            # Generate probability predictions if available\n",
    "            if hasattr(model, 'predict_proba'):\n",
    "                y_test_proba = model.predict_proba(X_test_use)\n",
    "                y_test_proba_pos = y_test_proba[:, 1]  # Probability of positive class\n",
    "            else:\n",
    "                y_test_proba = None\n",
    "                y_test_proba_pos = None\n",
    "            \n",
    "            # Calculate comprehensive metrics\n",
    "            test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "            test_precision = precision_score(y_test, y_test_pred, average='binary')\n",
    "            test_recall = recall_score(y_test, y_test_pred, average='binary')\n",
    "            test_f1 = f1_score(y_test, y_test_pred, average='binary')\n",
    "            \n",
    "            if y_test_proba_pos is not None:\n",
    "                test_roc_auc = roc_auc_score(y_test, y_test_proba_pos)\n",
    "                test_pr_auc = average_precision_score(y_test, y_test_proba_pos)\n",
    "            else:\n",
    "                test_roc_auc = None\n",
    "                test_pr_auc = None\n",
    "            \n",
    "            # Store results\n",
    "            test_results[model_name] = {\n",
    "                'predictions': y_test_pred,\n",
    "                'probabilities': y_test_proba_pos,\n",
    "                'accuracy': test_accuracy,\n",
    "                'precision': test_precision,\n",
    "                'recall': test_recall,\n",
    "                'f1_score': test_f1,\n",
    "                'roc_auc': test_roc_auc,\n",
    "                'pr_auc': test_pr_auc,\n",
    "                'confusion_matrix': confusion_matrix(y_test, y_test_pred)\n",
    "            }\n",
    "            \n",
    "            print(f\"  ✅ Metrics calculated:\")\n",
    "            print(f\"     • Accuracy: {test_accuracy:.4f}\")\n",
    "            print(f\"     • Precision: {test_precision:.4f}\")\n",
    "            print(f\"     • Recall: {test_recall:.4f}\")\n",
    "            print(f\"     • F1-Score: {test_f1:.4f}\")\n",
    "            if test_roc_auc:\n",
    "                print(f\"     • ROC-AUC: {test_roc_auc:.4f}\")\n",
    "                print(f\"     • PR-AUC: {test_pr_auc:.4f}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Error: {str(e)}\")\n",
    "            test_results[model_name] = None\n",
    "    \n",
    "    # 📊 Step 3: Create performance comparison table\n",
    "    print(f\"\\n📊 PERFORMANCE COMPARISON TABLE:\")\n",
    "    print(\"=\"*33)\n",
    "    \n",
    "    # Create DataFrame for easy comparison\n",
    "    results_data = []\n",
    "    for model_name, results in test_results.items():\n",
    "        if results is not None:\n",
    "            row = {\n",
    "                'Model': model_name,\n",
    "                'Accuracy': results['accuracy'],\n",
    "                'Precision': results['precision'],\n",
    "                'Recall': results['recall'],\n",
    "                'F1-Score': results['f1_score'],\n",
    "                'ROC-AUC': results['roc_auc'] if results['roc_auc'] else 'N/A',\n",
    "                'PR-AUC': results['pr_auc'] if results['pr_auc'] else 'N/A'\n",
    "            }\n",
    "            results_data.append(row)\n",
    "    \n",
    "    results_df = pd.DataFrame(results_data)\n",
    "    \n",
    "    if not results_df.empty:\n",
    "        # Display formatted table\n",
    "        print(results_df.round(4).to_string(index=False))\n",
    "        \n",
    "        # Find best models for different metrics\n",
    "        print(f\"\\n🏆 BEST PERFORMERS:\")\n",
    "        print(\"=\"*18)\n",
    "        \n",
    "        metrics_to_rank = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "        for metric in metrics_to_rank:\n",
    "            if metric in results_df.columns:\n",
    "                best_idx = results_df[metric].idxmax()\n",
    "                best_model = results_df.loc[best_idx, 'Model']\n",
    "                best_score = results_df.loc[best_idx, metric]\n",
    "                print(f\"  • Best {metric}: {best_model} ({best_score:.4f})\")\n",
    "        \n",
    "        # ROC-AUC (only for models with probabilities)\n",
    "        roc_auc_scores = results_df[results_df['ROC-AUC'] != 'N/A']['ROC-AUC']\n",
    "        if not roc_auc_scores.empty:\n",
    "            best_roc_idx = roc_auc_scores.idxmax()\n",
    "            best_roc_model = results_df.loc[best_roc_idx, 'Model']\n",
    "            best_roc_score = roc_auc_scores.loc[best_roc_idx]\n",
    "            print(f\"  • Best ROC-AUC: {best_roc_model} ({best_roc_score:.4f})\")\n",
    "    \n",
    "    # 📈 Step 4: Create comprehensive visualizations\n",
    "    print(f\"\\n📈 CREATING PERFORMANCE VISUALIZATIONS...\")\n",
    "    print(\"=\"*38)\n",
    "    \n",
    "    # Set up the visualization grid\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    # 1. Performance metrics comparison (bar chart)\n",
    "    ax1 = plt.subplot(2, 3, 1)\n",
    "    if not results_df.empty:\n",
    "        metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "        x = np.arange(len(results_df))\n",
    "        width = 0.2\n",
    "        \n",
    "        for i, metric in enumerate(metrics):\n",
    "            values = results_df[metric].values\n",
    "            ax1.bar(x + i*width, values, width, label=metric, alpha=0.8)\n",
    "        \n",
    "        ax1.set_xlabel('Models')\n",
    "        ax1.set_ylabel('Score')\n",
    "        ax1.set_title('📊 Model Performance Comparison', fontweight='bold')\n",
    "        ax1.set_xticks(x + width * 1.5)\n",
    "        ax1.set_xticklabels(results_df['Model'], rotation=45, ha='right')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.set_ylim(0, 1)\n",
    "    \n",
    "    # 2. ROC Curves comparison\n",
    "    ax2 = plt.subplot(2, 3, 2)\n",
    "    \n",
    "    for model_name, results in test_results.items():\n",
    "        if results is not None and results['probabilities'] is not None:\n",
    "            fpr, tpr, _ = roc_curve(y_test, results['probabilities'])\n",
    "            roc_auc = results['roc_auc']\n",
    "            ax2.plot(fpr, tpr, label=f'{model_name} (AUC = {roc_auc:.3f})', linewidth=2)\n",
    "    \n",
    "    ax2.plot([0, 1], [0, 1], 'k--', linewidth=1, alpha=0.5)\n",
    "    ax2.set_xlabel('False Positive Rate')\n",
    "    ax2.set_ylabel('True Positive Rate')\n",
    "    ax2.set_title('📈 ROC Curves Comparison', fontweight='bold')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Precision-Recall Curves\n",
    "    ax3 = plt.subplot(2, 3, 3)\n",
    "    \n",
    "    for model_name, results in test_results.items():\n",
    "        if results is not None and results['probabilities'] is not None:\n",
    "            precision, recall, _ = precision_recall_curve(y_test, results['probabilities'])\n",
    "            pr_auc = results['pr_auc']\n",
    "            ax3.plot(recall, precision, label=f'{model_name} (AUC = {pr_auc:.3f})', linewidth=2)\n",
    "    \n",
    "    # Baseline (random classifier)\n",
    "    baseline = (y_test == 1).mean()\n",
    "    ax3.axhline(y=baseline, color='k', linestyle='--', alpha=0.5, label=f'Baseline = {baseline:.3f}')\n",
    "    \n",
    "    ax3.set_xlabel('Recall')\n",
    "    ax3.set_ylabel('Precision')\n",
    "    ax3.set_title('📊 Precision-Recall Curves', fontweight='bold')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Confusion Matrices (show best model)\n",
    "    if not results_df.empty:\n",
    "        best_model_name = results_df.loc[results_df['F1-Score'].idxmax(), 'Model']\n",
    "        best_results = test_results[best_model_name]\n",
    "        \n",
    "        ax4 = plt.subplot(2, 3, 4)\n",
    "        cm = best_results['confusion_matrix']\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax4)\n",
    "        ax4.set_title(f'🎯 Confusion Matrix: {best_model_name}', fontweight='bold')\n",
    "        ax4.set_xlabel('Predicted')\n",
    "        ax4.set_ylabel('Actual')\n",
    "    \n",
    "    # 5. Feature Importance (for tree-based models)\n",
    "    ax5 = plt.subplot(2, 3, 5)\n",
    "    \n",
    "    # Find a tree-based model for feature importance\n",
    "    tree_models = ['Random Forest', 'XGBoost', 'Gradient Boosting']\n",
    "    importance_model = None\n",
    "    \n",
    "    for model_name in tree_models:\n",
    "        if model_name in trained_models:\n",
    "            model = trained_models[model_name]['model']\n",
    "            if hasattr(model, 'feature_importances_'):\n",
    "                importance_model = model_name\n",
    "                break\n",
    "    \n",
    "    if importance_model:\n",
    "        model = trained_models[importance_model]['model']\n",
    "        importances = model.feature_importances_\n",
    "        \n",
    "        # Get feature names\n",
    "        feature_names = X_train.columns if not trained_models[importance_model]['needs_scaling'] else X_train_scaled.columns\n",
    "        \n",
    "        # Sort by importance\n",
    "        indices = np.argsort(importances)[::-1][:10]  # Top 10 features\n",
    "        \n",
    "        ax5.bar(range(len(indices)), importances[indices], alpha=0.8)\n",
    "        ax5.set_xlabel('Features')\n",
    "        ax5.set_ylabel('Importance')\n",
    "        ax5.set_title(f'🌟 Feature Importance: {importance_model}', fontweight='bold')\n",
    "        ax5.set_xticks(range(len(indices)))\n",
    "        ax5.set_xticklabels([feature_names[i] for i in indices], rotation=45, ha='right')\n",
    "        ax5.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Model Reliability (performance consistency)\n",
    "    ax6 = plt.subplot(2, 3, 6)\n",
    "    \n",
    "    if 'cv_results' in globals():\n",
    "        cv_f1_means = []\n",
    "        cv_f1_stds = []\n",
    "        cv_model_names = []\n",
    "        \n",
    "        for model_name, cv_data in cv_results.items():\n",
    "            if cv_data and 'f1' in cv_data and cv_data['f1']:\n",
    "                cv_f1_means.append(cv_data['f1']['mean'])\n",
    "                cv_f1_stds.append(cv_data['f1']['std'])\n",
    "                cv_model_names.append(model_name)\n",
    "        \n",
    "        if cv_f1_means:\n",
    "            x_pos = range(len(cv_model_names))\n",
    "            ax6.bar(x_pos, cv_f1_means, yerr=cv_f1_stds, capsize=5, alpha=0.8)\n",
    "            ax6.set_xlabel('Models')\n",
    "            ax6.set_ylabel('F1-Score')\n",
    "            ax6.set_title('📊 Model Reliability (CV F1 ± Std)', fontweight='bold')\n",
    "            ax6.set_xticks(x_pos)\n",
    "            ax6.set_xticklabels(cv_model_names, rotation=45, ha='right')\n",
    "            ax6.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"✅ Comprehensive evaluation complete!\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No trained models available for evaluation\")\n",
    "    print(\"💡 Please run the model training notebook first\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
