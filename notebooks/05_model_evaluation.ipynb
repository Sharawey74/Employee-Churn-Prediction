{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0ba3e59",
   "metadata": {},
   "source": [
    "# üìà Model Evaluation: Measuring and Comparing Performance\n",
    "\n",
    "Welcome to **Model Evaluation** - the final and crucial step in our machine learning journey! üéØ This is where we become data scientists and thoroughly assess our models to choose the best one for production.\n",
    "\n",
    "## üîç What is Model Evaluation?\n",
    "\n",
    "Model evaluation is like conducting a **comprehensive job interview** for our ML models. We test them from every angle to see which one truly understands our data and can make reliable predictions in the real world.\n",
    "\n",
    "### üé≠ **The Evaluation Challenge:**\n",
    "- **üìä Accuracy isn't everything**: A model with 95% accuracy might still be terrible\n",
    "- **‚öñÔ∏è Imbalanced data requires special metrics**: Standard metrics can be misleading\n",
    "- **üéØ Business context matters**: Different errors have different costs\n",
    "- **üîÆ Real-world performance**: Lab performance vs. production performance\n",
    "\n",
    "## üìö **Comprehensive Evaluation Framework:**\n",
    "\n",
    "### üìä **Classification Metrics Deep Dive**\n",
    "- **Accuracy**: Overall correctness (but can be misleading)\n",
    "- **Precision**: Of predicted positives, how many were correct?\n",
    "- **Recall**: Of actual positives, how many did we catch?\n",
    "- **F1-Score**: Harmonic mean of precision and recall\n",
    "- **ROC-AUC**: Ability to distinguish between classes\n",
    "- **PR-AUC**: Performance on imbalanced data\n",
    "\n",
    "### üéØ **Advanced Evaluation Techniques**\n",
    "- **Confusion Matrix Analysis**: Detailed error breakdown\n",
    "- **ROC and PR Curves**: Visual performance assessment\n",
    "- **Feature Importance**: What drives predictions?\n",
    "- **Cross-Validation**: Robust performance estimation\n",
    "- **Error Analysis**: Understanding model failures\n",
    "\n",
    "### üèÜ **Model Selection Criteria**\n",
    "- **Primary Metric**: Best performance on key business metric\n",
    "- **Stability**: Consistent performance across different data splits\n",
    "- **Interpretability**: Can we understand and trust the model?\n",
    "- **Efficiency**: Training and prediction speed\n",
    "- **Robustness**: Performance on edge cases\n",
    "\n",
    "### üìà **Business Impact Assessment**\n",
    "- **Cost-Benefit Analysis**: Value of correct vs. cost of errors\n",
    "- **Production Readiness**: Real-world deployment considerations\n",
    "- **Monitoring Strategy**: How to track performance over time\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ **What We'll Accomplish:**\n",
    "\n",
    "By the end of this notebook, you'll:\n",
    "- ‚úÖ **Master evaluation metrics** for imbalanced classification\n",
    "- ‚úÖ **Create comprehensive performance reports** with visualizations\n",
    "- ‚úÖ **Understand model strengths and weaknesses** deeply\n",
    "- ‚úÖ **Select the optimal model** for your specific use case\n",
    "- ‚úÖ **Prepare for production deployment** with confidence\n",
    "- ‚úÖ **Set up monitoring and maintenance** strategies\n",
    "\n",
    "Let's evaluate our models like true data scientists! üî¨üìä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c02073ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ IMPORTING EVALUATION LIBRARIES...\n",
      "===================================\n",
      "‚úÖ SHAP available for model interpretation\n",
      "\n",
      "üìÅ SETTING UP EVALUATION ENVIRONMENT...\n",
      "=====================================\n",
      "üìÇ Models: c:\\Users\\DELL\\Desktop\\AI-Project\\AI-Project\\models\n",
      "üìÇ Results: c:\\Users\\DELL\\Desktop\\AI-Project\\AI-Project\\results\n",
      "üìÇ Reports: c:\\Users\\DELL\\Desktop\\AI-Project\\AI-Project\\results\\reports\n",
      "üìÇ Figures: c:\\Users\\DELL\\Desktop\\AI-Project\\AI-Project\\results\\figures\n",
      "\n",
      "üéØ EVALUATION PARAMETERS...\n",
      "=========================\n",
      "üé≤ Random state: 42\n",
      "üìä Primary metrics: ['f1_score', 'precision', 'recall', 'roc_auc', 'average_precision']\n",
      "üéØ Business metric: f1_score\n",
      "üìà Confidence level: 0.95\n",
      "\n",
      "‚úÖ Evaluation environment ready!\n"
     ]
    }
   ],
   "source": [
    "# üì¶ Step 1: Import Comprehensive Evaluation Libraries\n",
    "print(\"üì¶ IMPORTING EVALUATION LIBRARIES...\")\n",
    "print(\"=\"*35)\n",
    "\n",
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Enhanced plotting setup for evaluation\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"Set2\")\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Comprehensive evaluation metrics\n",
    "from sklearn.metrics import (\n",
    "    # Basic metrics\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    # Advanced metrics\n",
    "    roc_auc_score, average_precision_score, log_loss,\n",
    "    # Detailed analysis\n",
    "    classification_report, confusion_matrix,\n",
    "    # Curves and plots\n",
    "    roc_curve, precision_recall_curve, det_curve,\n",
    "    # Multi-class extensions\n",
    "    roc_auc_score, precision_recall_fscore_support\n",
    ")\n",
    "\n",
    "# Model interpretation and feature importance\n",
    "try:\n",
    "    import shap\n",
    "    print(\"‚úÖ SHAP available for model interpretation\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è SHAP not available - will use basic feature importance\")\n",
    "    shap = None\n",
    "\n",
    "# Advanced plotting\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, RocCurveDisplay, PrecisionRecallDisplay\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Model loading utilities\n",
    "import pickle\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "import json\n",
    "import sys\n",
    "\n",
    "# üìÅ Step 2: Set up evaluation environment\n",
    "print(\"\\nüìÅ SETTING UP EVALUATION ENVIRONMENT...\")\n",
    "print(\"=\"*37)\n",
    "\n",
    "current_dir = Path.cwd()\n",
    "project_root = current_dir.parent\n",
    "src_path = project_root / 'src'\n",
    "sys.path.append(str(src_path))\n",
    "\n",
    "# Directories\n",
    "models_dir = project_root / 'models'\n",
    "results_dir = project_root / 'results'\n",
    "reports_dir = results_dir / 'reports'\n",
    "figures_dir = results_dir / 'figures'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for directory in [results_dir, reports_dir, figures_dir]:\n",
    "    directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"üìÇ Models: {models_dir}\")\n",
    "print(f\"üìÇ Results: {results_dir}\")\n",
    "print(f\"üìÇ Reports: {reports_dir}\")\n",
    "print(f\"üìÇ Figures: {figures_dir}\")\n",
    "\n",
    "# üéØ Step 3: Define evaluation parameters\n",
    "print(f\"\\nüéØ EVALUATION PARAMETERS...\")\n",
    "print(\"=\"*25)\n",
    "\n",
    "# Evaluation configuration\n",
    "RANDOM_STATE = 42\n",
    "CONFIDENCE_LEVEL = 0.95\n",
    "DECIMAL_PLACES = 4\n",
    "\n",
    "# Metrics to focus on (for imbalanced classification)\n",
    "PRIMARY_METRICS = ['f1_score', 'precision', 'recall', 'roc_auc', 'average_precision']\n",
    "BUSINESS_METRIC = 'f1_score'  # Can be changed based on business needs\n",
    "\n",
    "print(f\"üé≤ Random state: {RANDOM_STATE}\")\n",
    "print(f\"üìä Primary metrics: {PRIMARY_METRICS}\")\n",
    "print(f\"üéØ Business metric: {BUSINESS_METRIC}\")\n",
    "print(f\"üìà Confidence level: {CONFIDENCE_LEVEL}\")\n",
    "\n",
    "print(\"\\n‚úÖ Evaluation environment ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0532a03",
   "metadata": {},
   "source": [
    "## üìä Step 1: Load Trained Models and Test Data\n",
    "\n",
    "Let's start by loading our trained models and preparing for comprehensive evaluation. We'll analyze performance from multiple angles to make an informed decision.\n",
    "\n",
    "### üîç **Evaluation Philosophy:**\n",
    "- **üìä Multiple metrics**: No single metric tells the whole story\n",
    "- **üìà Visual analysis**: Charts reveal patterns that numbers hide\n",
    "- **üéØ Business context**: Consider real-world impact of predictions\n",
    "- **‚öñÔ∏è Fairness assessment**: Ensure models work for all groups\n",
    "- **üîÆ Generalization**: Test on truly unseen data\n",
    "\n",
    "### üìã **Evaluation Checklist:**\n",
    "- ‚úÖ Load trained models and test data\n",
    "- ‚úÖ Calculate comprehensive metrics\n",
    "- ‚úÖ Create visualization dashboards\n",
    "- ‚úÖ Analyze feature importance\n",
    "- ‚úÖ Assess model reliability\n",
    "- ‚úÖ Make final model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18249a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä LOADING MODELS AND TEST DATA...\n",
      "=================================\n",
      "‚úÖ Loaded Gradient Boosting\n",
      "‚úÖ Loaded Logistic Regression\n",
      "‚úÖ Loaded Random Forest\n",
      "‚úÖ Loaded Support Vector Machine\n",
      "‚úÖ Loaded Xgboost\n",
      "‚úÖ Loaded evaluation data\n",
      "‚ùå No trained models available for evaluation\n",
      "üí° Please run the model training notebook first\n"
     ]
    }
   ],
   "source": [
    "# üìä Step 1: Load trained models and test data\n",
    "print(\"üìä LOADING MODELS AND TEST DATA...\")\n",
    "print(\"=\"*33)\n",
    "\n",
    "# Try to load from saved files\n",
    "try:\n",
    "    trained_models = {}\n",
    "    models_loaded = False\n",
    "    \n",
    "    # Load all model files\n",
    "    model_files = list(models_dir.glob('*_model.pkl'))\n",
    "    for model_file in model_files:\n",
    "        try:\n",
    "            model_info = joblib.load(model_file)\n",
    "            model_name = model_file.stem.replace('_model', '').replace('_', ' ').title()\n",
    "            trained_models[model_name] = model_info\n",
    "            models_loaded = True\n",
    "            print(f\"‚úÖ Loaded {model_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to load {model_file}: {e}\")\n",
    "    \n",
    "    # Load evaluation data\n",
    "    data_path = models_dir / 'evaluation_data.pkl'\n",
    "    if data_path.exists():\n",
    "        evaluation_data = joblib.load(data_path)\n",
    "        X_test = evaluation_data['X_test']\n",
    "        y_test = evaluation_data['y_test']\n",
    "        X_train = evaluation_data['X_train']\n",
    "        X_test_scaled = evaluation_data['X_test_scaled']\n",
    "        feature_columns = evaluation_data['feature_columns']\n",
    "        target_col = evaluation_data['target_col']\n",
    "        print(\"‚úÖ Loaded evaluation data\")\n",
    "    \n",
    "    models_available = models_loaded and 'X_test' in locals()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading from files: {e}\")\n",
    "    models_available = False\n",
    "\n",
    "if not models_available:\n",
    "    print(\"üîÑ Trying to use in-memory models...\")\n",
    "    # Check if models are available from training notebook\n",
    "    if 'trained_models' in globals() and 'X_test' in globals():\n",
    "        print(\"‚úÖ Using models from training notebook session\")\n",
    "        models_available = True\n",
    "    else:\n",
    "        print(\"‚ùå No models available. Please run training first.\")\n",
    "        models_available = False\n",
    "    \n",
    "    # üìà Step 2: Generate predictions on test set\n",
    "    print(f\"\\nüìà GENERATING TEST SET PREDICTIONS...\")\n",
    "    print(\"=\"*36)\n",
    "    \n",
    "    test_results = {}\n",
    "    \n",
    "    for model_name, model_info in trained_models.items():\n",
    "        print(f\"\\nüîÆ Evaluating {model_name}...\")\n",
    "        \n",
    "        try:\n",
    "            model = model_info['model']\n",
    "            needs_scaling = model_info['needs_scaling']\n",
    "            \n",
    "            # Use appropriate test data (scaled or original)\n",
    "            X_test_use = X_test_scaled if needs_scaling else X_test\n",
    "            \n",
    "            # Generate predictions\n",
    "            y_test_pred = model.predict(X_test_use)\n",
    "            \n",
    "            # Generate probability predictions if available\n",
    "            if hasattr(model, 'predict_proba'):\n",
    "                y_test_proba = model.predict_proba(X_test_use)\n",
    "                y_test_proba_pos = y_test_proba[:, 1]  # Probability of positive class\n",
    "            else:\n",
    "                y_test_proba = None\n",
    "                y_test_proba_pos = None\n",
    "            \n",
    "            # Calculate comprehensive metrics\n",
    "            test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "            test_precision = precision_score(y_test, y_test_pred, average='binary')\n",
    "            test_recall = recall_score(y_test, y_test_pred, average='binary')\n",
    "            test_f1 = f1_score(y_test, y_test_pred, average='binary')\n",
    "            \n",
    "            if y_test_proba_pos is not None:\n",
    "                test_roc_auc = roc_auc_score(y_test, y_test_proba_pos)\n",
    "                test_pr_auc = average_precision_score(y_test, y_test_proba_pos)\n",
    "            else:\n",
    "                test_roc_auc = None\n",
    "                test_pr_auc = None\n",
    "            \n",
    "            # Store results\n",
    "            test_results[model_name] = {\n",
    "                'predictions': y_test_pred,\n",
    "                'probabilities': y_test_proba_pos,\n",
    "                'accuracy': test_accuracy,\n",
    "                'precision': test_precision,\n",
    "                'recall': test_recall,\n",
    "                'f1_score': test_f1,\n",
    "                'roc_auc': test_roc_auc,\n",
    "                'pr_auc': test_pr_auc,\n",
    "                'confusion_matrix': confusion_matrix(y_test, y_test_pred)\n",
    "            }\n",
    "            \n",
    "            print(f\"  ‚úÖ Metrics calculated:\")\n",
    "            print(f\"     ‚Ä¢ Accuracy: {test_accuracy:.4f}\")\n",
    "            print(f\"     ‚Ä¢ Precision: {test_precision:.4f}\")\n",
    "            print(f\"     ‚Ä¢ Recall: {test_recall:.4f}\")\n",
    "            print(f\"     ‚Ä¢ F1-Score: {test_f1:.4f}\")\n",
    "            if test_roc_auc:\n",
    "                print(f\"     ‚Ä¢ ROC-AUC: {test_roc_auc:.4f}\")\n",
    "                print(f\"     ‚Ä¢ PR-AUC: {test_pr_auc:.4f}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Error: {str(e)}\")\n",
    "            test_results[model_name] = None\n",
    "    \n",
    "    # üìä Step 3: Create performance comparison table\n",
    "    print(f\"\\nüìä PERFORMANCE COMPARISON TABLE:\")\n",
    "    print(\"=\"*33)\n",
    "    \n",
    "    # Create DataFrame for easy comparison\n",
    "    results_data = []\n",
    "    for model_name, results in test_results.items():\n",
    "        if results is not None:\n",
    "            row = {\n",
    "                'Model': model_name,\n",
    "                'Accuracy': results['accuracy'],\n",
    "                'Precision': results['precision'],\n",
    "                'Recall': results['recall'],\n",
    "                'F1-Score': results['f1_score'],\n",
    "                'ROC-AUC': results['roc_auc'] if results['roc_auc'] else 'N/A',\n",
    "                'PR-AUC': results['pr_auc'] if results['pr_auc'] else 'N/A'\n",
    "            }\n",
    "            results_data.append(row)\n",
    "    \n",
    "    results_df = pd.DataFrame(results_data)\n",
    "    \n",
    "    if not results_df.empty:\n",
    "        # Display formatted table\n",
    "        print(results_df.round(4).to_string(index=False))\n",
    "        \n",
    "        # Find best models for different metrics\n",
    "        print(f\"\\nüèÜ BEST PERFORMERS:\")\n",
    "        print(\"=\"*18)\n",
    "        \n",
    "        metrics_to_rank = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "        for metric in metrics_to_rank:\n",
    "            if metric in results_df.columns:\n",
    "                best_idx = results_df[metric].idxmax()\n",
    "                best_model = results_df.loc[best_idx, 'Model']\n",
    "                best_score = results_df.loc[best_idx, metric]\n",
    "                print(f\"  ‚Ä¢ Best {metric}: {best_model} ({best_score:.4f})\")\n",
    "        \n",
    "        # ROC-AUC (only for models with probabilities)\n",
    "        roc_auc_scores = results_df[results_df['ROC-AUC'] != 'N/A']['ROC-AUC']\n",
    "        if not roc_auc_scores.empty:\n",
    "            best_roc_idx = roc_auc_scores.idxmax()\n",
    "            best_roc_model = results_df.loc[best_roc_idx, 'Model']\n",
    "            best_roc_score = roc_auc_scores.loc[best_roc_idx]\n",
    "            print(f\"  ‚Ä¢ Best ROC-AUC: {best_roc_model} ({best_roc_score:.4f})\")\n",
    "    \n",
    "    # üìà Step 4: Create comprehensive visualizations\n",
    "    print(f\"\\nüìà CREATING PERFORMANCE VISUALIZATIONS...\")\n",
    "    print(\"=\"*38)\n",
    "    \n",
    "    # Set up the visualization grid\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    # 1. Performance metrics comparison (bar chart)\n",
    "    ax1 = plt.subplot(2, 3, 1)\n",
    "    if not results_df.empty:\n",
    "        metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "        x = np.arange(len(results_df))\n",
    "        width = 0.2\n",
    "        \n",
    "        for i, metric in enumerate(metrics):\n",
    "            values = results_df[metric].values\n",
    "            ax1.bar(x + i*width, values, width, label=metric, alpha=0.8)\n",
    "        \n",
    "        ax1.set_xlabel('Models')\n",
    "        ax1.set_ylabel('Score')\n",
    "        ax1.set_title('üìä Model Performance Comparison', fontweight='bold')\n",
    "        ax1.set_xticks(x + width * 1.5)\n",
    "        ax1.set_xticklabels(results_df['Model'], rotation=45, ha='right')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.set_ylim(0, 1)\n",
    "    \n",
    "    # 2. ROC Curves comparison\n",
    "    ax2 = plt.subplot(2, 3, 2)\n",
    "    \n",
    "    for model_name, results in test_results.items():\n",
    "        if results is not None and results['probabilities'] is not None:\n",
    "            fpr, tpr, _ = roc_curve(y_test, results['probabilities'])\n",
    "            roc_auc = results['roc_auc']\n",
    "            ax2.plot(fpr, tpr, label=f'{model_name} (AUC = {roc_auc:.3f})', linewidth=2)\n",
    "    \n",
    "    ax2.plot([0, 1], [0, 1], 'k--', linewidth=1, alpha=0.5)\n",
    "    ax2.set_xlabel('False Positive Rate')\n",
    "    ax2.set_ylabel('True Positive Rate')\n",
    "    ax2.set_title('üìà ROC Curves Comparison', fontweight='bold')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Precision-Recall Curves\n",
    "    ax3 = plt.subplot(2, 3, 3)\n",
    "    \n",
    "    for model_name, results in test_results.items():\n",
    "        if results is not None and results['probabilities'] is not None:\n",
    "            precision, recall, _ = precision_recall_curve(y_test, results['probabilities'])\n",
    "            pr_auc = results['pr_auc']\n",
    "            ax3.plot(recall, precision, label=f'{model_name} (AUC = {pr_auc:.3f})', linewidth=2)\n",
    "    \n",
    "    # Baseline (random classifier)\n",
    "    baseline = (y_test == 1).mean()\n",
    "    ax3.axhline(y=baseline, color='k', linestyle='--', alpha=0.5, label=f'Baseline = {baseline:.3f}')\n",
    "    \n",
    "    ax3.set_xlabel('Recall')\n",
    "    ax3.set_ylabel('Precision')\n",
    "    ax3.set_title('üìä Precision-Recall Curves', fontweight='bold')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Confusion Matrices (show best model)\n",
    "    if not results_df.empty:\n",
    "        best_model_name = results_df.loc[results_df['F1-Score'].idxmax(), 'Model']\n",
    "        best_results = test_results[best_model_name]\n",
    "        \n",
    "        ax4 = plt.subplot(2, 3, 4)\n",
    "        cm = best_results['confusion_matrix']\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax4)\n",
    "        ax4.set_title(f'üéØ Confusion Matrix: {best_model_name}', fontweight='bold')\n",
    "        ax4.set_xlabel('Predicted')\n",
    "        ax4.set_ylabel('Actual')\n",
    "    \n",
    "    # 5. Feature Importance (for tree-based models)\n",
    "    ax5 = plt.subplot(2, 3, 5)\n",
    "    \n",
    "    # Find a tree-based model for feature importance\n",
    "    tree_models = ['Random Forest', 'XGBoost', 'Gradient Boosting']\n",
    "    importance_model = None\n",
    "    \n",
    "    for model_name in tree_models:\n",
    "        if model_name in trained_models:\n",
    "            model = trained_models[model_name]['model']\n",
    "            if hasattr(model, 'feature_importances_'):\n",
    "                importance_model = model_name\n",
    "                break\n",
    "    \n",
    "    if importance_model:\n",
    "        model = trained_models[importance_model]['model']\n",
    "        importances = model.feature_importances_\n",
    "        \n",
    "        # Get feature names\n",
    "        feature_names = X_train.columns if not trained_models[importance_model]['needs_scaling'] else X_train_scaled.columns\n",
    "        \n",
    "        # Sort by importance\n",
    "        indices = np.argsort(importances)[::-1][:10]  # Top 10 features\n",
    "        \n",
    "        ax5.bar(range(len(indices)), importances[indices], alpha=0.8)\n",
    "        ax5.set_xlabel('Features')\n",
    "        ax5.set_ylabel('Importance')\n",
    "        ax5.set_title(f'üåü Feature Importance: {importance_model}', fontweight='bold')\n",
    "        ax5.set_xticks(range(len(indices)))\n",
    "        ax5.set_xticklabels([feature_names[i] for i in indices], rotation=45, ha='right')\n",
    "        ax5.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Model Reliability (performance consistency)\n",
    "    ax6 = plt.subplot(2, 3, 6)\n",
    "    \n",
    "    if 'cv_results' in globals():\n",
    "        cv_f1_means = []\n",
    "        cv_f1_stds = []\n",
    "        cv_model_names = []\n",
    "        \n",
    "        for model_name, cv_data in cv_results.items():\n",
    "            if cv_data and 'f1' in cv_data and cv_data['f1']:\n",
    "                cv_f1_means.append(cv_data['f1']['mean'])\n",
    "                cv_f1_stds.append(cv_data['f1']['std'])\n",
    "                cv_model_names.append(model_name)\n",
    "        \n",
    "        if cv_f1_means:\n",
    "            x_pos = range(len(cv_model_names))\n",
    "            ax6.bar(x_pos, cv_f1_means, yerr=cv_f1_stds, capsize=5, alpha=0.8)\n",
    "            ax6.set_xlabel('Models')\n",
    "            ax6.set_ylabel('F1-Score')\n",
    "            ax6.set_title('üìä Model Reliability (CV F1 ¬± Std)', fontweight='bold')\n",
    "            ax6.set_xticks(x_pos)\n",
    "            ax6.set_xticklabels(cv_model_names, rotation=45, ha='right')\n",
    "            ax6.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úÖ Comprehensive evaluation complete!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No trained models available for evaluation\")\n",
    "    print(\"üí° Please run the model training notebook first\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
