{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0957f5f1",
   "metadata": {},
   "source": [
    "# ⚖️ Class Imbalance Analysis: Deep Dive into Data Balance\n",
    "\n",
    "Welcome to our **Class Imbalance Analysis**! 🎯 This is where we become data detectives, investigating the balance (or imbalance) in our target variable and learning advanced techniques to handle it.\n",
    "\n",
    "## 🔍 What is Class Imbalance?\n",
    "\n",
    "Imagine a medical test where 99% of people are healthy and only 1% have a disease. If our model just predicts \"healthy\" for everyone, it gets 99% accuracy! But it completely fails at its actual job - detecting the disease. 😰\n",
    "\n",
    "### 🎯 **Real-World Examples:**\n",
    "- **📧 Email Spam**: 95% normal emails, 5% spam\n",
    "- **🏥 Medical Diagnosis**: 99% healthy, 1% disease cases  \n",
    "- **💳 Fraud Detection**: 99.9% legitimate, 0.1% fraudulent\n",
    "- **👥 Employee Attrition**: 85% stay, 15% leave\n",
    "\n",
    "## 📚 What We'll Master Today:\n",
    "\n",
    "### 🔍 **Imbalance Detection & Analysis**\n",
    "- Measure imbalance severity\n",
    "- Understand impact on model performance  \n",
    "- Visualize class distributions\n",
    "- Identify which classes are problematic\n",
    "\n",
    "### 🛠️ **Advanced Balancing Techniques**\n",
    "- **SMOTE** variants (BorderlineSMOTE, ADASYN)\n",
    "- **Undersampling** methods (EditedNearestNeighbours, Tomek Links)\n",
    "- **Ensemble** methods (BalancedRandomForest)\n",
    "- **Cost-sensitive** learning\n",
    "\n",
    "### 📊 **Evaluation Strategies**\n",
    "- Metrics that matter for imbalanced data\n",
    "- Precision, Recall, F1-Score deep dive\n",
    "- ROC-AUC vs PR-AUC comparison\n",
    "- Confusion matrix interpretation\n",
    "\n",
    "### 🎛️ **Strategy Selection**\n",
    "- When to use each technique\n",
    "- Combining multiple approaches\n",
    "- Domain-specific considerations\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 Why This Matters\n",
    "\n",
    "By the end of this notebook, you'll:\n",
    "- ✅ **Detect imbalance** like a pro\n",
    "- ✅ **Choose the right strategy** for your data\n",
    "- ✅ **Apply advanced techniques** beyond basic SMOTE\n",
    "- ✅ **Evaluate properly** using appropriate metrics\n",
    "- ✅ **Build robust models** that work in real-world scenarios\n",
    "\n",
    "Let's dive deep into the world of balanced machine learning! 🏊‍♂️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44302745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📦 Step 1: Import Libraries for Advanced Imbalance Analysis\n",
    "print(\"📦 IMPORTING ADVANCED IMBALANCE LIBRARIES...\")\n",
    "print(\"=\"*45)\n",
    "\n",
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up beautiful plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Advanced imbalance-learn library techniques\n",
    "from imblearn.over_sampling import (\n",
    "    SMOTE,              # Standard SMOTE\n",
    "    BorderlineSMOTE,    # Borderline cases SMOTE\n",
    "    ADASYN,             # Adaptive Synthetic Sampling\n",
    "    SVMSMOTE            # SVM-based SMOTE\n",
    ")\n",
    "\n",
    "from imblearn.under_sampling import (\n",
    "    EditedNearestNeighbours,  # Remove noisy samples\n",
    "    TomekLinks,               # Remove Tomek link pairs\n",
    "    RandomUnderSampler,       # Random undersampling\n",
    "    NeighbourhoodCleaningRule # Clean overlapping samples\n",
    ")\n",
    "\n",
    "from imblearn.combine import (\n",
    "    SMOTETomek,         # SMOTE + Tomek links\n",
    "    SMOTEENN            # SMOTE + Edited Nearest Neighbours\n",
    ")\n",
    "\n",
    "# Ensemble methods for imbalanced data\n",
    "from imblearn.ensemble import (\n",
    "    BalancedRandomForestClassifier,\n",
    "    BalancedBaggingClassifier,\n",
    "    EasyEnsembleClassifier\n",
    ")\n",
    "\n",
    "# Evaluation metrics for imbalanced data\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    precision_recall_curve,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    accuracy_score\n",
    ")\n",
    "\n",
    "# Model selection and validation\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Utilities\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# 📁 Step 2: Set up project paths\n",
    "print(\"\\n📁 SETTING UP PROJECT PATHS...\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "current_dir = Path.cwd()\n",
    "project_root = current_dir.parent\n",
    "src_path = project_root / 'src'\n",
    "sys.path.append(str(src_path))\n",
    "\n",
    "print(f\"📂 Project root: {project_root}\")\n",
    "print(f\"📂 Data directory: {project_root / 'data'}\")\n",
    "print(f\"📂 Processed data: {project_root / 'data' / 'processed'}\")\n",
    "\n",
    "# Constants\n",
    "RANDOM_STATE = 42\n",
    "print(f\"\\n🎲 Random state: {RANDOM_STATE}\")\n",
    "print(\"✅ Advanced imbalance analysis setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2991186c",
   "metadata": {},
   "source": [
    "## 📊 Step 1: Load Data and Detect Imbalance\n",
    "\n",
    "Let's start by loading our processed data from the feature engineering notebook and conducting a thorough imbalance analysis.\n",
    "\n",
    "### 🔍 **What We'll Analyze:**\n",
    "- **📈 Imbalance severity**: How bad is the imbalance?\n",
    "- **📊 Class distribution**: Exact numbers and percentages\n",
    "- **🎯 Impact assessment**: How will this affect our models?\n",
    "- **📋 Baseline metrics**: What happens if we do nothing?\n",
    "\n",
    "### 📏 **Imbalance Metrics:**\n",
    "- **Imbalance Ratio**: Majority/Minority ratio\n",
    "- **Minority Percentage**: % of minority class\n",
    "- **Severity Level**: Critical, High, Moderate, or Low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4430f12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📁 Step 1: Load processed data\n",
    "print(\"📁 LOADING PROCESSED DATA...\")\n",
    "print(\"=\"*27)\n",
    "\n",
    "# Try to load from processed data directory\n",
    "processed_dir = project_root / 'data' / 'processed'\n",
    "data_loaded = False\n",
    "\n",
    "# Try to load feature engineered data\n",
    "try:\n",
    "    df_path = processed_dir / 'feature_engineered_data.csv'\n",
    "    if df_path.exists():\n",
    "        df = pd.read_csv(df_path)\n",
    "        print(f\"✅ Loaded feature engineered data: {df.shape}\")\n",
    "        data_loaded = True\n",
    "    else:\n",
    "        print(\"❌ Feature engineered data not found\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading data: {e}\")\n",
    "\n",
    "# If processed data not available, try to load raw data\n",
    "if not data_loaded:\n",
    "    print(\"\\n🔄 Trying to load raw data...\")\n",
    "    raw_data_paths = [\n",
    "        project_root / 'data' / 'raw' / 'employee_data.csv',\n",
    "        project_root / 'data' / 'employee_data.csv',\n",
    "        project_root / 'employee_data.csv'\n",
    "    ]\n",
    "    \n",
    "    for path in raw_data_paths:\n",
    "        if path.exists():\n",
    "            df = pd.read_csv(path)\n",
    "            print(f\"✅ Loaded raw data: {df.shape}\")\n",
    "            print(\"⚠️ Note: Using raw data - may need basic preprocessing\")\n",
    "            data_loaded = True\n",
    "            break\n",
    "\n",
    "if not data_loaded:\n",
    "    print(\"❌ No data found. Please ensure data files are available.\")\n",
    "    df = None\n",
    "\n",
    "# 🎯 Step 2: Identify target variable\n",
    "if df is not None:\n",
    "    print(f\"\\n🎯 IDENTIFYING TARGET VARIABLE...\")\n",
    "    print(\"=\"*32)\n",
    "    \n",
    "    # Look for common target variable names\n",
    "    target_candidates = ['Attrition', 'attrition', 'Left', 'left', 'Turnover', 'turnover']\n",
    "    target_col = None\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if col in target_candidates or any(candidate.lower() in col.lower() for candidate in target_candidates):\n",
    "            target_col = col\n",
    "            break\n",
    "    \n",
    "    # If not found, look for binary columns\n",
    "    if target_col is None:\n",
    "        binary_cols = [col for col in df.columns if df[col].nunique() == 2]\n",
    "        if binary_cols:\n",
    "            target_col = binary_cols[0]\n",
    "            print(f\"🔍 Using binary column as target: {target_col}\")\n",
    "    \n",
    "    if target_col:\n",
    "        print(f\"✅ Target variable identified: '{target_col}'\")\n",
    "        \n",
    "        # Get basic info about target\n",
    "        target_values = df[target_col].value_counts()\n",
    "        print(f\"📊 Target values: {list(target_values.index)}\")\n",
    "        print(f\"📋 Sample size: {len(df):,} records\")\n",
    "    else:\n",
    "        print(\"❌ Could not identify target variable automatically\")\n",
    "        print(\"Available columns:\", list(df.columns))\n",
    "\n",
    "# 📊 Step 3: Comprehensive imbalance analysis\n",
    "if df is not None and target_col:\n",
    "    print(f\"\\n📊 COMPREHENSIVE IMBALANCE ANALYSIS\")\n",
    "    print(\"=\"*38)\n",
    "    \n",
    "    # Basic distribution\n",
    "    target_counts = df[target_col].value_counts()\n",
    "    target_percentages = df[target_col].value_counts(normalize=True) * 100\n",
    "    \n",
    "    print(f\"🎯 Target variable: {target_col}\")\n",
    "    print(f\"📊 Class distribution:\")\n",
    "    \n",
    "    for value, count in target_counts.items():\n",
    "        percentage = target_percentages[value]\n",
    "        print(f\"  • {value}: {count:,} samples ({percentage:.2f}%)\")\n",
    "    \n",
    "    # Calculate imbalance metrics\n",
    "    minority_class_pct = target_percentages.min()\n",
    "    majority_class_pct = target_percentages.max()\n",
    "    imbalance_ratio = majority_class_pct / minority_class_pct\n",
    "    \n",
    "    minority_count = target_counts.min()\n",
    "    majority_count = target_counts.max()\n",
    "    \n",
    "    print(f\"\\n📏 IMBALANCE METRICS:\")\n",
    "    print(f\"  • Minority class: {minority_class_pct:.2f}% ({minority_count:,} samples)\")\n",
    "    print(f\"  • Majority class: {majority_class_pct:.2f}% ({majority_count:,} samples)\")\n",
    "    print(f\"  • Imbalance ratio: {imbalance_ratio:.1f}:1\")\n",
    "    \n",
    "    # Classify imbalance severity\n",
    "    if imbalance_ratio >= 20:\n",
    "        severity = \"🔴 CRITICAL\"\n",
    "        impact = \"Severe model bias expected\"\n",
    "        urgency = \"Immediate balancing required\"\n",
    "    elif imbalance_ratio >= 10:\n",
    "        severity = \"🟠 HIGH\"\n",
    "        impact = \"Significant model bias likely\"\n",
    "        urgency = \"Strong balancing recommended\"\n",
    "    elif imbalance_ratio >= 4:\n",
    "        severity = \"🟡 MODERATE\"\n",
    "        impact = \"Some model bias possible\"\n",
    "        urgency = \"Balancing beneficial\"\n",
    "    else:\n",
    "        severity = \"🟢 LOW\"\n",
    "        impact = \"Minimal bias expected\"\n",
    "        urgency = \"Balancing optional\"\n",
    "    \n",
    "    print(f\"\\n🚨 SEVERITY ASSESSMENT:\")\n",
    "    print(f\"  • Level: {severity}\")\n",
    "    print(f\"  • Impact: {impact}\")\n",
    "    print(f\"  • Action: {urgency}\")\n",
    "    \n",
    "    # Store important variables for later use\n",
    "    minority_class = target_counts.idxmin()\n",
    "    majority_class = target_counts.idxmax()\n",
    "    \n",
    "    print(f\"\\n📋 CLASS IDENTIFICATION:\")\n",
    "    print(f\"  • Minority class: '{minority_class}' ({minority_count:,} samples)\")\n",
    "    print(f\"  • Majority class: '{majority_class}' ({majority_count:,} samples)\")\n",
    "\n",
    "else:\n",
    "    print(\"⚠️ Skipping imbalance analysis - data or target not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25211a41",
   "metadata": {},
   "source": [
    "## 🛠️ Step 2: Advanced SMOTE Techniques Comparison\n",
    "\n",
    "Now let's explore **advanced SMOTE variants** beyond the basic SMOTE! Each technique has its strengths and is designed for different scenarios.\n",
    "\n",
    "### 🔬 **SMOTE Variants We'll Test:**\n",
    "\n",
    "#### 1️⃣ **Standard SMOTE**\n",
    "- **What**: Creates synthetic samples between existing minority samples\n",
    "- **Best for**: General use, balanced features\n",
    "- **Pros**: Simple, widely tested, good baseline\n",
    "- **Cons**: May create samples in noisy regions\n",
    "\n",
    "#### 2️⃣ **BorderlineSMOTE** \n",
    "- **What**: Focuses on minority samples near the decision boundary\n",
    "- **Best for**: When classes overlap significantly\n",
    "- **Pros**: Creates more meaningful synthetic samples\n",
    "- **Cons**: May miss some minority regions\n",
    "\n",
    "#### 3️⃣ **ADASYN (Adaptive Synthetic)**\n",
    "- **What**: Generates more synthetic data for harder-to-learn minority samples\n",
    "- **Best for**: Complex decision boundaries\n",
    "- **Pros**: Adaptive to data difficulty\n",
    "- **Cons**: More complex, sensitive to noise\n",
    "\n",
    "#### 4️⃣ **SVMSMOTE**\n",
    "- **What**: Uses SVM to find support vectors for synthetic sample generation\n",
    "- **Best for**: High-dimensional data\n",
    "- **Pros**: Theoretically motivated approach\n",
    "- **Cons**: Computationally expensive\n",
    "\n",
    "### 📊 **Evaluation Strategy:**\n",
    "- **Before/After visualization** of each technique\n",
    "- **Class distribution** comparison\n",
    "- **Sample quality** assessment\n",
    "- **Performance impact** on simple classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af2d63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🛠️ Step 1: Prepare data for SMOTE comparison\n",
    "if df is not None and target_col:\n",
    "    print(\"🛠️ PREPARING DATA FOR SMOTE COMPARISON...\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Prepare features (X) and target (y)\n",
    "    # Use only numerical features for SMOTE (it works better with numbers)\n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Remove target from features if it's numerical\n",
    "    if target_col in numerical_cols:\n",
    "        numerical_cols.remove(target_col)\n",
    "    \n",
    "    X = df[numerical_cols]\n",
    "    y = df[target_col]\n",
    "    \n",
    "    print(f\"📊 Features prepared:\")\n",
    "    print(f\"  • Feature matrix (X): {X.shape}\")\n",
    "    print(f\"  • Target vector (y): {y.shape}\")\n",
    "    print(f\"  • Numerical features used: {len(numerical_cols)}\")\n",
    "    \n",
    "    # Show original class distribution\n",
    "    original_distribution = Counter(y)\n",
    "    print(f\"\\n📋 Original class distribution:\")\n",
    "    for class_val, count in original_distribution.items():\n",
    "        percentage = (count / len(y)) * 100\n",
    "        print(f\"  • {class_val}: {count:,} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # 🔬 Step 2: Apply different SMOTE techniques\n",
    "    print(f\"\\n🔬 APPLYING SMOTE TECHNIQUES...\")\n",
    "    print(\"=\"*32)\n",
    "    \n",
    "    # Dictionary to store results\n",
    "    smote_results = {}\n",
    "    \n",
    "    # Define SMOTE techniques to test\n",
    "    smote_techniques = {\n",
    "        'Standard SMOTE': SMOTE(random_state=RANDOM_STATE),\n",
    "        'BorderlineSMOTE': BorderlineSMOTE(random_state=RANDOM_STATE),\n",
    "        'ADASYN': ADASYN(random_state=RANDOM_STATE),\n",
    "        'SVMSMOTE': SVMSMOTE(random_state=RANDOM_STATE)\n",
    "    }\n",
    "    \n",
    "    for name, technique in smote_techniques.items():\n",
    "        print(f\"\\n🔧 Testing {name}...\")\n",
    "        \n",
    "        try:\n",
    "            # Apply the technique\n",
    "            X_resampled, y_resampled = technique.fit_resample(X, y)\n",
    "            \n",
    "            # Store results\n",
    "            new_distribution = Counter(y_resampled)\n",
    "            smote_results[name] = {\n",
    "                'X_resampled': X_resampled,\n",
    "                'y_resampled': y_resampled,\n",
    "                'distribution': new_distribution,\n",
    "                'original_size': len(y),\n",
    "                'new_size': len(y_resampled),\n",
    "                'synthetic_created': len(y_resampled) - len(y)\n",
    "            }\n",
    "            \n",
    "            print(f\"  ✅ Success! Created {len(y_resampled) - len(y):,} synthetic samples\")\n",
    "            print(f\"     New distribution: {dict(new_distribution)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Failed: {str(e)}\")\n",
    "            smote_results[name] = None\n",
    "    \n",
    "    # 📊 Step 3: Compare results\n",
    "    print(f\"\\n📊 SMOTE TECHNIQUES COMPARISON:\")\n",
    "    print(\"=\"*35)\n",
    "    \n",
    "    # Create comparison table\n",
    "    print(f\"{'Technique':<20} {'Original':<10} {'New Size':<10} {'Synthetic':<10} {'Status'}\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    for name, result in smote_results.items():\n",
    "        if result is not None:\n",
    "            original = result['original_size']\n",
    "            new_size = result['new_size']\n",
    "            synthetic = result['synthetic_created']\n",
    "            status = \"✅ Success\"\n",
    "        else:\n",
    "            original = len(y)\n",
    "            new_size = \"Failed\"\n",
    "            synthetic = \"Failed\"\n",
    "            status = \"❌ Failed\"\n",
    "        \n",
    "        print(f\"{name:<20} {original:<10} {new_size:<10} {synthetic:<10} {status}\")\n",
    "    \n",
    "    # 📈 Step 4: Visualize distributions\n",
    "    print(f\"\\n📈 CREATING DISTRIBUTION VISUALIZATIONS...\")\n",
    "    print(\"=\"*38)\n",
    "    \n",
    "    # Count successful techniques for subplot arrangement\n",
    "    successful_techniques = {name: result for name, result in smote_results.items() if result is not None}\n",
    "    n_techniques = len(successful_techniques) + 1  # +1 for original\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(2, (n_techniques + 1) // 2, figsize=(15, 10))\n",
    "    fig.suptitle('📊 Class Distribution Comparison: Original vs SMOTE Techniques', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Flatten axes for easier indexing\n",
    "    if n_techniques > 1:\n",
    "        axes = axes.flatten()\n",
    "    else:\n",
    "        axes = [axes]\n",
    "    \n",
    "    # Plot original distribution\n",
    "    ax = axes[0]\n",
    "    original_counts = pd.Series(original_distribution)\n",
    "    colors = ['lightcoral', 'lightblue']\n",
    "    bars = ax.bar(original_counts.index, original_counts.values, color=colors, alpha=0.7)\n",
    "    ax.set_title('📊 Original Data', fontweight='bold')\n",
    "    ax.set_ylabel('Number of Samples')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, count in zip(bars, original_counts.values):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + bar.get_height()*0.01,\n",
    "                f'{count:,}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Plot SMOTE results\n",
    "    for i, (name, result) in enumerate(successful_techniques.items(), 1):\n",
    "        ax = axes[i]\n",
    "        smote_counts = pd.Series(result['distribution'])\n",
    "        bars = ax.bar(smote_counts.index, smote_counts.values, color=colors, alpha=0.7)\n",
    "        ax.set_title(f'⚖️ {name}', fontweight='bold')\n",
    "        ax.set_ylabel('Number of Samples')\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, count in zip(bars, smote_counts.values):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + bar.get_height()*0.01,\n",
    "                    f'{count:,}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(n_techniques, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"✅ Visualization complete!\")\n",
    "\n",
    "else:\n",
    "    print(\"⚠️ Skipping SMOTE comparison - data not available\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
